{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2047f37",
   "metadata": {},
   "source": [
    "# Capítulo 4: Modelos de entrenamiento\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Hasta ahora hemos tratado los modelos de aprendizaje automático y sus algoritmos de entrenamiento principalmente como cajas negras. \n",
    "\n",
    "Es posible que te haya sorprendido lo mucho que puedes hacer sin saber nada sobre lo que hay bajo el capó: optimizaste un sistema de regresión, mejoraste un clasificador de imágenes de dígitos e incluso construiste un clasificador de spam desde cero, todo sin saber cómo funcionan realmente. \n",
    "\n",
    "De hecho, en muchas situaciones no es necesario conocer los detalles de la implementación.\n",
    "\n",
    "Sin embargo, tener una buena comprensión de cómo funcionan las cosas puede ayudarte a encontrar rápidamente el modelo apropiado, el algoritmo de entrenamiento adecuado para usar y un buen conjunto de hiperparámetros para tu tarea. \n",
    "\n",
    "Comprender lo que hay \"bajo el capó\" también le ayudará a depurar los problemas y a realizar el análisis de errores de manera más eficiente. \n",
    "\n",
    "Por último, la mayoría de los temas tratados en este capítulo serán esenciales para comprender, construir y entrenar las redes neuronales (discutido en la Parte II de este libro).\n",
    "\n",
    "En este capítulo comenzaremos por ver el modelo de regresión lineal, uno de los modelos más simples que existen. Discutiremos dos formas muy diferentes de entrenarlo:\n",
    "\n",
    "* Usando una ecuación de \"**forma cerrada**\"⁠ que **calcule directamente** los parámetros del modelo que mejor se ajustan al modelo al conjunto de entrenamiento (es decir, los parámetros del modelo que minimizan la función de costo sobre el conjunto de entrenamiento).\n",
    "\n",
    "\n",
    "* Utilizando un enfoque de optimización **iterativo** llamado **descenso de gradiente** (GD) que ajusta **gradualmente** los parámetros del modelo para minimizar la función de costo sobre el conjunto de entrenamiento, eventualmente convergiendo con el mismo conjunto de parámetros que el primer método. Veremos algunas variantes de descenso de gradiente que usaremos una y otra vez cuando estudiemos las redes neuronales en la Parte II: **GD de lote, GD de minibate y GD estocástico**.\n",
    "\n",
    "\n",
    "A continuación, veremos la **regresión polinómica**, un modelo más complejo que puede adaptarse a conjuntos de datos **no lineales**. \n",
    "\n",
    "Dado que este modelo tiene **más parámetros que la regresión lineal**, es más propenso a **sobreajustar** los datos de entrenamiento. \n",
    "\n",
    "Exploraremos cómo detectar si este es el caso o no utilizando **curvas de aprendizaje**, y luego veremos varias **técnicas de regularización** que pueden reducir el riesgo de sobreadaptar el conjunto de entrenamiento.\n",
    "\n",
    "Por último, examinaremos dos **modelos más que se utilizan** comúnmente para las tareas de **clasificación**: **regresión logística y regresión softmax**.\n",
    "\n",
    "\n",
    "### ------------------------ ADVERTENCIA ------------------------\n",
    "\n",
    "Habrá bastantes ecuaciones matemáticas en este capítulo, utilizando nociones básicas de álgebra lineal y cálculo. \n",
    "\n",
    "Para entender estas ecuaciones, necesitarás saber qué son los vectores y las matrices; cómo transponerlos, multiplicarlos e invertirlos; y qué son las derivadas parciales. \n",
    "\n",
    "Si no estás familiarizado con estos conceptos, consulte los tutoriales introductorios de álgebra lineal y cálculo disponibles como cuadernos Jupyter en el material complementario en línea. \n",
    "\n",
    "Para aquellos que son realmente alérgicos a las matemáticas, aún así deben pasar por este capítulo y simplemente omitir las ecuaciones; con suerte, el texto será suficiente para ayudarle a entender la mayoría de los conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dc0d0",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "</bre/>\n",
    "En el capítulo 1 se analizó un modelo de **regresión simple** para comprobar la satisfacción con la vida:\n",
    "\n",
    "**<center>satif_vida = θ0 + θ1 × GDP_per_capita</center>**\n",
    "\n",
    "Es solo una función lineal (línea recta) con una sola característica de entrada y dos parámetros:\n",
    "\n",
    "- Característica: `GDP_per_capita`\n",
    "- Parámetros: `θ0` y `θ1`\n",
    "\n",
    "Un modelo lineal funciona haciendo una predicción calculando una suma ponderada de características de entrada más una constante (sesgo o término de intersección).\n",
    "\n",
    "\n",
    "### Ecuación 4-1: Predicción del modelo de Regresión Lineal\n",
    "</br>\n",
    "\n",
    "**<center>y = θ0 + θ1X1 + θ2X2 + ... + θnXn</center>**\n",
    "\n",
    "* **y** = valor predicho\n",
    "* **n** = número de características\n",
    "* **Xi** = valor de la primera característica\n",
    "* **θj** = parámetro del modelo j, incluido el término de sesgo θ0 y los pesos de características θ1, θ2, ..., θn.\n",
    "\n",
    "(Y esto se puede escribir de forma más concisa usando una forma vectorizada).\n",
    "\n",
    "\n",
    "### Ecuación 4-2: Predicción del modelo de Regresión Lineal (FORMA VECTORIAL)\n",
    "</br>\n",
    "\n",
    "**<center>y = hθ(X) = θ*X</center>**\n",
    "\n",
    "* **hθ** = función de hipótesis, que usa los parámetros del modelo θ\n",
    "* **θ** = número de características\n",
    "* **X** = vector de parámetros del modelo, que contiene el término de sesgo θ0 y los pesos de características θ1 a θn.\n",
    "* **θ*X** = producto punto de los vectores θ y X, que es igual a θ0x0 + θ1x1 + θ2x2 + ... θnxn\n",
    "\n",
    "</br></br>\n",
    "**---------------------- NOTA ------------------------**\n",
    "\n",
    "En el aprendizaje automático, los vectores a menudo se representan como vectores de columna (row-vector), que son matrices 2D con una sola columna.\n",
    "\n",
    "Si **θ** y **x** son vectores de columna, entonces la predicción es `y = θ^T*X`, donde `θ^T` es la transposición de `θ` (un vector fila en lugar de un vector columna) y `θ^T*X` es la multiplicación de la matriz de `θ^T` y `X`.\n",
    "\n",
    "Por supuesto, es la misma predicción, excepto que ahora se representa como una matriz de una sola celda en lugar de un valor escalar. \n",
    "\n",
    "**------------------------------------------------------**\n",
    "</br>\n",
    "\n",
    "Vale, ese es el modelo de regresión lineal, pero ¿cómo lo entrenamos? Bueno, recuerda que entrenar un modelo significa establecer sus parámetros para que el modelo se ajuste mejor al conjunto de entrenamiento. \n",
    "\n",
    "Para este propósito, primero necesitamos una medida de qué tan bien (o mal) se ajusta el modelo a los datos de entrenamiento. \n",
    "\n",
    "En el capítulo 2 vimos que la medida de rendimiento más común de un modelo de regresión es el **error cuadrado medio** de la raíz. \n",
    "\n",
    "Por lo tanto, para entrenar un modelo de regresión lineal, necesitamos encontrar el valor de θ que minimice el **RMSE**. \n",
    "\n",
    "En la práctica, es más sencillo minimizar el error medio al cuadrado (MSE) que el RMSE, y conduce al mismo resultado (porque el valor que minimiza una función positiva también minimiza su raíz cuadrada).\n",
    "\n",
    "**---------------------- ADVERTENCIA ------------------------**\n",
    "\n",
    "Los algoritmos de aprendizaje a menudo optimizarán una función de pérdida diferente durante el entrenamiento que la medida de rendimiento utilizada para evaluar el modelo final. Esto se debe generalmente a que la función es más fácil de optimizar y/o porque tiene términos adicionales necesarios solo durante el entrenamiento (por ejemplo, para la regularización). Una buena métrica de rendimiento está lo más cerca posible del objetivo comercial final. Una buena pérdida de entrenamiento es fácil de optimizar y está fuertemente correlacionada con la métrica. Por ejemplo, los clasificadores a menudo se entrenan utilizando una función de costo como la pérdida de registro (como verá más adelante en este capítulo), pero se evalúan utilizando precisión/retirada. La pérdida de registro es fácil de minimizar, y hacerlo generalmente mejorará la precisión/retirada.\n",
    "\n",
    "**----------------------------------------------------------------**\n",
    "\n",
    "El MSE de una hipótesis de regresión lineal hθ en un set de entrenamiento ´X´ se calcula utilizando la ecuación 4-3.\n",
    "\n",
    "\n",
    "### Ecuación 4-3: Función de coste de MSE para un modelo de regresión lineal\n",
    "</br>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mrow>\n",
    "    <mtext>MSE</mtext>\n",
    "    <mrow>\n",
    "      <mo>(</mo>\n",
    "      <mi mathvariant=\"bold\">X</mi>\n",
    "      <mo>,</mo>\n",
    "      <msub><mi>h</mi> <mi mathvariant=\"bold\">θ</mi> </msub>\n",
    "      <mo>)</mo>\n",
    "    </mrow>\n",
    "    <mo>=</mo>\n",
    "    <mstyle scriptlevel=\"0\" displaystyle=\"true\">\n",
    "      <mfrac><mn>1</mn> <mi>m</mi></mfrac>\n",
    "    </mstyle>\n",
    "    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>\n",
    "    <msup><mrow><mo>(</mo><msup><mi mathvariant=\"bold\">θ</mi> <mo>⊺</mo> </msup><msup><mi mathvariant=\"bold\">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup>\n",
    "  </mrow>\n",
    "</math>\n",
    "\n",
    "MSE(X,hθ) = 1/m * SUM i to m (θT * X^i - y^i)^2\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/qsT73Cj/Captura-de-pantalla-2023-08-18-a-las-20-00-06.png\" alt=\"Captura-de-pantalla-2023-08-18-a-las-20-00-06\" border=\"0\"></a><br /><a target='_blank' href='https://imgbb.com/'></a><br />\n",
    "\n",
    "La mayoría de estas anotaciones se presentaron en el capítulo 2. La única diferencia es que escribimos hθ en lugar de solo h para dejar claro que el modelo está parametrizado por el vector θ. Para simplificar las anotaciones, solo escribiremos MSE(θ) en lugar de MSE(X, hθ).\n",
    "\n",
    "### Ecuación normal\n",
    "\n",
    "Para encontrar el valor de θ que minimiza el MSE, existe una solución de forma cerrada, en otras palabras, una ecuación matemática que da el resultado directamente. \n",
    "\n",
    "Esto se llama **ecuación normal** y **nunca vamos a calcular en problemas complicados** porque es increiblemente lento (imposible computacionalmente) y tendremos que saber como acercarnos a la solución sin calcular esto.\n",
    "\n",
    "\n",
    "### Ecuación 4-4: Ecuación Normal\n",
    "\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/51vHrfN/Captura-de-pantalla-2023-08-18-a-las-20-06-44.png\" alt=\"Captura-de-pantalla-2023-08-18-a-las-20-06-44\" border=\"0\"></a>\n",
    "\n",
    "Donde:\n",
    "\n",
    "* **θ** = valor de θ que minimiza la función de coste.\n",
    "* **y** = vector de valores onjetivo que contiene **y(1)** a **y(m)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b6db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos algunos datos con forma lineal para probar que la ecuación funciona\n",
    "# Conjunto de datos lineal generado al azar.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "m = 100  # número de instancias\n",
    "X = 2 * np.random.rand(m, 1)  # vector columna\n",
    "y = 4 + 3 * X + np.random.randn(m, 1)  # vector columna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42288497",
   "metadata": {},
   "source": [
    "![data_points_lineal](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca7379",
   "metadata": {},
   "source": [
    "Ahora vamos a calcular **θ^** usando la **Ecuación Normal**. \n",
    "\n",
    "Usamos la función `inv()` del módulo de álgebra lineal de NumPy  (`np.linalg`) para hacer la inversa de la matriz, y el método `dot()` para la multiplicación de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348d63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "X_b = add_dummy_feature(X)  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c2ce0",
   "metadata": {},
   "source": [
    "### -------------------------- NOTA --------------------------\n",
    "\n",
    "El operador `@` realiza la multiplicación de matrices. \n",
    "\n",
    "Si **A** y **B** son matrices NumPy, entonces **A @ B** es equivalente a `np.matmul(A, B)`. \n",
    "\n",
    "Muchas otras bibliotecas, como TensorFlow, PyTorch y JAX, también admiten el operador **@**. Sin embargo, no puede usar @ en arreglos puros de Python (es decir, listas de listas).\n",
    "\n",
    "### -------------------------------------------------------------\n",
    "\n",
    "La función que usamos para generar los datos es `y = 4+3x1 + ruido_gaussiano`.\n",
    "Veamos que encontró la ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0bd692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba551d4",
   "metadata": {},
   "source": [
    "Habríamos esperado **(θ0 = 4** y **θ1 = 3)** en lugar de **(θ0 = 4.215** y **θ1 = 2.770)**. \n",
    "\n",
    "Lo suficientemente cerca, pero el ruido hizo imposible recuperar los parámetros exactos de la función original. \n",
    "\n",
    "**Cuanto más pequeño y ruidoso sea el conjunto de datos, más difícil se vuelve**.\n",
    "\n",
    "Ahora podemos hacer predicciones usando **θ^**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = add_dummy_feature(X_new)  # agregar x0 = 1 a cada instancia\n",
    "y_predict = X_new_b @ theta_best\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57611120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizamos las predicciones del modelo de R.Lineal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cbc570",
   "metadata": {},
   "source": [
    "Realizar una regresión lineal con **Scickit-Learn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10290589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e77552",
   "metadata": {},
   "source": [
    "Observa que Scikit-Learn separa el término de bias (`intercept_`) de los pesos de características (`coef_`).\n",
    "\n",
    "La clase LinearRegression se basa en la función `scipy.linalg.lstsq()` (el nombre se refiere a \"mínimos cuadrados\"), a la que se puede llamar directamente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df62e8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f3da7",
   "metadata": {},
   "source": [
    "Esta función calcula **θ^ = X+y**, donde **X+** es la pseudoinversa de **X** (específicamente la inversa de Moore-Penrose).\n",
    "\n",
    "Puedes usar `np.linalg.pinv()` para hacer la pseudoinversa directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad75318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21810da",
   "metadata": {},
   "source": [
    "La pseudoinversa en sí misma se calcula utilizando una técnica de factorización de matriz estándar llamada descomposición de valores singulares (**SVD**) que puede descomponer la matriz del conjunto de entrenamiento **X** en la multiplicación de matrices de tres matrices U Σ V⊺ (`vernumpynumpy.linalg.svd()`). \n",
    "\n",
    "La pseudoinversa se calcula como:\n",
    "\n",
    "**<center>X+ = VΣ+U⊺</center>** \n",
    "\n",
    "Para calcular la matriz Σ+, el algoritmo toma Σ y establece en cero todos los valores más pequeños que un pequeño valor de umbral, luego reemplaza todos los valores distintos de cero con su inverso y, finalmente, transpone la matriz resultante. \n",
    "\n",
    "Este enfoque es más eficiente que calcular la ecuación Normal, además de que maneja bien los casos de borde: de hecho, la ecuación Normal puede no funcionar si la matriz **X⊺X** no es invertible (es decir, singular), como si m < n o si algunas características son redundantes, pero el pseudoinverse siempre está definido.\n",
    "\n",
    "\n",
    "### Complejidad computacional\n",
    "</br></br>\n",
    "La ecuación normal calcula la inversa de **X⊺ X**, que es una matriz (n + 1) × (n + 1) (donde n es el número de características). \n",
    "\n",
    "La complejidad computacional de invertir dicha matriz suele ser de _O(n2.4) a O(n3)_, dependiendo de la implementación. \n",
    "\n",
    "En otras palabras, si duplicas el número de características, multiplicas el tiempo de cálculo por aproximadamente 22,4 = 5,3 a 23 = 8.\n",
    "\n",
    "El enfoque SVD utilizado por la clase `LinearRegression` de Scikit-Learn se trata de _O(n2)_. \n",
    "\n",
    "**Si duplicas la cantidad de características, multiplica el tiempo de cálculo por aproximadamente 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37e98d",
   "metadata": {},
   "source": [
    "#### ---------------------------------- ADVERTENCIA ----------------------------------\n",
    "\n",
    "Tanto la **ecuación normal** como el enfoque **SVD** se **ralentizan** mucho cuando el número de características **crece** (por ejemplo, 100.000). \n",
    "\n",
    "En el lado positivo, **ambos son lineales con respecto al número de instancias** en el conjunto de entrenamiento (son _O(m)_), por lo que manejan grandes conjuntos de entrenamiento de manera eficiente, siempre que puedan caber en la memoria.\n",
    "### --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Además, una vez que hayas entrenado tu modelo de regresión lineal (usando la ecuación normal o cualquier otro algoritmo), **las predicciones son muy rápidas: la complejidad computacional es lineal con respecto tanto al número de instancias en las que desea hacer predicciones como al número de características**. En otras palabras, hacer predicciones sobre el doble de casos (o el doble de características) tomará aproximadamente el doble de tiempo.\n",
    "\n",
    "**<center>x2 características = x2 tiempo</center>**\n",
    "\n",
    "#### Ahora veremos una forma muy diferente de entrenar un modelo de regresión lineal, que es más adecuado para los casos en los que hay un gran número de características o demasiadas instancias de entrenamiento para encajar en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05772e8",
   "metadata": {},
   "source": [
    "## Descenso del gradiente\n",
    "\n",
    "</br></br>\n",
    "El _descenso de gradiente_ es un algoritmo de optimización genérico capaz de encontrar **soluciones óptimas** para una amplia gama de problemas. \n",
    "\n",
    "La idea general del descenso del gradiente es **ajustar los parámetros de forma iterativa** para minimizar una **función de coste*.\n",
    "\n",
    "Supongamos que estás perdido en las montañas en una densa niebla, y solo puedes sentir la pendiente del suelo debajo de tus pies. Una buena estrategia para llegar al fondo del valle rápidamente es ir cuesta abajo en dirección a la pendiente más empinada. \n",
    "Esto es exactamente lo que hace el descenso del gradiente: mide el gradiente local de la función de error con respecto al vector de parámetros θ, y va en la dirección del gradiente descendente. \n",
    "Una vez que el gradiente es cero (casi nunca pasa), ¡has alcanzado un mínimo!\n",
    "\n",
    "En la práctica, comienzas por llenar θ con valores aleatorios (esto se llama inicialización aleatoria). \n",
    "\n",
    "Luego lo mejoras gradualmente, dando un paso a la vez, cada paso tratando de disminuir la función de costo (por ejemplo, el MSE), hasta que el algoritmo converge al mínimo.\n",
    "\n",
    "![descenso_gradiente](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0403.png)\n",
    "\n",
    "(_En esta representación del descenso de gradiente, los parámetros del modelo se inicializan al azar y se ajustan repetidamente para minimizar la función de costo; el tamaño del paso de aprendizaje es proporcional a la pendiente de la función de costo, por lo que los pasos se reducen gradualmente a medida que el costo se acerca al mínimo_)\n",
    "\n",
    "</br></br>\n",
    "\n",
    "Un parámetro importante en el descenso de gradiente es el tamaño de los pasos, determinado por el hiperparámetro de la **tasa de aprendizaje** o learning rate.\n",
    "\n",
    "Si la tasa de aprendizaje es **demasiado pequeña**, entonces el algoritmo tendrá que pasar por **muchas iteraciones para converger**, lo que llevará mucho tiempo.\n",
    "\n",
    "![small_learning_rate](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0404.png)\n",
    "\n",
    "</br></br>\n",
    "\n",
    "Por otro lado, si la tasa de aprendizaje es **demasiado alta**, podrías saltar a través del valle y terminar en el otro lado, posiblemente incluso más alto de lo que estabas antes. \n",
    "Esto podría hacer que el algoritmo **diverja** (no converja), con valores cada vez más grandes, al no encontrar una buena solución.\n",
    "\n",
    "![big_learning_rate](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0405.png)\n",
    "\n",
    "Además, no todas las funciones de coste parecen cuencos bonitos y normales. Puede haber agujeros, crestas, mesetas y todo tipo de terreno irregular, lo que dificulta la convergencia al mínimo. \n",
    "\n",
    "La siguiente figura muestra los dos **desafíos principales** con el descenso en pendiente. \n",
    "\n",
    "Si la inicialización aleatoria inicia el algoritmo a la **izquierda**, entonces convergerá a un mínimo local, que no es tan bueno como el mínimo global. \n",
    "\n",
    "Si comienza a la **derecha**, entonces llevará mucho tiempo cruzar la meseta. Y si te detienes demasiado pronto, nunca alcanzarás el mínimo global.\n",
    "\n",
    "![desafios_descenso_gradiente](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0406.png)\n",
    "\n",
    "\n",
    "Afortunadamente, la función de costo de MSE para un modelo de regresión lineal resulta ser una función **convexa**, lo que significa que si eliges dos puntos en la curva, el segmento de línea que los une nunca está por debajo de la curva. Esto implica que no hay mínimos locales, solo un mínimo global. \n",
    "\n",
    "También es una función continua con una pendiente que nunca cambia abruptamente.⁠\n",
    "\n",
    "Estos dos hechos tienen una gran consecuencia: se garantiza que el descenso del gradiente se acerca arbitrariamente al mínimo global (si esperas lo suficiente y si la tasa de aprendizaje no es demasiado alta).\n",
    "\n",
    "Si bien la función de costo tiene la forma de un tazón, puede ser un tazón alargado si las características tienen escalas muy diferentes. \n",
    "\n",
    "La siguiente imagen muestra el descenso en gradiente en un conjunto de entrenamiento donde las características 1 y 2 tienen la misma escala (a la izquierda), y en un conjunto de entrenamiento donde la característica 1 tiene valores mucho más pequeños que la característica 2 (a la derecha).⁠\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0407.png)\n",
    "\n",
    "Como puedes ver, a la izquierda el algoritmo de descenso de gradiente va directamente hacia el mínimo, llegando así rápidamente, mientras que a la derecha primero va en una dirección casi ortogonal a la dirección del mínimo global, y termina con una larga marcha por un valle casi plano. \n",
    "\n",
    "Eventualmente alcanzará el mínimo, pero llevará mucho tiempo.\n",
    "\n",
    "#### ----------------------------- ADVERTENCIA -----------------------------\n",
    "\n",
    "Cuando uses el gradiente descendente, deberías asegurarte que todas las características tienen una escala similar (ej: usando `StandardScaler`), de lo contrario tardará mucho más en converger.\n",
    "#### -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844bb5a3",
   "metadata": {},
   "source": [
    "Este diagrama también ilustra el hecho de que **entrenar un modelo significa buscar una combinación de parámetros del modelo que minimice una función de costo** (sobre el conjunto de entrenamiento). \n",
    "\n",
    "Es una búsqueda en el espacio de parámetros del modelo. \n",
    "\n",
    "Cuantos más parámetros tenga un modelo, más dimensiones tenga este espacio y más difícil sea la búsqueda: buscar una aguja en un pajar de 300 dimensiones es mucho más complicado que en 3 dimensiones. \n",
    "\n",
    "Afortunadamente, dado que la función de costo es convexa en el caso de la regresión lineal, la aguja está simplemente en la parte inferior del tazón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d5375",
   "metadata": {},
   "source": [
    "## Descenso del gradiente por lotes (BATCH)\n",
    "\n",
    "Para implementar el descenso de gradiente, es necesario **calcular el gradiente de la función de costo con respecto a cada parámetro del modelo θj**. \n",
    "\n",
    "En otras palabras, necesitas calcular cuánto cambiará la función de costo si cambias θj solo un poco. Esto se llama derivada parcial. \n",
    "\n",
    "Es como preguntar: \"¿Cuál es la pendiente de la montaña bajo mis pies si miro hacia el este\"? y luego hacer la misma pregunta mirando hacia el norte (y así sucesivamente para todas las demás dimensiones, si puedes imaginar un universo con más de tres dimensiones). \n",
    "\n",
    "La ecuación 4-5 calcula la derivada parcial del MSE con respecto al parámetro θj, señalado ∂ MSE(θ) / ∂θj.\n",
    "\n",
    "### Ecuación 4-5: Derivadas parciales de la función de coste\n",
    "\n",
    "<a href=\"https://ibb.co/z51jft4\"><img src=\"https://i.ibb.co/Sx2WvZ3/Captura-de-pantalla-2023-08-20-a-las-21-19-22.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-19-22\" border=\"0\"></a>\n",
    "\n",
    "En lugar de calcular estas derivadas parciales individualmente, puedes usar la ecuación 4-6 para calcularlas todas de una sola vez. \n",
    "\n",
    "El vector de gradiente, señalado ∇θMSE(θ), contiene todas las derivadas parciales de la función de coste (una para cada parámetro del modelo).\n",
    "\n",
    "### Ecuación 4-6: Vector de gradiente de la función de coste\n",
    "\n",
    "<a href=\"https://ibb.co/zPx8DbB\"><img src=\"https://i.ibb.co/CV8JDvY/Captura-de-pantalla-2023-08-20-a-las-21-20-53.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-20-53\" border=\"0\"></a>\n",
    "</br>\n",
    "\n",
    "#### ---------------------- ADVERTENCIA -------------------\n",
    "</br>\n",
    "\n",
    "¡Ten en cuenta que esta fórmula implica cálculos sobre el conjunto completo de entrenamiento X, en cada paso de descenso de gradiente! \n",
    "\n",
    "Esta es la razón por la que el algoritmo se llama descenso de gradiente **por lotes**: **utiliza todo el lote de datos de entrenamiento en cada paso** (en realidad, el descenso de gradiente completo probablemente sería un mejor nombre). \n",
    "\n",
    "Como resultado, es **terriblemente lento en conjuntos de entrenamiento muy grandes** (en breve veremos algunos algoritmos de descenso de gradiente mucho más rápidos). \n",
    "\n",
    "Sin embargo, el descenso del gradiente se escala bien con el número de características; entrenar un modelo de regresión lineal cuando hay cientos de miles de características es mucho más rápido usando el descenso del gradiente que usando la ecuación normal o la descomposición de SVD.\n",
    "\n",
    "#### -----------------------------------------------------------\n",
    "\n",
    "Una vez que tengas el vector de gradiente, que apunta cuesta arriba, simplemente ve en la dirección opuesta para ir cuesta abajo. \n",
    "\n",
    "Esto significa restar ∇θMSE(θ) de θ. \n",
    "\n",
    "Aquí es donde entra en juego la tasa de aprendizaje η:⁠4 multiplica el vector de gradiente por η para determinar el tamaño del paso de descenso (Ecuación 4-7).\n",
    "\n",
    "### Ecuación 4-7: Paso (step) del descenso del gradiente\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/1MHnNPW/Captura-de-pantalla-2023-08-20-a-las-21-24-54.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-24-54\" border=\"0\"></a>\n",
    "\n",
    "Vamos a hacer una implementación básica de este algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b0b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_epochs = 1000\n",
    "m = len(X_b)  # número de instancias\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # parámetros del modelo inicializados aleatoriamente\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b6a17",
   "metadata": {},
   "source": [
    "No fue muy dificil! Cada iteración del conjunto de entrenamiento es llamada \"**época**\".\n",
    "\n",
    "Vamos a ver el resultado de `theta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47e8ecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb21860",
   "metadata": {},
   "source": [
    "Ey, ¡eso es exactamente lo que encontró la ecuación normal! \n",
    "\n",
    "El descenso en gradiente funcionó perfectamente. Pero, ¿y si hubieras usado una tasa de aprendizaje diferente (`eta`)? \n",
    "\n",
    "La figura siguiente muestra los primeros 20 pasos de descenso de gradiente utilizando tres tasas de aprendizaje diferentes. \n",
    "\n",
    "La línea en la parte inferior de cada gráfico representa el punto de partida aleatorio, luego cada época está representada por una línea cada vez más oscura.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0408.png)\n",
    "\n",
    "- A la **izquierda**, la tasa de aprendizaje es demasiado baja: el algoritmo finalmente llegará a la solución, pero llevará mucho tiempo. \n",
    "\n",
    "- En el **medio**, la tasa de aprendizaje se ve bastante bien: en solo unas pocas épocas, ya ha convergido hacia la solución. \n",
    "\n",
    "- A la **derecha**, la tasa de aprendizaje es demasiado alta: el algoritmo diverge, saltando por todas partes y en realidad se aleja cada vez más de la solución en cada paso.\n",
    "\n",
    "</br>\n",
    "\n",
    "Para **encontrar una buena tasa de aprendizaje**, puede utilizar la **búsqueda en grill** (`SearchGrill` en el capítulo 2). Sin embargo, es posible que desee **limitar el número de épocas** para que la búsqueda en la cuadrícula pueda eliminar los modelos que tardan demasiado en converger.\n",
    "\n",
    "Puede que te preguntes cómo establecer el **número de épocas**: \n",
    "\n",
    "Si es demasiado **bajo**, todavía estarás **lejos de la solución** óptima cuando el algoritmo se detenga; pero si es demasiado **alto**, **perderás el tiempo mientras los parámetros del modelo ya no cambian**. \n",
    "\n",
    "Una solución simple es establecer un número muy grande de épocas, pero interrumpir el algoritmo cuando el vector de gradiente se vuelve diminuto, es decir, cuando su norma se vuelve más pequeña que un número diminuto ε (llamado _tolerancia_), porque esto sucede cuando el descenso del gradiente (casi) ha alcanzado el mínimo.\n",
    "\n",
    "#### --------------------- TASA DE CONVERGENCIA -------------------------\n",
    "Cuando la función de costo es convexa y su pendiente no cambia abruptamente (como es el caso de la función de costo de MSE), el descenso del gradiente de lote con una tasa de aprendizaje fija eventualmente convergerá a la solución óptima, pero es posible que tenga que esperar un tiempo: puede tomar iteraciones O(1/ε) para alcanzar el óptimo dentro de un rango de Si divides la tolerancia entre 10 para tener una solución más precisa, entonces es posible que el algoritmo tenga que funcionar unas 10 veces más tiempo.\n",
    "#### ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e5640",
   "metadata": {},
   "source": [
    "## Descenso del gradiente estocástico\n",
    "</br></br>\n",
    "El principal problema con el descenso del gradiente por **lotes** (batch) es el hecho de que **utiliza todo el conjunto de entrenamiento** para calcular los gradientes en cada paso, lo que lo hace **muy lento cuando el conjunto de entrenamiento es grande**. \n",
    "\n",
    "En el extremo opuesto, el descenso del gradiente **estocástico** elige una **instancia aleatoria** en el conjunto de entrenamiento en cada paso y calcula los gradientes basados **solo en esa única instancia**. \n",
    "\n",
    "Obviamente, trabajar en una sola instancia a la vez hace que el algoritmo sea mucho más rápido porque tiene muy pocos datos para manipular en cada iteración. \n",
    "\n",
    "También hace posible entrenar en grandes conjuntos de entrenamiento, ya que solo una instancia debe estar en la memoria en cada iteración (el GD estocástico se puede implementar como un algoritmo fuera del núcleo; véase el capítulo 1).\n",
    "\n",
    "Por otro lado, debido a su naturaleza estocástica (es decir, aleatoria), este algoritmo es **mucho menos regular** que el descenso del gradiente de lotes: en lugar de disminuir suavemente hasta que alcance el mínimo, la función de costo rebotará hacia arriba y hacia abajo, disminuyendo solo en promedio. \n",
    "\n",
    "Con el tiempo terminará muy cerca del mínimo, pero una vez que llegue allí, **continuará rebotando, sin asentarse** (ver figura siguiente). \n",
    "\n",
    "Una vez que el algoritmo se detenga, los valores finales de los parámetros serán buenos, pero **no óptimos**.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0409.png)\n",
    "\n",
    "(_Con el descenso de gradiente estocástico, cada paso de entrenamiento es mucho más rápido, pero también mucho más estocástico que cuando se usa el descenso de gradiente por lotes_)\n",
    "\n",
    "Cuando la función de costo es muy irregular, esto en realidad puede ayudar al algoritmo a saltar de los mínimos locales, por lo que el descenso del gradiente estocástico tiene una mejor oportunidad de encontrar el mínimo global que el descenso del gradiente por lotes.\n",
    "\n",
    "Por lo tanto, la aleatoriedad es buena para escapar del \n",
    "óptimo local, pero mala porque significa que el algoritmo nunca puede establecerse al mínimo. \n",
    "\n",
    "Una solución a este dilema es reducir gradualmente la tasa de aprendizaje. Los pasos comienzan a lo grande (lo que ayuda a progresar rápidamente y a escapar de los mínimos locales), luego se hacen cada vez más pequeños, lo que permite que el algoritmo se establezca en el mínimo global. \n",
    "Este proceso es similar al recocido simulado, un algoritmo inspirado en el proceso en la metalurgia del recocido, donde el metal fundido se enfría lentamente. \n",
    "\n",
    "La función que determina la tasa de aprendizaje en cada iteración se llama programa de aprendizaje. \n",
    "\n",
    "Si la tasa de aprendizaje se reduce demasiado rápido, puede quedar atrapado en un mínimo local, o incluso terminar congelado a mitad del mínimo. \n",
    "\n",
    "Si la tasa de aprendizaje se reduce demasiado lentamente, puede saltar alrededor del mínimo durante mucho tiempo y terminar con una solución subóptima si detiene el entrenamiento demasiado pronto.\n",
    "\n",
    "Este código implementa el descenso del gradiente estocástico utilizando un programa de aprendizaje simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550c4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index : random_index + 1]\n",
    "        yi = y[random_index : random_index + 1]\n",
    "        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n",
    "        eta = learning_schedule(epoch * m + iteration)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e32385",
   "metadata": {},
   "source": [
    "Por convención, iteramos por rondas de m iteraciones; cada ronda se llama una época, como antes. \n",
    "\n",
    "Mientras que el código de descenso de gradiente por lotes iteró 1000 veces a través de todo el conjunto de entrenamiento, este código pasa por el conjunto de entrenamiento solo 50 veces y alcanza una solución bastante buena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baaa2c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21076011],\n",
       "       [2.74856079]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafd3c0",
   "metadata": {},
   "source": [
    "Tenga en cuenta que, dado que las instancias se eligen al azar, algunas instancias pueden elegirse varias veces por época, mientras que otras pueden no elegirse en absoluto. \n",
    "\n",
    "Si quieres estar seguro de que el algoritmo pasa por cada instancia en cada época, otro enfoque es barajar el conjunto de entrenamiento (asegurándose de barajar las características de entrada y las etiquetas de forma conjunta), luego pasar por él instancia por instancia, luego barajarlo de nuevo, y así sucesivamente. Sin embargo, este enfoque es más complejo y, en general, no mejora el resultado.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0410.png)\n",
    "\n",
    "Figura 4-10 (_Los primeros 20 pasos del descenso del gradiente estocástico_)\n",
    "\n",
    "\n",
    "\n",
    "#### ----------------------------- ADVERTENCIA --------------------------------\n",
    "\n",
    "Cuando se utiliza el descenso del gradiente estocástico, las instancias de entrenamiento deben ser independientes y distribuidas de manera idéntica (IID) para garantizar que los parámetros se tiren hacia el óptimo global, en promedio. \n",
    "Una forma sencilla de garantizar esto es barajar las instancias durante el entrenamiento (por ejemplo, elegir cada instancia al azar o barajar el conjunto de entrenamiento al comienzo de cada época). \n",
    "Si no baraja las instancias, por ejemplo, si las instancias están ordenadas por etiqueta, entonces SGD comenzará optimizando para una etiqueta, luego la siguiente, y así sucesivamente, y no se ajustará cerca del mínimo global.\n",
    "#### --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Para realizar una regresión lineal utilizando GD estocástico con Scikit-Learn, puede utilizar la clase `SGDRegressor`, que por defecto es la optimización de la función de costo de MSE. \n",
    "\n",
    "El siguiente código se ejecuta durante un máximo de 1000 épocas (`max_iter`) o hasta que la pérdida cae en menos de 10-5 (`tol`) durante 100 épocas (`n_iter_no_change`). \n",
    "\n",
    "Comienza con una tasa de aprendizaje de 0,01 (`eta0`), utilizando el horario de aprendizaje predeterminado (diferente del que usamos). Por último, no utiliza ninguna regularización (`penalty=None`; más detalles sobre esto en breve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7461ce2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n",
    "                       n_iter_no_change=100, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() porque fit() espera etiquetas de 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93495f",
   "metadata": {},
   "source": [
    "Una vez más, encuentras una solución bastante cercana a la devuelta por la Normalequation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102b95c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21278812]), array([2.77270267]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1087fc",
   "metadata": {},
   "source": [
    "#### ------------------------------------ TIP ------------------------------------\n",
    "\n",
    "Todos los estimadores de Scikit-Learn se pueden entrenar usando el método `fit()`, pero algunos estimadores también tienen un método `part_fit()` al que se puede llamar para ejecutar una única ronda de entrenamiento en una o más instancias (ignora hiperparámetros como `max_iter` o `tol`). . \n",
    "\n",
    "Llamar repetidamente a `part_fit()` entrenará gradualmente el modelo. Esto resulta útil cuando necesita más control sobre el proceso de formación. \n",
    "\n",
    "Otros modelos tienen en su lugar un hiperparámetro `warm_start` (y algunos tienen ambos): si estableces `warm_start=True`, llamar al método `fit()` en un modelo entrenado no restablecerá el modelo; simplemente continuará entrenando donde lo dejó, respetando hiperparámetros como `max_iter` y `tol`. \n",
    "\n",
    "Tenga en cuenta que `fit()` restablece el contador de iteraciones utilizado por el programa de aprendizaje, mientras que `parcial_fit()` no lo hace.\n",
    "#### ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957940b",
   "metadata": {},
   "source": [
    "## Descenso de gradiente por Mini-Lotes (minibatches)\n",
    "</br></br>\n",
    "El último algoritmo de descenso de gradiente que veremos se llama descenso de gradiente de mini lotes. Es sencillo una vez que conoces el descenso del gradiente estocástico y por lotes: en cada paso, en lugar de calcular los gradientes basados en el conjunto de entrenamiento completo (como en el GD por lotes) o basados en una sola instancia (como en el GD estocástico), el GD de minibatería calcula los gradientes en pequeños conjuntos aleatorios de instancias\n",
    "\n",
    "El progreso del algoritmo en el espacio de parámetros es menos errático que con el GD estocástico, especialmente con minilotes bastante grandes. Como resultado, el GD en minilote terminará caminando un poco más cerca del mínimo que el GD estocástico, pero puede ser más difícil para él escapar de los mínimos locales (en el caso de problemas que sufren de mínimos locales, a diferencia de la regresión lineal con la función de costo de MSE). La figura 4-11 muestra los caminos tomados por los tres algoritmos de descenso de gradiente en el espacio de parámetros durante el entrenamiento. Todos terminan cerca del mínimo, pero el camino del lote GD en realidad se detiene en el mínimo, mientras que tanto el GD estocástico como el GD de minilote continúan caminando. Sin embargo, no olvides que el GD por lotes toma mucho tiempo para dar cada paso, y el GD estocástico y el GD de mini-lote también alcanzarían el mínimo si usaras un buen horario de aprendizaje.\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0411.png)\n",
    "\n",
    "<center>(Figura 4-11. Rutas de descenso de gradiente en el espacio de parámetros)</center>\n",
    "\n",
    "La tabla 4-1 compara los algoritmos que hemos discutido hasta ahora para la regresión lineal⁠ (recuerde que m es el número de instancias de entrenamiento y n es el número de características).\n",
    "\n",
    "_Tabla 4-1. Comparación de algoritmos para regresión lineal_\n",
    "\n",
    "| **Algoritmo**       | **m grande** | **Soporte fuera del núcleo** | **n grande** | **Hiperparámetros** | **Requiere escalado** | **Scikit-Learn**    |\n",
    "|-----------------|----------|--------------------------|----------|-----------------|-------------------|-----------------|\n",
    "| Ecuación normal | Rapido   | No                       | Lento    | 0               | No                | N/A             |\n",
    "| SVD             | Rapido   | No                       | Lento    | 0               | No                | LinearRegresion |\n",
    "| Batch GD        | Lento    | No                       | Rapido   | 2               | Yes               | N/A             |\n",
    "| SGD             | Rapido   | Si                       | Rapido   | >=2             | Yes               | SGDRegressor    |\n",
    "| Mini-batch GD   | Rapido   | Si                       | Rapido   | >=2             | Yes               | N/A             |\n",
    "\n",
    "</br></br>\n",
    "Casi no hay diferencia después del entrenamiento: todos estos algoritmos terminan con modelos muy similares y hacen predicciones exactamente de la misma manera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfc0c9",
   "metadata": {},
   "source": [
    "## Regresión Polinómica\n",
    "\n",
    "**¿Qué pasa si tus datos son más complejos que una línea recta?** Sorprendentemente, puedes usar un modelo lineal para ajustar los datos no lineales. Una forma sencilla de hacer esto es agregar poderes de cada característica como nuevas características, y luego entrenar un modelo lineal en este conjunto extendido de características. Esta técnica se llama **regresión polinómica**.\n",
    "\n",
    "Echemos un vistazo a un ejemplo. \n",
    "En primer lugar, generaremos algunos datos no lineales (ver Figura 4-12), basados en una ecuación cuadrática simple, que es una ecuación de la forma **y = ax² +bx + c**, más algo de ruido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550a4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5a77a",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0412.png)\n",
    "\n",
    "_Figura 4-12. Conjunto de datos no lineales y ruidoso generado_\n",
    "\n",
    "Es evidente que una línea recta nunca se ajustará correctamente a estos datos. Entonces, usemos la clase `PolynomialFeatures` de Scikit-Learn para transformar nuestros datos de entrenamiento, agregando el cuadrado (polinomio de segundo grado) de cada característica en el conjunto de entrenamiento como una nueva característica (en este caso solo hay una característica):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce471cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75275929])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef09e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75275929,  0.56664654])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b67d7c",
   "metadata": {},
   "source": [
    "`X_poly` ahora contiene la característica original de `X` más el cuadrado de esta característica. \n",
    "\n",
    "Ahora podemos ajustar un modelo de `LinearRegression` a estos datos de entrenamiento extendidos (Figura 4-13):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ca058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.78134581]), array([[0.93366893, 0.56456263]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a295c8b",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0413.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf18756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
