{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2047f37",
   "metadata": {},
   "source": [
    "# Capítulo 4: Modelos de entrenamiento\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Hasta ahora hemos tratado los modelos de aprendizaje automático y sus algoritmos de entrenamiento principalmente como cajas negras. \n",
    "\n",
    "Es posible que te haya sorprendido lo mucho que puedes hacer sin saber nada sobre lo que hay bajo el capó: optimizaste un sistema de regresión, mejoraste un clasificador de imágenes de dígitos e incluso construiste un clasificador de spam desde cero, todo sin saber cómo funcionan realmente. \n",
    "\n",
    "De hecho, en muchas situaciones no es necesario conocer los detalles de la implementación.\n",
    "\n",
    "Sin embargo, tener una buena comprensión de cómo funcionan las cosas puede ayudarte a encontrar rápidamente el modelo apropiado, el algoritmo de entrenamiento adecuado para usar y un buen conjunto de hiperparámetros para tu tarea. \n",
    "\n",
    "Comprender lo que hay \"bajo el capó\" también le ayudará a depurar los problemas y a realizar el análisis de errores de manera más eficiente. \n",
    "\n",
    "Por último, la mayoría de los temas tratados en este capítulo serán esenciales para comprender, construir y entrenar las redes neuronales (discutido en la Parte II de este libro).\n",
    "\n",
    "En este capítulo comenzaremos por ver el modelo de regresión lineal, uno de los modelos más simples que existen. Discutiremos dos formas muy diferentes de entrenarlo:\n",
    "\n",
    "* Usando una ecuación de \"**forma cerrada**\"⁠ que **calcule directamente** los parámetros del modelo que mejor se ajustan al modelo al conjunto de entrenamiento (es decir, los parámetros del modelo que minimizan la función de costo sobre el conjunto de entrenamiento).\n",
    "\n",
    "\n",
    "* Utilizando un enfoque de optimización **iterativo** llamado **descenso de gradiente** (GD) que ajusta **gradualmente** los parámetros del modelo para minimizar la función de costo sobre el conjunto de entrenamiento, eventualmente convergiendo con el mismo conjunto de parámetros que el primer método. Veremos algunas variantes de descenso de gradiente que usaremos una y otra vez cuando estudiemos las redes neuronales en la Parte II: **GD de lote, GD de minibate y GD estocástico**.\n",
    "\n",
    "\n",
    "A continuación, veremos la **regresión polinómica**, un modelo más complejo que puede adaptarse a conjuntos de datos **no lineales**. \n",
    "\n",
    "Dado que este modelo tiene **más parámetros que la regresión lineal**, es más propenso a **sobreajustar** los datos de entrenamiento. \n",
    "\n",
    "Exploraremos cómo detectar si este es el caso o no utilizando **curvas de aprendizaje**, y luego veremos varias **técnicas de regularización** que pueden reducir el riesgo de sobreadaptar el conjunto de entrenamiento.\n",
    "\n",
    "Por último, examinaremos dos **modelos más que se utilizan** comúnmente para las tareas de **clasificación**: **regresión logística y regresión softmax**.\n",
    "\n",
    "\n",
    "### ------------------------ ADVERTENCIA ------------------------\n",
    "\n",
    "Habrá bastantes ecuaciones matemáticas en este capítulo, utilizando nociones básicas de álgebra lineal y cálculo. \n",
    "\n",
    "Para entender estas ecuaciones, necesitarás saber qué son los vectores y las matrices; cómo transponerlos, multiplicarlos e invertirlos; y qué son las derivadas parciales. \n",
    "\n",
    "Si no estás familiarizado con estos conceptos, consulte los tutoriales introductorios de álgebra lineal y cálculo disponibles como cuadernos Jupyter en el material complementario en línea. \n",
    "\n",
    "Para aquellos que son realmente alérgicos a las matemáticas, aún así deben pasar por este capítulo y simplemente omitir las ecuaciones; con suerte, el texto será suficiente para ayudarle a entender la mayoría de los conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dc0d0",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "</bre/>\n",
    "En el capítulo 1 se analizó un modelo de **regresión simple** para comprobar la satisfacción con la vida:\n",
    "\n",
    "**<center>satif_vida = θ0 + θ1 × GDP_per_capita</center>**\n",
    "\n",
    "Es solo una función lineal (línea recta) con una sola característica de entrada y dos parámetros:\n",
    "\n",
    "- Característica: `GDP_per_capita`\n",
    "- Parámetros: `θ0` y `θ1`\n",
    "\n",
    "Un modelo lineal funciona haciendo una predicción calculando una suma ponderada de características de entrada más una constante (sesgo o término de intersección).\n",
    "\n",
    "\n",
    "### Ecuación 4-1: Predicción del modelo de Regresión Lineal\n",
    "</br>\n",
    "\n",
    "**<center>y = θ0 + θ1X1 + θ2X2 + ... + θnXn</center>**\n",
    "\n",
    "* **y** = valor predicho\n",
    "* **n** = número de características\n",
    "* **Xi** = valor de la primera característica\n",
    "* **θj** = parámetro del modelo j, incluido el término de sesgo θ0 y los pesos de características θ1, θ2, ..., θn.\n",
    "\n",
    "(Y esto se puede escribir de forma más concisa usando una forma vectorizada).\n",
    "\n",
    "\n",
    "### Ecuación 4-2: Predicción del modelo de Regresión Lineal (FORMA VECTORIAL)\n",
    "</br>\n",
    "\n",
    "**<center>y = hθ(X) = θ*X</center>**\n",
    "\n",
    "* **hθ** = función de hipótesis, que usa los parámetros del modelo θ\n",
    "* **θ** = número de características\n",
    "* **X** = vector de parámetros del modelo, que contiene el término de sesgo θ0 y los pesos de características θ1 a θn.\n",
    "* **θ*X** = producto punto de los vectores θ y X, que es igual a θ0x0 + θ1x1 + θ2x2 + ... θnxn\n",
    "\n",
    "</br></br>\n",
    "**---------------------- NOTA ------------------------**\n",
    "\n",
    "En el aprendizaje automático, los vectores a menudo se representan como vectores de columna (row-vector), que son matrices 2D con una sola columna.\n",
    "\n",
    "Si **θ** y **x** son vectores de columna, entonces la predicción es `y = θ^T*X`, donde `θ^T` es la transposición de `θ` (un vector fila en lugar de un vector columna) y `θ^T*X` es la multiplicación de la matriz de `θ^T` y `X`.\n",
    "\n",
    "Por supuesto, es la misma predicción, excepto que ahora se representa como una matriz de una sola celda en lugar de un valor escalar. \n",
    "\n",
    "**------------------------------------------------------**\n",
    "</br>\n",
    "\n",
    "Vale, ese es el modelo de regresión lineal, pero ¿cómo lo entrenamos? Bueno, recuerda que entrenar un modelo significa establecer sus parámetros para que el modelo se ajuste mejor al conjunto de entrenamiento. \n",
    "\n",
    "Para este propósito, primero necesitamos una medida de qué tan bien (o mal) se ajusta el modelo a los datos de entrenamiento. \n",
    "\n",
    "En el capítulo 2 vimos que la medida de rendimiento más común de un modelo de regresión es el **error cuadrado medio** de la raíz. \n",
    "\n",
    "Por lo tanto, para entrenar un modelo de regresión lineal, necesitamos encontrar el valor de θ que minimice el **RMSE**. \n",
    "\n",
    "En la práctica, es más sencillo minimizar el error medio al cuadrado (MSE) que el RMSE, y conduce al mismo resultado (porque el valor que minimiza una función positiva también minimiza su raíz cuadrada).\n",
    "\n",
    "**---------------------- ADVERTENCIA ------------------------**\n",
    "\n",
    "Los algoritmos de aprendizaje a menudo optimizarán una función de pérdida diferente durante el entrenamiento que la medida de rendimiento utilizada para evaluar el modelo final. Esto se debe generalmente a que la función es más fácil de optimizar y/o porque tiene términos adicionales necesarios solo durante el entrenamiento (por ejemplo, para la regularización). Una buena métrica de rendimiento está lo más cerca posible del objetivo comercial final. Una buena pérdida de entrenamiento es fácil de optimizar y está fuertemente correlacionada con la métrica. Por ejemplo, los clasificadores a menudo se entrenan utilizando una función de costo como la pérdida de registro (como verá más adelante en este capítulo), pero se evalúan utilizando precisión/retirada. La pérdida de registro es fácil de minimizar, y hacerlo generalmente mejorará la precisión/retirada.\n",
    "\n",
    "**----------------------------------------------------------------**\n",
    "\n",
    "El MSE de una hipótesis de regresión lineal hθ en un set de entrenamiento ´X´ se calcula utilizando la ecuación 4-3.\n",
    "\n",
    "\n",
    "### Ecuación 4-3: Función de coste de MSE para un modelo de regresión lineal\n",
    "</br>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mrow>\n",
    "    <mtext>MSE</mtext>\n",
    "    <mrow>\n",
    "      <mo>(</mo>\n",
    "      <mi mathvariant=\"bold\">X</mi>\n",
    "      <mo>,</mo>\n",
    "      <msub><mi>h</mi> <mi mathvariant=\"bold\">θ</mi> </msub>\n",
    "      <mo>)</mo>\n",
    "    </mrow>\n",
    "    <mo>=</mo>\n",
    "    <mstyle scriptlevel=\"0\" displaystyle=\"true\">\n",
    "      <mfrac><mn>1</mn> <mi>m</mi></mfrac>\n",
    "    </mstyle>\n",
    "    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>\n",
    "    <msup><mrow><mo>(</mo><msup><mi mathvariant=\"bold\">θ</mi> <mo>⊺</mo> </msup><msup><mi mathvariant=\"bold\">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup>\n",
    "  </mrow>\n",
    "</math>\n",
    "\n",
    "MSE(X,hθ) = 1/m * SUM i to m (θT * X^i - y^i)^2\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/qsT73Cj/Captura-de-pantalla-2023-08-18-a-las-20-00-06.png\" alt=\"Captura-de-pantalla-2023-08-18-a-las-20-00-06\" border=\"0\"></a><br /><a target='_blank' href='https://imgbb.com/'></a><br />\n",
    "\n",
    "La mayoría de estas anotaciones se presentaron en el capítulo 2. La única diferencia es que escribimos hθ en lugar de solo h para dejar claro que el modelo está parametrizado por el vector θ. Para simplificar las anotaciones, solo escribiremos MSE(θ) en lugar de MSE(X, hθ).\n",
    "\n",
    "### Ecuación normal\n",
    "\n",
    "Para encontrar el valor de θ que minimiza el MSE, existe una solución de forma cerrada, en otras palabras, una ecuación matemática que da el resultado directamente. \n",
    "\n",
    "Esto se llama **ecuación normal** y **nunca vamos a calcular en problemas complicados** porque es increiblemente lento (imposible computacionalmente) y tendremos que saber como acercarnos a la solución sin calcular esto.\n",
    "\n",
    "\n",
    "### Ecuación 4-4: Ecuación Normal\n",
    "\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/51vHrfN/Captura-de-pantalla-2023-08-18-a-las-20-06-44.png\" alt=\"Captura-de-pantalla-2023-08-18-a-las-20-06-44\" border=\"0\"></a>\n",
    "\n",
    "Donde:\n",
    "\n",
    "* **θ** = valor de θ que minimiza la función de coste.\n",
    "* **y** = vector de valores onjetivo que contiene **y(1)** a **y(m)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b6db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos algunos datos con forma lineal para probar que la ecuación funciona\n",
    "# Conjunto de datos lineal generado al azar.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "m = 100  # número de instancias\n",
    "X = 2 * np.random.rand(m, 1)  # vector columna\n",
    "y = 4 + 3 * X + np.random.randn(m, 1)  # vector columna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42288497",
   "metadata": {},
   "source": [
    "![data_points_lineal](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca7379",
   "metadata": {},
   "source": [
    "Ahora vamos a calcular **θ^** usando la **Ecuación Normal**. \n",
    "\n",
    "Usamos la función `inv()` del módulo de álgebra lineal de NumPy  (`np.linalg`) para hacer la inversa de la matriz, y el método `dot()` para la multiplicación de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348d63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "X_b = add_dummy_feature(X)  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c2ce0",
   "metadata": {},
   "source": [
    "### -------------------------- NOTA --------------------------\n",
    "\n",
    "El operador `@` realiza la multiplicación de matrices. \n",
    "\n",
    "Si **A** y **B** son matrices NumPy, entonces **A @ B** es equivalente a `np.matmul(A, B)`. \n",
    "\n",
    "Muchas otras bibliotecas, como TensorFlow, PyTorch y JAX, también admiten el operador **@**. Sin embargo, no puede usar @ en arreglos puros de Python (es decir, listas de listas).\n",
    "\n",
    "### -------------------------------------------------------------\n",
    "\n",
    "La función que usamos para generar los datos es `y = 4+3x1 + ruido_gaussiano`.\n",
    "Veamos que encontró la ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0bd692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba551d4",
   "metadata": {},
   "source": [
    "Habríamos esperado **(θ0 = 4** y **θ1 = 3)** en lugar de **(θ0 = 4.215** y **θ1 = 2.770)**. \n",
    "\n",
    "Lo suficientemente cerca, pero el ruido hizo imposible recuperar los parámetros exactos de la función original. \n",
    "\n",
    "**Cuanto más pequeño y ruidoso sea el conjunto de datos, más difícil se vuelve**.\n",
    "\n",
    "Ahora podemos hacer predicciones usando **θ^**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = add_dummy_feature(X_new)  # agregar x0 = 1 a cada instancia\n",
    "y_predict = X_new_b @ theta_best\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57611120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABC7UlEQVR4nO3deXwUVbr/8W8SSAAlUZTVDluIgAwi4sIigsgyDK7jDwYXRoegiDCKOiqLIiISGB1GBRRwcnEbERdwGR2U641wuYgDCAwyimGJ0IIyKiaAGkhSvz/OJBCydSdV1VXdn/fr1a+2KpXuU3TKevqc5zwnzrIsSwAAAC6Jj3QDAABAbCH4AAAAriL4AAAAriL4AAAAriL4AAAAriL4AAAAriL4AAAAriL4AAAArqoT6QacqLi4WHv37lXDhg0VFxcX6eYAAIAQWJalgwcPqkWLFoqPr7pvw3PBx969e5WamhrpZgAAgBrYs2ePAoFAlcd4Lvho2LChJNP45OTkCLcGAACEIj8/X6mpqaX38ap4LvgoGWpJTk4m+AAAwGdCSZkg4RQAALiK4AMAALiK4AMAALiK4AMAALiK4AMAALiK4AMAALiK4AMAALiK4AMAALiK4AMAALiK4AMAALiK4AMAEJOCQSk72zzDXWEHH6tWrdLll1+uFi1aKC4uTm+88UaZny9dulSDBg3S6aefrri4OG3atMmmpgIAYI+sLKlVK6lfP/OclRXpFsWWsIOPw4cPq0uXLpo7d26lP+/Vq5dmzpxZ68YBAGC3YFC65RapuNhsFxdLo0fTA+KmsFe1HTx4sAYPHlzpz0eMGCFJys3NrXGjAABwSk7OscCjRFGRtH27FAhEpk2xJuzgw24FBQUqKCgo3c7Pz49gawAA0S49XYqPLxuAJCRI7dpFrk2xJuIJp5mZmUpJSSl9pKamRrpJAIAoFghICxeagEMyzwsW0OvhpogHHxMnTlReXl7pY8+ePZFuEgAgymVkSLm5ZrZLbq7ZhnsiPuySlJSkpKSkSDcDABBjAgF6OyIl4j0fAAAgtoTd83Ho0CFt3769dHvXrl3atGmTGjVqpJYtW+r777/X7t27tXfvXknStm3bJEnNmjVTs2bNbGo2AADwq7B7PtavX6+uXbuqa9eukqS77rpLXbt21ZQpUyRJb731lrp27aohQ4ZIkoYPH66uXbtq/vz5NjYbAAD4VZxlWVakG3G8/Px8paSkKC8vT8nJyZFuDgAACEE4929yPgAAgKsIPgAAgKsIPgAAgKsIPgAAiELBoCmi5sUF8wg+AACIMllZUqtWUr9+5jkrK9ItKovgAwCAKBIMSrfccmzhvOJiafRob/WAEHwAABBFcnLKrtgrSUVF0nH1QSOO4AMAgCiSni7Fn3B3T0iQ2rWLTHsqQvABAEAUCQSkhQtNwCGZ5wULvLWIXsRXtQUAAPbKyJAGDTJDLe3aeSvwkAg+AACISoGA94KOEgy7AAAAVxF8AAAAVxF8AAAAVxF8AAAAVxF8AAAAVxF8AAAAVxF8AAAAVxF8AAAAVxF8AAAAVxF8AAAAVxF8AABQjWBQys42z6g9gg8AAKqQlSW1aiX162ees7Ii3SL/I/gAAKASwaB0yy1ScbHZLi6WRo+mB6S2CD4AAKhETs6xwKNEUZFZqr4iDM+EhuADAIBKpKdL8SfcKRMSpHbtyh/L8EzoCD4AIIbwzTw8gYC0cKEJOCTzvGCB2X88hmfCQ/ABADGCb+Y1k5Eh5eaaoC0312yfKNzhGTv4OZAk+ACAGMA389oJBKS+fcv3eJQIZ3jGDn4PJAk+ACAGROKbeSwJdXjGDtEQSNaJdAMAAM4r+WZ+fADi5DfzWJSRIQ0aZAK6du2cCTykqgNJp97TbvR8AEAMcPObeSyrbnjGDm4P8Tgh7OBj1apVuvzyy9WiRQvFxcXpjTfeKPNzy7I0depUtWjRQvXr11ffvn21detWu9oLAKihUBIn4X3REEiGHXwcPnxYXbp00dy5cyv8+R//+EfNnj1bc+fO1bp169SsWTMNGDBABw8erHVjAQC148Y3czjP74FknGVZVo1/OS5Oy5Yt01VXXSXJ9Hq0aNFC48eP13333SdJKigoUNOmTTVr1iyNHj262tfMz89XSkqK8vLylJycXNOmAQAAF4Vz/7Y152PXrl36+uuvNXDgwNJ9SUlJ6tOnj9asWVPh7xQUFCg/P7/MAwAARC9bg4+vv/5aktS0adMy+5s2bVr6sxNlZmYqJSWl9JGammpnkwAAgMc4MtslLi6uzLZlWeX2lZg4caLy8vJKH3v27HGiSQAAwCNsrfPRrFkzSaYHpHnz5qX79+/fX643pERSUpKSkpLsbAYAAPAwW3s+2rRpo2bNmmnFihWl+44cOaKVK1eqZ8+edr4VAADwqbB7Pg4dOqTtx9Xj3bVrlzZt2qRGjRqpZcuWGj9+vGbMmKH09HSlp6drxowZatCgga677jpbGw4AAPwp7OBj/fr1uuSSS0q377rrLknSjTfeqGeffVb33nuvfvrpJ9122206cOCALrzwQr3//vtq2LChfa0GAAC+Vas6H06gzgcAAP4TsTofAAC4KRg0VT79tKIrCD4AAD6VlSW1aiX162ees7Ii3SKEiuADAOA7waB0yy3HlpYvLpZGj/ZvD4irPTg//CD9858uvFHlCD4AAL6Tk3Ms8ChRVCQdNxnTN1zrwfnxR2nWLKlNG2noUKmw0KE3qh7BBwDAd9LTpfgT7mAJCVK7dpFpT0250oNz9Kg0f775x5kwwfR81K0rffWVjW8SHoIPAIDvBALSwoUm4JDM84IFZr+fONqDU1wsvfSS1LGjNGaMtG+f1Lq19Pzz0ubNppslQmwtrw4AgFsyMqRBg8yNul07/wUe0rEenOMDkFr34FiW9O670uTJJsiQpKZNpfvvN90siYm1arMd6PkAAPhWICD17evPwEOqvAdHqmEC6urV0sUXS5ddZgKP5GRp+nQToY0b54nAQyL4AADAcVXNZsnIkHJzzc9zc82+sBNQN2+WhgyRevc2AUi9etK990q7dpkekJNPtvFsao/gAwAAB4Uym6WkB0cKMwF1+3bpuuukc84xQy0JCeYXtm83M1saNXLgjGqP4AMAIIlqoU4IdzZLyAmoe/dKt95qkkkXLzb7hg+XPvvMzGw54wxbz8NuBB8AEIZovUFTLdQZ4c5mqXYK8fffS/fdZ3YsWGBqdQweLH3yiQlC0tNtPwcnEHwAQIii9QYdbdVCvSTceiSVTiE+9bA0Y4bUtq30xz9KP/0k9ewprVxphlu6dnX2RGxG8AEAIYjmG3Q0VQv1mprUIymTgPrFEWX8PE9KSzOJo3l5UufO0ttvH5vZ4kPU+QCAEFR1g/brNM8SjtSaQKma1CMJNC9S4MPF0sgpZsaKZHo9pk2Trr22fHeKz/i79QDgkmgp512RaKkW6mUh1yOxLNOrcc450ogRJvBo1kyaN88kk15/ve8DD4ngAwBCEu036BNrTWRkRLpFMWjlSqlXL+mKK6RPP1Uw+Sxlj/qrgit3SLfd5pkCYXaIsyzLinQjjpefn6+UlBTl5eUpOTk50s0BgDKCQX+X84YHbdwoTZokLV9utuvXV1bfF3TLe79WcXGc4uNN4Ov1gDCc+zfBBwAAkZCTIz3wgLRkidmuU0e65RYFR05RqwualsvByc31dsAbzv2bYRcAANxUMnWqY0cTeMTFmVyOzz+X5s1TTn7TqJ99xGwXAADc8N130syZ0ty50s8/m32XXSY98oh09tmlh8XC7CN6PgAAcNKhQ2Zl2bZtpcceM4FHyQJwb79dJvCQoj+5WaLnAwAAZxQUmChi+nRp/36zr0sXKTNT+uUvzXBLJWpSG8RPCD4AALBTUZH0179KU6ZIX35p9qWlmSBk2LCQ63QEAtEXdJQg+AAAwA6WJb35pnT//dLWrWZfixYmCBk5UqpbN7Lt8xCCDwCA5wWDZmZqerpHewOys6WJE6WPPzbbp54qTZggjRsnNWgQ2bZ5EAmnAABP8/RqwuvXSwMHmsZ9/LEJNCZNknbulO69l8CjEgQfAADP8uxqwp9/Lg0dKp1/vrRihRlSGTdO2rHDTJ095ZQIN9DbCD4AAJ5V1WrCEbFnjzRqlNSpk/Taa2bGyogR0rZt0pw5ZhE4VIucDwCAZ3mm4Na335opsvPmmSm0klkAbvp0qXNnlxvjf/R8AAA8K+IFtw4elB56yBQImz3bBB59+khr1piZLQQeNULPBwDA0yJScOvnn6X5803+xrffmn3nnivNmGESTKsoEIbqOdLzcfDgQY0fP16tWrVS/fr11bNnT61bt86JtwIAxIBAQOrb14XAo7BQWrRIat9euvNOE3iceab0yivSunUmCiLwqDVHgo9Ro0ZpxYoVeuGFF7RlyxYNHDhQ/fv311dffeXE2wEAUDuWJS1daoZRRo6Udu+WzjhDeuYZUzBs6NCQK5OienGWZVl2vuBPP/2khg0b6s0339SQIUNK959zzjm67LLLNH369Cp/Pz8/XykpKcrLy1NycrKdTQMAeFxEiol98IEpEFbSQ9+okanVcdttUv36LjXC/8K5f9ue81FYWKiioiLVq1evzP769etr9erV5Y4vKChQQUnmsEzjAQCxJyvrWE2P+HiTaJqR4eAbrltngo4PPjDbJ50k3XWXdPfdUkqKg28M2/uQGjZsqB49eujhhx/W3r17VVRUpBdffFEff/yx9u3bV+74zMxMpaSklD5SU1PtbhIAwONcLSb22WfSNddIF1xgAo/EROn2201V0mnTCDxc4MgA1gsvvCDLsnTGGWcoKSlJTz75pK677jollMyVOs7EiROVl5dX+tizZ48TTQIAeJgrxcR27zb5HL/4hcnviI+XbrzRFAh74gmpSRMb3wxVcWSqbVpamlauXKnDhw8rPz9fzZs3129+8xu1adOm3LFJSUlKSkpyohkAAJ9wtJjY/v1miuzTT0tHjph9V19tCoSddZYNb4BwOZq6e9JJJ6l58+Y6cOCA3nvvPV155ZVOvh0AwKccKSaWny89+KCUlmZ6No4ckS65RFq71vR81CLwCAbNQrYRX2PGp2yf7SJJ7733nizLUvv27bV9+3bdc889SkpK0urVq1W3bt0qf5fZLgAQu4JBG4qJ/fyzKYOemSl9953Z162b2e7fv9Z1OlxPjPWJcO7fjvR85OXlaezYserQoYN++9vf6qKLLtL7779fbeABAIhtxxcTC7t3obBQ+stfzBjOH/5gAo/27c0CcOvWSQMG1Drw8NIqu37ufXEk+Bg2bJh27NihgoIC7du3T3PnzlUK2cMAgBBlZUmtWkn9+pnnrKwqDi4ull591SSS3nyzuRsHAuaXPv3UzGyxqSqpV1bZDevfx4McGXapDYZdACC2BYPmhnpi8mlu7glDMZYlrVhhCoJt2GD2nX662R4zRjqh3pSrbXOQF9pQkYgPuwAAUFMh9S6sXWu+9g8aZAKPk082yaU7dpg1WRwIPCQPrLIr7/S+1Aar2gIAPKXKabdbt0qTJ5vl7CVTIGzsWFOptHFjV9oXkVV2j+PotGSX0PMBAJDknQTGCnsXHvlWgck3moXf3nzT3H1HjjTdALNnuxZ4HN9GV1bZreS9I937UlvkfAAAPDl9NBiUtv/je7X72+MKvDhTOnrU/OCaa6SHH5Y6drTtfVxfzM4GtkxLtlE492+CDwCIcZ5MYMzLkx57TPrzn6XDh82+/v1NpdLzz7ftbbwYdPkVCacAgJB5KoHxp5+kRx+V2rY15c8PHz62ANyKFbYGHl6q2RFrCD4AIMaVJDAez/UExqNHTbdDerp0773S99+bYZWlS4/NbLGZp4KuGEPwAQAxLqIJjMXF0pIlUqdOptvhq6+kli2lRYukLVvMAnA2FQg7kSeCrhhF8AHAt7wyOyMaZGSYHI/sbPPseN6DZUnLl0vnnScNH266IRo3NgvAffGFdNNNx6Ihh0TDrBG/IuEUgC+RKOhja9aYuhyrVpnthg2le+6Rxo83/+0yr80a8StmuwCIap6cnYHq/fOfpkDY3/5mtpOSpHHjpAkTTFl0+BqzXQBENRIFwxPx4amdO6UbbpDOOccEHgkJ0qhR5oN87DECjxhE8AHAd0gUDF24q5/aGqjs22dKn7dvL/31rybPY+hQUyL9mWek1FQb3gR+RPABwHdIFAxNuHUsbFum/YcfzMqy7dpJTz0lFRaaxVDWr5deecUEIzaKeM8OwkbwAcCXXJ+d4UPhDE/ZUnDrxx+lWbOkNm2kzEyz3b27+ZCWL5e6davxuVTGtoAJriL4AOBbkVzcyw/CGZ6qVR7N0aPS/PnmhSdMMD0fnTpJb7xhZrb07VuzE6gGFUr9i+ADAKJUOMNTNcqjKS6WXnrJVCIdM8bkeLRuLT3/vLR5s3TllY4VCJNIPPYzgg8AiGKhDk+FlUdjWdI770jnnitdf720Y4fUtKk0Z460bZs0YoTjBcIkEo/9rE6kGwAAcFYgENrQVEaGyQutsuDW6tWmQNjq1WY7OdmsxXLHHdLJJ9va7uqUBEyjR5seDxKP/YMiYwCA6m3ebGawvPuu2a5XT7r9dum++xT8sZFyckxPRCRu/FQo9QaKjAEA7LF9u3TddaZA2Lvvmu6F0aPN/lmzlLWsUeTqiPwHicf+Q/ABAChv716TRNqxo7R4sdk3fLj02WdmZssZZ0Sujgh8j+ADAHDMgQNmumy7dibIKCyUBg+WPvnEBCHp6aWHul5HBFGDhFMAsEkwqIjmPtTK4cPSk0+aImF5eWZfz56mWNjFF1f4KyWzTU5c4C/cOiK++7dCrdHzAQA28O2QwpEj0rx5UlqaSSjNy5M6d5beftvMaKkk8JBcqCOCqMVsFwCopWDQBBwn9gDk5nr4W31RkRlGmTJF2rXL7GvbVpo2Tbr22vKRQhVCnW2SlVV+Wixl8aNHOPdvhl0AoJZ8NaRgWWZZ+0mTpE8/NfuaNZMeeMAsc5+YGPZL2lpHBDGB4AMAaimc3IeIWrnSFAj76COzfcop0n33Sb//vXTSSa40IdRABdGNnA8AqKWwSpNHwsaNZsZK374m8Khf38xo2bnTPLsUeAAl6PkAABt4ckghJ8cMpyxZYrbr1JFuvtnsa97clSb4egYQHEPPBwDYxDOVNkuKanTsaAKPuDizANznn0tPPeVa4OHbGUBwnO3BR2Fhoe6//361adNG9evXV9u2bTVt2jQVn5iNBQCw13ffSffcY7oZnnnGZL1edpm0aZP04otmOq1LKCqGqtg+7DJr1izNnz9fzz33nDp16qT169frd7/7nVJSUnTHHXfY/XYAgEOHpMcflx59VMrPN/t69zYFwnr1ikiTfDUDCK6zPfj46KOPdOWVV2rIkCGSpNatW2vx4sVav3693W8FALGtoMBkuk6fLu3fb/Z16WKCjl/+0gy3RIhvZgAhImwfdrnooov0wQcf6IsvvpAkbd68WatXr9avfvUru98KACLGidVZQ1ZUJD3/vNS+vVnWfv9+M6SyeLFZg2Xw4IgGHpIPZgAhomzv+bjvvvuUl5enDh06KCEhQUVFRXrkkUd07bXXVnh8QUGBCgoKSrfzS7oMAcCjsrKO5TPEx5ubrCuVOi1LevNN6f77pa1bzb4WLUyV0pEjpbp1XWhEaIJBUzD1o4/MsjGemQEET7C952PJkiV68cUX9dJLL+mTTz7Rc889p8cee0zPPfdchcdnZmYqJSWl9JGammp3kwDANhFLpMzOlnr0kK6+2gQep56q4MR5yv7LDgWHjPZU4HH8LJfu3aUdOwg8UJbta7ukpqZqwoQJGjt2bOm+6dOn68UXX9Tnn39e7viKej5SU1NZ2wWAJ2Vnm5tqRfv79nXgDTdsMKXQ33/fbDdoII0fr6xmk3XL+Abu975Uw5fr3MAWEV3b5ccff1T8CQsSJSQkVDrVNikpSUlJSXY3A0AMcbOQlWuJlNu2meGV114z23Xrmi6WyZMVLGymW1qV730ZNCjyN3hmuTgvGgq32T7scvnll+uRRx7RO++8o9zcXC1btkyzZ8/W1VdfbfdbAYDrhawcT6Tcs8cs8Napkwk84uKkESNMMDJnjtSsWZU3+EgrCc6OxywX+0RL4Tbbh10OHjyoBx54QMuWLdP+/fvVokULXXvttZoyZYoSQ1gtMZxuGwCxLZJd/KEuIx+yb781U2TnzTNTaCXpiivMNNrOncu9t5eHNrKyTE9MUdGx4MwLQ0J+5/XPPZz7t+3BR20RfAAIlev5F044eFCaPVv605/Mf0tSnz4mEOnRo9Jf8/oN3vbgDJ7/e49ozgcAuMXXhax+/lmaP1965BHT6yFJXbuaoGPgwGrrdHhyIbvjBALea5Pf+frv/QQsLAfAt3xZyKqwUFq0yBQIu/NOE3iceaZZAG79ehNRhFggLNSF7CJaEA228eXfeyUYdgHge77o4rcsadkyafJks7qsJJ1xhjR1qnTTTWa5ewdEqiBaNMzI8Cqv/r2T8wEAXvLBB9LEidK6dWa7USNTu+O226T69R1720glKEasAiwiKpz7N8MuAOCUdeuk/v3NY9066aSTpAcekHbulO6+29HAQ6q65oZTIlYBFr5CwikA2O2zz0yBsKVLzXZionTrrWbIpUkT15oRiQRFiowhFPR8AIBddu82C7z94hcm8IiPl2680RQIe+IJVwMPKTIJihQZQygIPgCgtvbvl8aPN3feRYvMV/+rr5a2bJGefVZq3TpiTcvIMDke2dnm2enci2iakQHnkHAKADWVn2+Kg82eLR06ZPZdcomp1XHhhZFtW4R5dUYGnEORMQBw0s8/S089Jc2YIX33ndnXrZsJOvr3D7lORzSjyBiqQvABAKEqLJSee87U5iiZvtG+valS+utfE3QAISL4AIDqFBdLr79upslu22b2BQLSQw9Jv/2tYwXCgGjFFQPUANUbY4RlSStWmIJgGzaYfaefbrbHjJHq1Yts+wCfYrYLEKasLFM1sl8/85yVFekWwRFr15oPedAgE3icfLL04IPSjh1mTRYCD6DGmO0ChCFS5arhoq1bTTGwN98024mJ0tixpjx648aRbRvgYZRXBxwSiXLVcElurikI1rmzCTzi403BsJwcM5WWwAOwDTkfQBgiUa4aDvvmGzNbZf586ehRs++aa6SHH5Y6doxs23yMvChUhZ4PIAxUb4wieXlm9kpamjRnjgk8+veX/vEPBR9/Tdlfd2QxtBoiLwrVIecDqAGqN/rYTz9Jc+dKM2dK339v9l1wgSkQ1q8fy8HXEnlRsYucD8BhgYDUty//M/WVo0dNJJGeLt17rwk8OnY0C8D9Z2YLy8HXHnlRCAXBB4DoVlwsLVkidepkIomvvpJatjQLwG3ZYhaA+09lUm6ctceqtggFwQdqJBg0q2TyjRCeZVnS8uXSeedJw4ebyKJxY7O0/RdfSDfddCx55z+4cdYeeVEIBcEHwhZLyWQEWT61Zo0ZFxs8WNq4UWrYUJo2zRQIu/12KSmpwl/jxmmPjAyT45GdbZ7JmcGJSDhFWGIpmYzEw+p5bjrlli2mQNjbb5vtpCRp3DhpwgRTFj1EJBQD4SPhFI6JlTFxEg+r56kesJ07pREjpC5dTOCRkCCNGmX+YB97LKzAQyKhGHAawQfCEitj4rESZNWUZ4Kzr782PRsdOkgvvmjyPIYONSXSn3lGSk11uUEAQkHwgbDEyph4rARZNRXx4OyHH8zKsmlp0rx5ZhrtoEHS+vXSK69I7du71BAANUHwgbDFQjJZrARZNRWx4OzHH6VZs6Q2bUxRsB9/lLp3N3+My5dL3bo53AAAdmBtF9RIIBD9N+KMDPNlmsTD8kqCs9GjTY+H48HZ0aMmqWTaNGnfPrOvUyezJssVV5TW6QDgD8x2AVBjjs8KKS6WXn5ZmjLFTJOVpNatTRBy3XXl6nQAiJxw7t/0fACoMcd6wCxLevddM21282azr2lT6f77TaZrYqIDbwrALQQfHua5GgqAG1avliZOVHD1LuUoXeknd1Bgwg3SHXdIJ58c6da5gmsf0c72hNPWrVsrLi6u3GPs2LF2v1VU81QNBcANmzdLQ4ZIvXsra/WZaqUv1U/ZavXjv5TVbHLMBB5c+4gFtud8/Pvf/1ZRUVHp9qeffqoBAwYoOztbffv2rfb3yfmIrSqigLZvNzkdixdLkoLxLdXK2qVi69h3I7///Yfak8G1Dz+LaIXTxo0bq1mzZqWPv/3tb0pLS1OfPn3sfquoFfEaCoAb9u6Vxowxy9r/J/DQ8OHKefb/ygQekr///sPpyeDaR6xwtM7HkSNH9OKLL2rkyJGKYypcyChwhah24IBZa6VdO2n+fKmw0CwA98kn0uLFSr8kEDV//+FWguXaR6xwNPh444039MMPP+imm26q9JiCggLl5+eXecQ6ClzBTa6t3Hv4sCkM1qaNKRT2009Sz57SypVmZkvXrpKi6+8/3J6MaDp3oCqO1vkYNGiQEhMT9XbJCpMVmDp1qh566KFy+2M556MEK2vCaa6s3HvkiFln5eGHpW++Mfs6d5ZmzDAJppX0ikbD339Nczii4dwRe8LJ+XAs+Pjyyy/Vtm1bLV26VFdeeWWlxxUUFKigoKB0Oz8/X6mpqQQfgAOOT3yUHE5uLCoyuRxTpki7dpl9bduaAmHXXlt+fCFKZWWVrwQbjUsSAJ4oMrZo0SI1adJEQ4YMqfK4pKQkJSUlOdUMAP9xYi/HXXdVPiRQq+DDsqS//c0UCNuyxexr1kx64AGzzH2MFQijTD9QniPBR3FxsRYtWqQbb7xRdepQxwyItIoSH//8ZzPicXzfZ62TG1etkiZOlNasMdunnCLdd5/0+99LJ51Uixf2t1hYCwkIhyP9nv/93/+t3bt3a+TIkU68PIAwVZb4ePfdNiU3btxoZqz06WMCj/r1zYyWnTvNcw0DD9eSYQG4ioXlgBhQVeKjVIshgZwcM5yyZInZrlNHuvlms69581q12ZVk2BBR7hyoXkSLjAHwnqqmcAYCUt++Yd5US8ZxOnY0gUdcnHT99dLnn0tPPVXrwCPc+hhOotw5YD96PoAYUuspnN99J82cKc2dK/38s9l32WXSI49IZ59tWzuzs83NvqL9IazSYBvKnQOh88RsFyCa+bUbvsaJj4cOSY8/Lj36qFRSCLB3b1M0rFcvO5so6VilzxNv+m5X+qyqSJifPnfAaxh2AcIUU93wBQXSnDlSWprJ48jPl7p0MRVJV650JPCQvFPpk3LngDMYdgHCEDPd8EVF0l//agqEffml2ZeWJk2fLg0b5lqBMC9U+qRIGBAahl0Ah0R9N7xlSW++Kd1/v7R1q9nXooUJQkaOlOrWdbU5XqiPQZEwwH4EH0AYvJKL4IjsbFMg7OOPzfapp5oaHePGSQ0aRLZtEeaFIAiIJuR8AGHwSi6CrTZsMF/t+/UzgUeDBtKkSaZA2L33+jrwoEgZ4E30fABhippu+G3bzPDKa6+Z7bp1TXLD5MlmLRaf81KRMgBlkXAKxJo9e6SHHpKefdYkrMTFSTfcYPa1aRPp1tkiZhKDAQ+hwimA8r791izmkp5uugWKiqQrrpA2b5aefz5qAg+p6sRgAJHHsAsQ7Q4eNEvYPvaY+W/JLACXmSn16BHZtjkkqhODgShAzwcQrQoKpCeeMPU5HnzQBB5du0rLl5sszCgNPKQoTQwGogg9H0C0KSyUXnhBmjpV2r3b7DvzTOnhh6X/9/9cKxAWaVGTGAxEIYIPwGWOrQtjWdKyZWa2yuefm31nnGGCkJtuMsvdxxjqcwDeFBtfgQCPcGxdmA8+kC68ULrmGhN4NGpkcjxycqRRo2Iy8ADgXUy1BVziyPTPdetMVdIPPjDbJ50k3XWXmdWSklLbJgNAyFjbBfAgW9eF+ewzUyBs6VKznZgo3XqrGXJp0sSW9gKAUwg+AJfYMv1z926Tw/Hcc8dKd44YYfa1bm1vgwHAIeR8wNf8tHZHKNM/Kz2f/ful8eNNBLNokQk8rr5a2rLFVCol8ADgIwQf8C3HkjcdlJFhcjyys83z8WuNVHg++fmmRkdamqnZceSIdMkl0tq1ZsjlrLMidCYAUHMknMKXom3tjgrPJ65YuSldFPjhU7OjWzdTlbR/f7MeS5RxbAoyAFewtguiXrSt3VHh+Vjx2v7DaVL79mbl2XXrpAEDojLw8GMvFoCaI/iAL5Ukbx7Pz2t3pLezFB9XthMyQYVq9+it0qefmvodURh0SKbH45ZbjgVfxcXS6NH+yOMBUDMEH/ClqFm7w7Kk999X4OrztdAapQQVSjJDLguethT4w/CoLxAWbb1YAKoX3f9XQ1Tz/dodH39sCoRlZ0uSMk7epkE3d9L2S0erXZeTFAg4993AS/kVrEALxB56PuBrgYDUt2/kb6Bh2brVTJPt3t0EHomJZhrtzp0KzL5LfYec5Oj5eC2/Imp6sQCEjNkugFtyc8202RdeMMMt8fFmwbcHH5RatnSlCZGaJRRKT0sw6ONeLADMdgE85ZtvpNtvN8vaP/+8CTyuucYkkmZluRZ4SJHJrwi1p8WXvVgAaoTgA3BKXp70wAOmQNicOdLRo6ZGxz/+YabOduzoepPcniXETBYAFSH4AOz200/So49KbdtK06dLhw9LF1xgVp5dsUI6//yINc3t/ApmsgCoCLNdUCNemi3hGUePmnVXpk2TvvrK7OvYUXrkEemqqzxTp8PNWULMZAFQEUd6Pr766ivdcMMNOu2009SgQQOdc8452rBhgxNvhQjw2myJiCsulpYskTp1MmMKX31l8jgWLTILv119tWcCjxJu5VdEw0wWPy1eCPiF7bNdDhw4oK5du+qSSy7RmDFj1KRJE+3YsUOtW7dWWlpatb/PbBdvi7Y1VWrFsqT33pMmTZI2bjT7GjeW7r/fBCFJSZFtn4f4dSZLVtaxnJX4eBNIHb8YIIBjwrl/2z7sMmvWLKWmpmrRokWl+1qz3HfUqGoM3083lVpbs8YUCFu1ymw3bCjdc4+p19GwYUSb5kWBgP/+PipLlh00yH/nAniN7cMub731ls477zwNHTpUTZo0UdeuXfXMM8/Y/TaIkGhbUyVsW7ZIV1wh9eplAo+kJOnuu6WdO83MljADD7r0vYtkWcA5tgcfO3fu1NNPP6309HS99957uvXWW3X77bfr+eefr/D4goIC5efnl3nAu6JhDL9Gdu6URoyQunSR3n7bnPioUeYO9dhj0umnh/2S5M54W8wH2oCDbM/5SExM1Hnnnac1a9aU7rv99tu1bt06ffTRR+WOnzp1qh566KFy+8n58Da/jeHXeHbO11+b6bILF5rZLJI0dKj08MNmqftatIfcGe/LyjJDLUVFxwJtcj6AikW0wmnz5s111llnldnXsWNH7d69u8LjJ06cqLy8vNLHnj177G4SHOCnapTh9DCUDoNszTOJpGlp0rx5JvAYNEhav1565ZVaBR4SXfp+kZFhAsLsbPNM4AHYw/aE0169emnbtm1l9n3xxRdq1apVhccnJSUpiVkBcEg4SYNmZoOl4uI4xetkLdQ3ytCPZgG4zEwTbdmE+hf+4cdkWcDrbO/5uPPOO7V27VrNmDFD27dv10svvaSFCxdq7Nixdr8VUK1QexiCu47qlpuLVVxs6nEUK0GjtVDBvyw3M1tsDDykGM6dAQA5EHycf/75WrZsmRYvXqxf/OIXevjhh/X444/r+uuvt/utPIsZDN5RbdJgcbG0eLFyLvqdiq2yBxYpQdvTBjlWIIwufQCxyvaE09rye5ExihJ5T4VJgyMt6d13pcmTpc2bFdQZaqUvVayE0t8jAdQ+lOMHol9EE05jGSt4Vi1SPULlehjar5Yuvli67DJp82YpOVmB6WO0cO5RhkEcwJRiACdiYTkbUf2zcpHuEQoEpMB3m6XRk0yPh6RgUppyrrpH6ZOHKdD5VGVIGnSlv6YQex1VQgFUhJ4PG1GUqGIR7xHavl267jrpnHNM4JGQoKw+z6nV0Rz1WzJarc45tfTbuJ+mEPsBU4oBVITgw0bMYKhYxG5Ae/dKY8aYZe0XLzb7hg9XMDtHt/zvb4/NbGF4zDE1DchJ2gaiG8GHzZjBUJ7rPUIHDkgTJpg3mD9fKiyUBg+WPvnEzGwpbMO3cZfUJCAnRwSIfsx2gStcKVN9+LD05JPSrFlSXp7Z17OnKRB28cWlh1Ha3H2hluPnswH8K5z7NwmnsE1V0ykzMkySoSPJnEeOSM88Y9Zb+eYbs69zZ2nGDGnIkHJ1Okq+jZ8YDHFzc06oVUJJ2gZiA8EHbBHKbBbby1QXFZlcjilTpF27zL62baVp06Rrry0/1nMcR4Mh1Bhl54HYQM4Has312SyWZZa179rVLHO/a5fUrJlZAO6zz6Trr68y8CjBzBbv8WLSNsmvgP0IPlBrrs5mWbVKuugi6YorpC1bpFNOMTkd27dLt90mJSY68KZwk5eStkl+BZxBwmkUiHTpaleSBDduNEvcL19utuvXl+64Q7r3XunUU216E++K9Gcci0h+BcJDefUY4oVvZo52lefkSMOHS+eeawKPOnVM7Y4dO0yPRwwEHl74jGMRBdIA59Dz4WNe+2YW6nTKkHz1lUkczcoy/8ePizNJpNOmSWlptrTXD7z2GccS/u2B8NDzEcWOT37z2jczWxI4v/vODKW0a2e6U4qKzAJwmzZJf/1rTAUekvc+41jixeRXIFow1dZHTpzOOmtWFE1LPHRIevxx6dFHpfx8s693bzO00qtXRJsWSUw9jSymZAPOoOfDJyqazjphgjRzps+/mRUUSHPmmB6NBx4wgUeXLmYBuJUrYzrwkPj27QVMyQbsR8+HT1TW/X7++WYM2nffzIqKzDDKlCnSl1+afWlp0vTp0rBhIdXpiBV8+wYQbQg+fKKq7nfbK4c6ybKkN9+U7r9f2rrV7GvRwgQhI0dKdetGtn0e5avPGACqwddLn4iK7vfsbKlHD+nqq03gceqpJnElJ8eURCXwAICYQM9HGCJd6Mm33e8bNpgCYe+/b7YbNJDGj5fuucdUKAUAxBR6PkLklUJPvkp+27ZNGjpUOu88E3jUrSuNG2cKhD3yiGOBB2tx2It/TwB2I/gIgesLp7nM9pvLnj3SqFFSp07Sa6+ZAmEjRphgZM4cswicQ7wSJEYL/j0BOIHgIwTRXOjJ1pvLt99Kd99txqVKKpNecYW0ebP0/PNSmza2tbsi0R4kuo1/TwBOIfgIQclMk+NFQ6En224uBw+asudt20qzZ5vaHX36SGvWmJktnTvb3vaKRHOQGAn8ewJwCsFHCKJipkkFan1zKSiQnnjC1Od48EEThHTtahaAK5nZ4qJoDRIjhX9PAE4h+AhRRoYp5pWdbZ4zMiLdotoL5eZSYT5IYaG0aJF05plm1sq//21ebMkSaf16MyUnLs6NUygjWoPESOHfE4BTWNU2xmVlmaGWoqJjN5eSwOrEtWQWLrCU0WiZgvfNUc52KV05Cpwh0+tx002eqdNh6+q64N8TQEjCuX8TfKDCm0uFy4mrSJm6TxM0S8VKUHxcsRbOK1TGmMTINBwA4BkEH6i17GwzA+ZEcSqSpYTS7YQEMwwVLd+II11IDgD8Kpz7NzkfMSLcWh7pylG8ymajxscVlwk8pOia/UBNCwBwB8FHDAjrprp7tzRypAL9O2ihblaCCiVJCQmWZv0xPmpnP1DTAgDcQ/AR5UK+qe7fb2aupKebmSzFxcq4+oByP9j5nxk+cfrDH6J39gM1LQDAPbYHH1OnTlVcXFyZRzMHy2nbze5S45FeF6Pam2p+vpmtkpZmanYcOSJdcom0dq20dKkC/c4ss5ZMNE45lqhpAQBucqTno1OnTtq3b1/pY8uWLU68je3sHvP3Qg5BpTfVwM+mGmnbtqY66aFDUrduZgG4Dz6QLryw0tf01eJ2IaKmBQC4x/bZLlOnTtUbb7yhTZs21ej3IzXbpcKppbWYyWH369VG2VoelhbcsFoZH1x3rDumfXuzyuyvfx2R4mBeQk0LAKiZiM92ycnJUYsWLdSmTRsNHz5cO3fudOJtbGX3mL+XcggyMqTcXZayp65Ubqu+ynjuYnOXDQRMZPLpp9I118R84CFFZ68OAHhNHbtf8MILL9Tzzz+vM888U998842mT5+unj17auvWrTrttNPKHV9QUKCCgoLS7fz8fLubFJKS4YkTeypqOuZv9+vVmGVJK1YoMGmSAhs2mH2nny5NmiSNGSPVq+dygwAAsc72no/BgwfrmmuuUefOndW/f3+98847kqTnnnuuwuMzMzOVkpJS+khNTbW7SSGxe8zfEzkEH38sXXqpWWtlwwbp5JNNcumOHdKddxJ4AAAiwpUKpwMGDFC7du309NNPl/tZRT0fqampEatwaveYf0RyCLZule6/X3rjDbOdmCjddpvp7Wjc2KVG+J8T1U6poAogWoWT82H7sMuJCgoK9Nlnn6l3794V/jwpKUlJSUlONyNkgYC9NwW7X69KubmmZ+OFF8xwS3y8WfDtwQelli1dakR0KLeo3sLaTyt24jUBwI9s7/n4wx/+oMsvv1wtW7bU/v37NX36dK1cuVJbtmxRq1atqv191napgW++MbNV5s+Xjh41+665Rnr4YaljR9eb4/dv907MVPLS7CcAcEJEZ7sEg0Fde+21at++vX79618rMTFRa9euDSnwQJjy8qQHHjAFwubMMYFH//7SP/4hvfZaRAIPL9Q2qS0nZip5afYTAEQaq9r60U8/SfPmSZmZ0vffm30XXGC2K1qK1iXR8u2eng8ACF/E63zAIUePmkSB9HTpnntM4NGxo7R0qSmHHsHAQ4qeb/fhzFQKtXy+J2Y/AYBH0PPhB8XF0quvmiGWnByzr2VL6aGHpBEjjt3RIizavt1XN1OpJgmkVFAFEK3CuX8TfHiZZUnvvWemyG7caPY1bmym0Y4eLXlollCJsqXczbf7aJzREW2BFgDUlqem2qK8kGaDrFkjTZworVplths2NEMt48eb/3a7PSHKyDA1zaL9231VQ0zRes4AYBdyPmop1DH/EtXOBtmyRbriCqlXLxN4JCVJd98t7dxphl0qCTzCbUfI7amBWFgfpdLVgt0unw8APkTwUQvh3riDQenmm499Yy4uNkMUwaBMcDFihNSli/T22+ZONmqU+Yr92GNmPRab2nF8e0pyFsq1B1UigRQAao6cjxoKdcz/+CGNJ54wccSJsq96Qn3fuedYgbChQ02BsPbtbWtHRbKzK54gk51tei5QPRJIAcAg58MFoYz5Hz8borLV6uNVqHZvPCrpqEmWeOQRqVs3W9tRGc+svOtjrpbPB4AowbBLDVU35n/ikIZlmceJ7tJsBbqnmu6G5cvDCjxCaUdVGDoAAEQCwUcNVXfjrqhH4kQJKtIdf+liZrbUcJyjtgFERoYZosnONs/ROC0WAOAtMZvzYdf00srG/CvKxYhTkeJlqUh1lBBfrAXzpYyb7Yn/yD0AAEQS5dWrYef00sqmlQbOsLRw3D+VoCJJUoIK9UzyH5Q77QVlv39UuV/G2xZ4VNUOAAC8JuZ6PlypTLl6tSkQtnq1gjpD2xt0UbuxgxSYMlI6+WSb3gQAAO9gtksVHK1MuXmzNHmy9M47ZrtePQVuv16B++6TGjWq5YsDABAdYi74cGR66Y4d0pQp0uLFZkpLSYGwBx6Qzjij1m0GACCaxFzOR6izQ0IqV753rzRmjNShg/TSSybwGD5c+uwzaf58Ag8AACoQcz0fUvWLn1W7VPqBA9KsWdKTT0o//WT2DR5sCoR17eraeQAA4Ecxl3BanSoTUk89bAKOWbOkvDzzw549pcxM6eKLXW8rAABeQcJpLVSakProMgWWjJG++cbs7NxZmjFDGjKk8trpAACgHIKPE1SYkKpCtXvy95K+kdq2laZNk669tnxdcwAAUC3unicIBKSFCywlxJvoI0GFWqDRCjQrkubNM8mk119P4AEAQA3R83GiVauUsWiiBhV/qe1qp3bJ/1Zg4gjp99ulk06KdOtgE7vK6wMAwkfwUWLjRmnSJLOyrKRA/foK3NFDuvde6dRTI9w42Kna2UwAAEcxdpCTY2pznHuuCTzq1DG1O3bsMLNYCDyiSjB4LPCQzPPo0dXUcwEA2Cp2g4+vvjJ3nY4dpSVLzIyV666TPv9ceuopqXnzSLcQDqiqvD4AwB2xN+zy3XemTsecOdLPP5t9l11mCoSdfXZk2wbHOVJeHwAQltjp+Th0yAQYbdtKjz5qAo/evc0KtG+/TeARI0Itrw8AcE7sVDjNyTFDLEVFUpcuJp/jl7+kQFiMCgYrL68PAAgfFU4rkp4uPfigeR42jDodMS4QIOgAgEiJneBDMkvcAwCAiOLrPwAAcBXBBwAAcBXBBwAAcJXjwUdmZqbi4uI0fvx4p98KAAD4gKPBx7p167Rw4UKdTQ0NAADwH44FH4cOHdL111+vZ555RqeyPoojgkEpO5t1SQAA/uJY8DF27FgNGTJE/fv3r/K4goIC5efnl3mgellZUqtWUr9+5jkrK9ItAgAgNI4EHy+//LI++eQTZWZmVntsZmamUlJSSh+pqalONCmqsDIrAMDPbA8+9uzZozvuuEMvvvii6tWrV+3xEydOVF5eXuljz549djcp6rAyKwDAz2yvcLphwwbt379f3bp1K91XVFSkVatWae7cuSooKFBCyapekpKSkpSUlGR3M6Ka11dmDQZNgJSeTglzAEB5tvd8XHrppdqyZYs2bdpU+jjvvPN0/fXXa9OmTWUCD9SMl1dmJRcFAFAdV1a17du3r8455xw9/vjj1R7r2Kq2UchrK7MGgybgOLFHJjfXG+0DADiHVW1jhNdWZq0qF8VL7QQARJYrwceHH37oxtsgwryeiwIA8AbWdoFtvJyLAgDwDoZdYKuMDGnQIG/logAAvIXgA7bzWi4KAMBbGHYBAACuIvgAAACuIvgAAACuIvgAAACuIvgAAACuIvgAAACuIvgAAACuIvgAAACuIvgAAACuIvgAAACuIvjwkGBQys42zwAARCuCD4/IypJatZL69TPPWVmRbhEAAM4g+PCAYFC65RapuNhsFxdLo0fTAwIAiE4EHx6Qk3Ms8ChRVGSWpQcAINoQfHhAeroUf8InkZAgtWsXmfYAAOAkgo9asCtBNBCQFi40AYdknhcsMPsBAIg2BB81ZHeCaEaGlJtrgpncXLMNAEA0irMsy4p0I46Xn5+vlJQU5eXlKTk5OdLNqVAwaAKO4/M0EhJM0EBvBQAgFoVz/6bnowZIEAUAoOYIPmrArwmiFDEDAHgBwUcN+DFBlCJmAACvIOejFoJBM9TSrp23Aw9yVAAATgvn/l3HpTZFpUDAHzfvqnJU/NB+AEB0YdglBvg1RwUAEJ0IPmKAH3NUAADRi2GXGJGRIQ0a5I8cFQBAdCP4iCF+yVEBAEQ3hl0AAICrCD4AAICrbA8+nn76aZ199tlKTk5WcnKyevToob///e92vw0AAPAp24OPQCCgmTNnav369Vq/fr369eunK6+8Ulu3brX7rQAAgA+5UuG0UaNGevTRR5URwjrxfqpwCgAADM9UOC0qKtKrr76qw4cPq0ePHhUeU1BQoIKCgtLt/Px8J5sEAAAizJGE0y1btujkk09WUlKSbr31Vi1btkxnnXVWhcdmZmYqJSWl9JGamupEkwAAgEc4Muxy5MgR7d69Wz/88INef/11/eUvf9HKlSsrDEAq6vlITU1l2AUAAB8JZ9jFlZyP/v37Ky0tTQsWLKj2WHI+AADwn3Du367U+bAsq0zvBgAAiF22J5xOmjRJgwcPVmpqqg4ePKiXX35ZH374oZYvX273WwEAAB+yPfj45ptvNGLECO3bt08pKSk6++yztXz5cg0YMCCk3y8ZBWLWCwAA/lFy3w4lm8OVnI9wBINBZrwAAOBTe/bsUaCaVUw9F3wUFxdr7969atiwoeLi4mx97ZKZNHv27InKZNZoPz8p+s+R8/O/aD/HaD8/KfrP0anzsyxLBw8eVIsWLRQfX3VKqaNFxmoiPj6+2oiptkrWnYlW0X5+UvSfI+fnf9F+jtF+flL0n6MT55eSkhLScaxqCwAAXEXwAQAAXBVTwUdSUpIefPBBJSUlRbopjoj285Oi/xw5P/+L9nOM9vOTov8cvXB+nks4BQAA0S2mej4AAEDkEXwAAABXEXwAAABXEXwAAABX+Tr4eOqpp9SmTRvVq1dP3bp10//+7/9WefzKlSvVrVs31atXT23bttX8+fPLHfP666/rrLPOUlJSks466ywtW7bMqeaHJJxzXLp0qQYMGKDGjRsrOTlZPXr00HvvvVfmmGeffVZxcXHlHj///LPTp1KhcM7vww8/rLDtn3/+eZnjvPQZhnN+N910U4Xn16lTp9JjvPb5rVq1SpdffrlatGihuLg4vfHGG9X+jp+uw3DPz2/XYLjn58drMNxz9NN1mJmZqfPPP18NGzZUkyZNdNVVV2nbtm3V/p4XrkHfBh9LlizR+PHjNXnyZG3cuFG9e/fW4MGDtXv37gqP37Vrl371q1+pd+/e2rhxoyZNmqTbb79dr7/+eukxH330kX7zm99oxIgR2rx5s0aMGKFhw4bp448/duu0ygj3HFetWqUBAwbo3Xff1YYNG3TJJZfo8ssv18aNG8scl5ycrH379pV51KtXz41TKiPc8yuxbdu2Mm1PT08v/ZmXPsNwz++JJ54oc1579uxRo0aNNHTo0DLHeeXzk6TDhw+rS5cumjt3bkjH++06DPf8/HYNhnt+JfxyDUrhn6OfrsOVK1dq7NixWrt2rVasWKHCwkINHDhQhw8frvR3PHMNWj51wQUXWLfeemuZfR06dLAmTJhQ4fH33nuv1aFDhzL7Ro8ebXXv3r10e9iwYdYvf/nLMscMGjTIGj58uE2tDk+451iRs846y3rooYdKtxctWmSlpKTY1cRaCff8srOzLUnWgQMHKn1NL32Gtf38li1bZsXFxVm5ubml+7z0+Z1IkrVs2bIqj/HjdVgilPOriJevweOFcn5+uwZPVJPP0E/X4f79+y1J1sqVKys9xivXoC97Po4cOaINGzZo4MCBZfYPHDhQa9asqfB3Pvroo3LHDxo0SOvXr9fRo0erPKay13RSTc7xRMXFxTp48KAaNWpUZv+hQ4fUqlUrBQIBXXbZZeW+lbmhNufXtWtXNW/eXJdeeqmys7PL/Mwrn6Edn19WVpb69++vVq1aldnvhc+vpvx2HdaWl6/B2vDDNWgXP12HeXl5klTu7+14XrkGfRl8fPvttyoqKlLTpk3L7G/atKm+/vrrCn/n66+/rvD4wsJCffvtt1UeU9lrOqkm53iiP/3pTzp8+LCGDRtWuq9Dhw569tln9dZbb2nx4sWqV6+eevXqpZycHFvbX52anF/z5s21cOFCvf7661q6dKnat2+vSy+9VKtWrSo9xiufYW0/v3379unvf/+7Ro0aVWa/Vz6/mvLbdVhbXr4Ga8JP16Ad/HQdWpalu+66SxdddJF+8YtfVHqcV65Bz61qG464uLgy25ZlldtX3fEn7g/3NZ1W0/YsXrxYU6dO1ZtvvqkmTZqU7u/evbu6d+9eut2rVy+de+65mjNnjp588kn7Gh6icM6vffv2at++fel2jx49tGfPHj322GO6+OKLa/SaTqtpW5599lmdcsopuuqqq8rs99rnVxN+vA5rwi/XYDj8eA3Whp+uw3Hjxumf//ynVq9eXe2xXrgGfdnzcfrppyshIaFcFLZ///5y0VqJZs2aVXh8nTp1dNppp1V5TGWv6aSanGOJJUuWKCMjQ6+88or69+9f5bHx8fE6//zzXY/Ya3N+x+vevXuZtnvlM6zN+VmWpf/6r//SiBEjlJiYWOWxkfr8aspv12FN+eEatItXr8Ha8tN1+Pvf/15vvfWWsrOzFQgEqjzWK9egL4OPxMREdevWTStWrCizf8WKFerZs2eFv9OjR49yx7///vs677zzVLdu3SqPqew1nVSTc5TMt62bbrpJL730koYMGVLt+1iWpU2bNql58+a1bnM4anp+J9q4cWOZtnvlM6zN+a1cuVLbt29XRkZGte8Tqc+vpvx2HdaEX65Bu3j1GqwtP1yHlmVp3LhxWrp0qf7nf/5Hbdq0qfZ3PHMN2pa66rKXX37Zqlu3rpWVlWX961//ssaPH2+ddNJJpRnJEyZMsEaMGFF6/M6dO60GDRpYd955p/Wvf/3LysrKsurWrWu99tprpcf83//9n5WQkGDNnDnT+uyzz6yZM2daderUsdauXev6+VlW+Of40ksvWXXq1LHmzZtn7du3r/Txww8/lB4zdepUa/ny5daOHTusjRs3Wr/73e+sOnXqWB9//LHnz+/Pf/6ztWzZMuuLL76wPv30U2vChAmWJOv1118vPcZLn2G451fihhtusC688MIKX9NLn59lWdbBgwetjRs3Whs3brQkWbNnz7Y2btxoffnll5Zl+f86DPf8/HYNhnt+frsGLSv8cyzhh+twzJgxVkpKivXhhx+W+Xv78ccfS4/x6jXo2+DDsixr3rx5VqtWrazExETr3HPPLTO96MYbb7T69OlT5vgPP/zQ6tq1q5WYmGi1bt3aevrpp8u95quvvmq1b9/eqlu3rtWhQ4cyF1UkhHOOffr0sSSVe9x4442lx4wfP95q2bKllZiYaDVu3NgaOHCgtWbNGhfPqKxwzm/WrFlWWlqaVa9ePevUU0+1LrroIuudd94p95pe+gzD/Rv94YcfrPr161sLFy6s8PW89vmVTL2s7G/O79dhuOfnt2sw3PPz4zVYk79Rv1yHFZ2XJGvRokWlx3j1Goz7zwkAAAC4wpc5HwAAwL8IPgAAgKsIPgAAgKsIPgAAgKsIPgAAgKsIPgAAgKsIPgAAgKsIPgAAgKsIPgAAgKsIPgAAgKsIPgAAgKsIPgAAgKv+P63y0L8iIYRvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizamos las predicciones del modelo de R.Lineal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cbc570",
   "metadata": {},
   "source": [
    "Realizar una regresión lineal con **Scickit-Learn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10290589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e77552",
   "metadata": {},
   "source": [
    "Observa que Scikit-Learn separa el término de bias (`intercept_`) de los pesos de características (`coef_`).\n",
    "\n",
    "La clase LinearRegression se basa en la función `scipy.linalg.lstsq()` (el nombre se refiere a \"mínimos cuadrados\"), a la que se puede llamar directamente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df62e8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f3da7",
   "metadata": {},
   "source": [
    "Esta función calcula **θ^ = X+y**, donde **X+** es la pseudoinversa de **X** (específicamente la inversa de Moore-Penrose).\n",
    "\n",
    "Puedes usar `np.linalg.pinv()` para hacer la pseudoinversa directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad75318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21810da",
   "metadata": {},
   "source": [
    "La pseudoinversa en sí misma se calcula utilizando una técnica de factorización de matriz estándar llamada descomposición de valores singulares (**SVD**) que puede descomponer la matriz del conjunto de entrenamiento **X** en la multiplicación de matrices de tres matrices U Σ V⊺ (`vernumpynumpy.linalg.svd()`). \n",
    "\n",
    "La pseudoinversa se calcula como:\n",
    "\n",
    "**<center>X+ = VΣ+U⊺</center>** \n",
    "\n",
    "Para calcular la matriz Σ+, el algoritmo toma Σ y establece en cero todos los valores más pequeños que un pequeño valor de umbral, luego reemplaza todos los valores distintos de cero con su inverso y, finalmente, transpone la matriz resultante. \n",
    "\n",
    "Este enfoque es más eficiente que calcular la ecuación Normal, además de que maneja bien los casos de borde: de hecho, la ecuación Normal puede no funcionar si la matriz **X⊺X** no es invertible (es decir, singular), como si m < n o si algunas características son redundantes, pero el pseudoinverse siempre está definido.\n",
    "\n",
    "\n",
    "### Complejidad computacional\n",
    "</br></br>\n",
    "La ecuación normal calcula la inversa de **X⊺ X**, que es una matriz (n + 1) × (n + 1) (donde n es el número de características). \n",
    "\n",
    "La complejidad computacional de invertir dicha matriz suele ser de _O(n2.4) a O(n3)_, dependiendo de la implementación. \n",
    "\n",
    "En otras palabras, si duplicas el número de características, multiplicas el tiempo de cálculo por aproximadamente 22,4 = 5,3 a 23 = 8.\n",
    "\n",
    "El enfoque SVD utilizado por la clase `LinearRegression` de Scikit-Learn se trata de _O(n2)_. \n",
    "\n",
    "**Si duplicas la cantidad de características, multiplica el tiempo de cálculo por aproximadamente 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37e98d",
   "metadata": {},
   "source": [
    "#### ---------------------------------- ADVERTENCIA ----------------------------------\n",
    "\n",
    "Tanto la **ecuación normal** como el enfoque **SVD** se **ralentizan** mucho cuando el número de características **crece** (por ejemplo, 100.000). \n",
    "\n",
    "En el lado positivo, **ambos son lineales con respecto al número de instancias** en el conjunto de entrenamiento (son _O(m)_), por lo que manejan grandes conjuntos de entrenamiento de manera eficiente, siempre que puedan caber en la memoria.\n",
    "### --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Además, una vez que hayas entrenado tu modelo de regresión lineal (usando la ecuación normal o cualquier otro algoritmo), **las predicciones son muy rápidas: la complejidad computacional es lineal con respecto tanto al número de instancias en las que desea hacer predicciones como al número de características**. En otras palabras, hacer predicciones sobre el doble de casos (o el doble de características) tomará aproximadamente el doble de tiempo.\n",
    "\n",
    "**<center>x2 características = x2 tiempo</center>**\n",
    "\n",
    "#### Ahora veremos una forma muy diferente de entrenar un modelo de regresión lineal, que es más adecuado para los casos en los que hay un gran número de características o demasiadas instancias de entrenamiento para encajar en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05772e8",
   "metadata": {},
   "source": [
    "## Descenso del gradiente\n",
    "\n",
    "</br></br>\n",
    "El _descenso de gradiente_ es un algoritmo de optimización genérico capaz de encontrar **soluciones óptimas** para una amplia gama de problemas. \n",
    "\n",
    "La idea general del descenso del gradiente es **ajustar los parámetros de forma iterativa** para minimizar una **función de coste*.\n",
    "\n",
    "Supongamos que estás perdido en las montañas en una densa niebla, y solo puedes sentir la pendiente del suelo debajo de tus pies. Una buena estrategia para llegar al fondo del valle rápidamente es ir cuesta abajo en dirección a la pendiente más empinada. \n",
    "Esto es exactamente lo que hace el descenso del gradiente: mide el gradiente local de la función de error con respecto al vector de parámetros θ, y va en la dirección del gradiente descendente. \n",
    "Una vez que el gradiente es cero (casi nunca pasa), ¡has alcanzado un mínimo!\n",
    "\n",
    "En la práctica, comienzas por llenar θ con valores aleatorios (esto se llama inicialización aleatoria). \n",
    "\n",
    "Luego lo mejoras gradualmente, dando un paso a la vez, cada paso tratando de disminuir la función de costo (por ejemplo, el MSE), hasta que el algoritmo converge al mínimo.\n",
    "\n",
    "![descenso_gradiente](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0403.png)\n",
    "\n",
    "(_En esta representación del descenso de gradiente, los parámetros del modelo se inicializan al azar y se ajustan repetidamente para minimizar la función de costo; el tamaño del paso de aprendizaje es proporcional a la pendiente de la función de costo, por lo que los pasos se reducen gradualmente a medida que el costo se acerca al mínimo_)\n",
    "\n",
    "</br></br>\n",
    "\n",
    "Un parámetro importante en el descenso de gradiente es el tamaño de los pasos, determinado por el hiperparámetro de la **tasa de aprendizaje** o learning rate.\n",
    "\n",
    "Si la tasa de aprendizaje es **demasiado pequeña**, entonces el algoritmo tendrá que pasar por **muchas iteraciones para converger**, lo que llevará mucho tiempo.\n",
    "\n",
    "![small_learning_rate](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0404.png)\n",
    "\n",
    "</br></br>\n",
    "\n",
    "Por otro lado, si la tasa de aprendizaje es **demasiado alta**, podrías saltar a través del valle y terminar en el otro lado, posiblemente incluso más alto de lo que estabas antes. \n",
    "Esto podría hacer que el algoritmo **diverja** (no converja), con valores cada vez más grandes, al no encontrar una buena solución.\n",
    "\n",
    "![big_learning_rate](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0405.png)\n",
    "\n",
    "Además, no todas las funciones de coste parecen cuencos bonitos y normales. Puede haber agujeros, crestas, mesetas y todo tipo de terreno irregular, lo que dificulta la convergencia al mínimo. \n",
    "\n",
    "La siguiente figura muestra los dos **desafíos principales** con el descenso en pendiente. \n",
    "\n",
    "Si la inicialización aleatoria inicia el algoritmo a la **izquierda**, entonces convergerá a un mínimo local, que no es tan bueno como el mínimo global. \n",
    "\n",
    "Si comienza a la **derecha**, entonces llevará mucho tiempo cruzar la meseta. Y si te detienes demasiado pronto, nunca alcanzarás el mínimo global.\n",
    "\n",
    "![desafios_descenso_gradiente](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0406.png)\n",
    "\n",
    "\n",
    "Afortunadamente, la función de costo de MSE para un modelo de regresión lineal resulta ser una función **convexa**, lo que significa que si eliges dos puntos en la curva, el segmento de línea que los une nunca está por debajo de la curva. Esto implica que no hay mínimos locales, solo un mínimo global. \n",
    "\n",
    "También es una función continua con una pendiente que nunca cambia abruptamente.⁠\n",
    "\n",
    "Estos dos hechos tienen una gran consecuencia: se garantiza que el descenso del gradiente se acerca arbitrariamente al mínimo global (si esperas lo suficiente y si la tasa de aprendizaje no es demasiado alta).\n",
    "\n",
    "Si bien la función de costo tiene la forma de un tazón, puede ser un tazón alargado si las características tienen escalas muy diferentes. \n",
    "\n",
    "La siguiente imagen muestra el descenso en gradiente en un conjunto de entrenamiento donde las características 1 y 2 tienen la misma escala (a la izquierda), y en un conjunto de entrenamiento donde la característica 1 tiene valores mucho más pequeños que la característica 2 (a la derecha).⁠\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0407.png)\n",
    "\n",
    "Como puedes ver, a la izquierda el algoritmo de descenso de gradiente va directamente hacia el mínimo, llegando así rápidamente, mientras que a la derecha primero va en una dirección casi ortogonal a la dirección del mínimo global, y termina con una larga marcha por un valle casi plano. \n",
    "\n",
    "Eventualmente alcanzará el mínimo, pero llevará mucho tiempo.\n",
    "\n",
    "#### ----------------------------- ADVERTENCIA -----------------------------\n",
    "\n",
    "Cuando uses el gradiente descendente, deberías asegurarte que todas las características tienen una escala similar (ej: usando `StandardScaler`), de lo contrario tardará mucho más en converger.\n",
    "#### -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844bb5a3",
   "metadata": {},
   "source": [
    "Este diagrama también ilustra el hecho de que **entrenar un modelo significa buscar una combinación de parámetros del modelo que minimice una función de costo** (sobre el conjunto de entrenamiento). \n",
    "\n",
    "Es una búsqueda en el espacio de parámetros del modelo. \n",
    "\n",
    "Cuantos más parámetros tenga un modelo, más dimensiones tenga este espacio y más difícil sea la búsqueda: buscar una aguja en un pajar de 300 dimensiones es mucho más complicado que en 3 dimensiones. \n",
    "\n",
    "Afortunadamente, dado que la función de costo es convexa en el caso de la regresión lineal, la aguja está simplemente en la parte inferior del tazón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d5375",
   "metadata": {},
   "source": [
    "## Descenso del gradiente por lotes (BATCH)\n",
    "\n",
    "Para implementar el descenso de gradiente, es necesario **calcular el gradiente de la función de costo con respecto a cada parámetro del modelo θj**. \n",
    "\n",
    "En otras palabras, necesitas calcular cuánto cambiará la función de costo si cambias θj solo un poco. Esto se llama derivada parcial. \n",
    "\n",
    "Es como preguntar: \"¿Cuál es la pendiente de la montaña bajo mis pies si miro hacia el este\"? y luego hacer la misma pregunta mirando hacia el norte (y así sucesivamente para todas las demás dimensiones, si puedes imaginar un universo con más de tres dimensiones). \n",
    "\n",
    "La ecuación 4-5 calcula la derivada parcial del MSE con respecto al parámetro θj, señalado ∂ MSE(θ) / ∂θj.\n",
    "\n",
    "### Ecuación 4-5: Derivadas parciales de la función de coste\n",
    "\n",
    "<a href=\"https://ibb.co/z51jft4\"><img src=\"https://i.ibb.co/Sx2WvZ3/Captura-de-pantalla-2023-08-20-a-las-21-19-22.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-19-22\" border=\"0\"></a>\n",
    "\n",
    "En lugar de calcular estas derivadas parciales individualmente, puedes usar la ecuación 4-6 para calcularlas todas de una sola vez. \n",
    "\n",
    "El vector de gradiente, señalado ∇θMSE(θ), contiene todas las derivadas parciales de la función de coste (una para cada parámetro del modelo).\n",
    "\n",
    "### Ecuación 4-6: Vector de gradiente de la función de coste\n",
    "\n",
    "<a href=\"https://ibb.co/zPx8DbB\"><img src=\"https://i.ibb.co/CV8JDvY/Captura-de-pantalla-2023-08-20-a-las-21-20-53.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-20-53\" border=\"0\"></a>\n",
    "</br>\n",
    "\n",
    "#### ---------------------- ADVERTENCIA -------------------\n",
    "</br>\n",
    "\n",
    "¡Ten en cuenta que esta fórmula implica cálculos sobre el conjunto completo de entrenamiento X, en cada paso de descenso de gradiente! \n",
    "\n",
    "Esta es la razón por la que el algoritmo se llama descenso de gradiente **por lotes**: **utiliza todo el lote de datos de entrenamiento en cada paso** (en realidad, el descenso de gradiente completo probablemente sería un mejor nombre). \n",
    "\n",
    "Como resultado, es **terriblemente lento en conjuntos de entrenamiento muy grandes** (en breve veremos algunos algoritmos de descenso de gradiente mucho más rápidos). \n",
    "\n",
    "Sin embargo, el descenso del gradiente se escala bien con el número de características; entrenar un modelo de regresión lineal cuando hay cientos de miles de características es mucho más rápido usando el descenso del gradiente que usando la ecuación normal o la descomposición de SVD.\n",
    "\n",
    "#### -----------------------------------------------------------\n",
    "\n",
    "Una vez que tengas el vector de gradiente, que apunta cuesta arriba, simplemente ve en la dirección opuesta para ir cuesta abajo. \n",
    "\n",
    "Esto significa restar ∇θMSE(θ) de θ. \n",
    "\n",
    "Aquí es donde entra en juego la tasa de aprendizaje η:⁠4 multiplica el vector de gradiente por η para determinar el tamaño del paso de descenso (Ecuación 4-7).\n",
    "\n",
    "### Ecuación 4-7: Paso (step) del descenso del gradiente\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/1MHnNPW/Captura-de-pantalla-2023-08-20-a-las-21-24-54.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-24-54\" border=\"0\"></a>\n",
    "\n",
    "Vamos a hacer una implementación básica de este algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b0b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_epochs = 1000\n",
    "m = len(X_b)  # número de instancias\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # parámetros del modelo inicializados aleatoriamente\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b6a17",
   "metadata": {},
   "source": [
    "No fue muy dificil! Cada iteración del conjunto de entrenamiento es llamada \"**época**\".\n",
    "\n",
    "Vamos a ver el resultado de `theta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47e8ecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb21860",
   "metadata": {},
   "source": [
    "Ey, ¡eso es exactamente lo que encontró la ecuación normal! \n",
    "\n",
    "El descenso en gradiente funcionó perfectamente. Pero, ¿y si hubieras usado una tasa de aprendizaje diferente (`eta`)? \n",
    "\n",
    "La figura siguiente muestra los primeros 20 pasos de descenso de gradiente utilizando tres tasas de aprendizaje diferentes. \n",
    "\n",
    "La línea en la parte inferior de cada gráfico representa el punto de partida aleatorio, luego cada época está representada por una línea cada vez más oscura.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0408.png)\n",
    "\n",
    "- A la **izquierda**, la tasa de aprendizaje es demasiado baja: el algoritmo finalmente llegará a la solución, pero llevará mucho tiempo. \n",
    "\n",
    "- En el **medio**, la tasa de aprendizaje se ve bastante bien: en solo unas pocas épocas, ya ha convergido hacia la solución. \n",
    "\n",
    "- A la **derecha**, la tasa de aprendizaje es demasiado alta: el algoritmo diverge, saltando por todas partes y en realidad se aleja cada vez más de la solución en cada paso.\n",
    "\n",
    "</br>\n",
    "\n",
    "Para **encontrar una buena tasa de aprendizaje**, puede utilizar la **búsqueda en grill** (`SearchGrill` en el capítulo 2). Sin embargo, es posible que desee **limitar el número de épocas** para que la búsqueda en la cuadrícula pueda eliminar los modelos que tardan demasiado en converger.\n",
    "\n",
    "Puede que te preguntes cómo establecer el **número de épocas**: \n",
    "\n",
    "Si es demasiado **bajo**, todavía estarás **lejos de la solución** óptima cuando el algoritmo se detenga; pero si es demasiado **alto**, **perderás el tiempo mientras los parámetros del modelo ya no cambian**. \n",
    "\n",
    "Una solución simple es establecer un número muy grande de épocas, pero interrumpir el algoritmo cuando el vector de gradiente se vuelve diminuto, es decir, cuando su norma se vuelve más pequeña que un número diminuto ε (llamado _tolerancia_), porque esto sucede cuando el descenso del gradiente (casi) ha alcanzado el mínimo.\n",
    "\n",
    "#### --------------------- TASA DE CONVERGENCIA -------------------------\n",
    "Cuando la función de costo es convexa y su pendiente no cambia abruptamente (como es el caso de la función de costo de MSE), el descenso del gradiente de lote con una tasa de aprendizaje fija eventualmente convergerá a la solución óptima, pero es posible que tenga que esperar un tiempo: puede tomar iteraciones O(1/ε) para alcanzar el óptimo dentro de un rango de Si divides la tolerancia entre 10 para tener una solución más precisa, entonces es posible que el algoritmo tenga que funcionar unas 10 veces más tiempo.\n",
    "#### ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e5640",
   "metadata": {},
   "source": [
    "## Descenso del gradiente estocástico\n",
    "</br></br>\n",
    "El principal problema con el descenso del gradiente por **lotes** (batch) es el hecho de que **utiliza todo el conjunto de entrenamiento** para calcular los gradientes en cada paso, lo que lo hace **muy lento cuando el conjunto de entrenamiento es grande**. \n",
    "\n",
    "En el extremo opuesto, el descenso del gradiente **estocástico** elige una **instancia aleatoria** en el conjunto de entrenamiento en cada paso y calcula los gradientes basados **solo en esa única instancia**. \n",
    "\n",
    "Obviamente, trabajar en una sola instancia a la vez hace que el algoritmo sea mucho más rápido porque tiene muy pocos datos para manipular en cada iteración. \n",
    "\n",
    "También hace posible entrenar en grandes conjuntos de entrenamiento, ya que solo una instancia debe estar en la memoria en cada iteración (el GD estocástico se puede implementar como un algoritmo fuera del núcleo; véase el capítulo 1).\n",
    "\n",
    "Por otro lado, debido a su naturaleza estocástica (es decir, aleatoria), este algoritmo es **mucho menos regular** que el descenso del gradiente de lotes: en lugar de disminuir suavemente hasta que alcance el mínimo, la función de costo rebotará hacia arriba y hacia abajo, disminuyendo solo en promedio. \n",
    "\n",
    "Con el tiempo terminará muy cerca del mínimo, pero una vez que llegue allí, **continuará rebotando, sin asentarse** (ver figura siguiente). \n",
    "\n",
    "Una vez que el algoritmo se detenga, los valores finales de los parámetros serán buenos, pero **no óptimos**.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0409.png)\n",
    "\n",
    "(_Con el descenso de gradiente estocástico, cada paso de entrenamiento es mucho más rápido, pero también mucho más estocástico que cuando se usa el descenso de gradiente por lotes_)\n",
    "\n",
    "Cuando la función de costo es muy irregular, esto en realidad puede ayudar al algoritmo a saltar de los mínimos locales, por lo que el descenso del gradiente estocástico tiene una mejor oportunidad de encontrar el mínimo global que el descenso del gradiente por lotes.\n",
    "\n",
    "Por lo tanto, la aleatoriedad es buena para escapar del \n",
    "óptimo local, pero mala porque significa que el algoritmo nunca puede establecerse al mínimo. \n",
    "\n",
    "Una solución a este dilema es reducir gradualmente la tasa de aprendizaje. Los pasos comienzan a lo grande (lo que ayuda a progresar rápidamente y a escapar de los mínimos locales), luego se hacen cada vez más pequeños, lo que permite que el algoritmo se establezca en el mínimo global. \n",
    "Este proceso es similar al recocido simulado, un algoritmo inspirado en el proceso en la metalurgia del recocido, donde el metal fundido se enfría lentamente. \n",
    "\n",
    "La función que determina la tasa de aprendizaje en cada iteración se llama programa de aprendizaje. \n",
    "\n",
    "Si la tasa de aprendizaje se reduce demasiado rápido, puede quedar atrapado en un mínimo local, o incluso terminar congelado a mitad del mínimo. \n",
    "\n",
    "Si la tasa de aprendizaje se reduce demasiado lentamente, puede saltar alrededor del mínimo durante mucho tiempo y terminar con una solución subóptima si detiene el entrenamiento demasiado pronto.\n",
    "\n",
    "Este código implementa el descenso del gradiente estocástico utilizando un programa de aprendizaje simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550c4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index : random_index + 1]\n",
    "        yi = y[random_index : random_index + 1]\n",
    "        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n",
    "        eta = learning_schedule(epoch * m + iteration)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e32385",
   "metadata": {},
   "source": [
    "Por convención, iteramos por rondas de m iteraciones; cada ronda se llama una época, como antes. \n",
    "\n",
    "Mientras que el código de descenso de gradiente por lotes iteró 1000 veces a través de todo el conjunto de entrenamiento, este código pasa por el conjunto de entrenamiento solo 50 veces y alcanza una solución bastante buena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baaa2c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21076011],\n",
       "       [2.74856079]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafd3c0",
   "metadata": {},
   "source": [
    "Tenga en cuenta que, dado que las instancias se eligen al azar, algunas instancias pueden elegirse varias veces por época, mientras que otras pueden no elegirse en absoluto. \n",
    "\n",
    "Si quieres estar seguro de que el algoritmo pasa por cada instancia en cada época, otro enfoque es barajar el conjunto de entrenamiento (asegurándose de barajar las características de entrada y las etiquetas de forma conjunta), luego pasar por él instancia por instancia, luego barajarlo de nuevo, y así sucesivamente. Sin embargo, este enfoque es más complejo y, en general, no mejora el resultado.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0410.png)\n",
    "\n",
    "Figura 4-10 (_Los primeros 20 pasos del descenso del gradiente estocástico_)\n",
    "\n",
    "\n",
    "\n",
    "#### ----------------------------- ADVERTENCIA --------------------------------\n",
    "\n",
    "Cuando se utiliza el descenso del gradiente estocástico, las instancias de entrenamiento deben ser independientes y distribuidas de manera idéntica (IID) para garantizar que los parámetros se tiren hacia el óptimo global, en promedio. \n",
    "Una forma sencilla de garantizar esto es barajar las instancias durante el entrenamiento (por ejemplo, elegir cada instancia al azar o barajar el conjunto de entrenamiento al comienzo de cada época). \n",
    "Si no baraja las instancias, por ejemplo, si las instancias están ordenadas por etiqueta, entonces SGD comenzará optimizando para una etiqueta, luego la siguiente, y así sucesivamente, y no se ajustará cerca del mínimo global.\n",
    "#### --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Para realizar una regresión lineal utilizando GD estocástico con Scikit-Learn, puede utilizar la clase `SGDRegressor`, que por defecto es la optimización de la función de costo de MSE. \n",
    "\n",
    "El siguiente código se ejecuta durante un máximo de 1000 épocas (`max_iter`) o hasta que la pérdida cae en menos de 10-5 (`tol`) durante 100 épocas (`n_iter_no_change`). \n",
    "\n",
    "Comienza con una tasa de aprendizaje de 0,01 (`eta0`), utilizando el horario de aprendizaje predeterminado (diferente del que usamos). Por último, no utiliza ninguna regularización (`penalty=None`; más detalles sobre esto en breve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7461ce2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n",
    "                       n_iter_no_change=100, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() porque fit() espera etiquetas de 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93495f",
   "metadata": {},
   "source": [
    "Una vez más, encuentras una solución bastante cercana a la devuelta por la Normalequation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102b95c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21278812]), array([2.77270267]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1087fc",
   "metadata": {},
   "source": [
    "#### ------------------------------------ TIP ------------------------------------\n",
    "\n",
    "Todos los estimadores de Scikit-Learn se pueden entrenar usando el método `fit()`, pero algunos estimadores también tienen un método `part_fit()` al que se puede llamar para ejecutar una única ronda de entrenamiento en una o más instancias (ignora hiperparámetros como `max_iter` o `tol`). . \n",
    "\n",
    "Llamar repetidamente a `part_fit()` entrenará gradualmente el modelo. Esto resulta útil cuando necesita más control sobre el proceso de formación. \n",
    "\n",
    "Otros modelos tienen en su lugar un hiperparámetro `warm_start` (y algunos tienen ambos): si estableces `warm_start=True`, llamar al método `fit()` en un modelo entrenado no restablecerá el modelo; simplemente continuará entrenando donde lo dejó, respetando hiperparámetros como `max_iter` y `tol`. \n",
    "\n",
    "Tenga en cuenta que `fit()` restablece el contador de iteraciones utilizado por el programa de aprendizaje, mientras que `parcial_fit()` no lo hace.\n",
    "#### ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957940b",
   "metadata": {},
   "source": [
    "## Descenso de gradiente por Mini-Lotes (minibatches)\n",
    "</br></br>\n",
    "El último algoritmo de descenso de gradiente que veremos se llama descenso de gradiente de mini lotes. Es sencillo una vez que conoces el descenso del gradiente estocástico y por lotes: en cada paso, en lugar de calcular los gradientes basados en el conjunto de entrenamiento completo (como en el GD por lotes) o basados en una sola instancia (como en el GD estocástico), el GD de minibatería calcula los gradientes en pequeños conjuntos aleatorios de instancias\n",
    "\n",
    "El progreso del algoritmo en el espacio de parámetros es menos errático que con el GD estocástico, especialmente con minilotes bastante grandes. Como resultado, el GD en minilote terminará caminando un poco más cerca del mínimo que el GD estocástico, pero puede ser más difícil para él escapar de los mínimos locales (en el caso de problemas que sufren de mínimos locales, a diferencia de la regresión lineal con la función de costo de MSE). La figura 4-11 muestra los caminos tomados por los tres algoritmos de descenso de gradiente en el espacio de parámetros durante el entrenamiento. Todos terminan cerca del mínimo, pero el camino del lote GD en realidad se detiene en el mínimo, mientras que tanto el GD estocástico como el GD de minilote continúan caminando. Sin embargo, no olvides que el GD por lotes toma mucho tiempo para dar cada paso, y el GD estocástico y el GD de mini-lote también alcanzarían el mínimo si usaras un buen horario de aprendizaje.\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0411.png)\n",
    "\n",
    "<center>(Figura 4-11. Rutas de descenso de gradiente en el espacio de parámetros)</center>\n",
    "\n",
    "La tabla 4-1 compara los algoritmos que hemos discutido hasta ahora para la regresión lineal⁠ (recuerde que m es el número de instancias de entrenamiento y n es el número de características).\n",
    "\n",
    "_Tabla 4-1. Comparación de algoritmos para regresión lineal_\n",
    "\n",
    "| **Algoritmo**       | **m grande** | **Soporte fuera del núcleo** | **n grande** | **Hiperparámetros** | **Requiere escalado** | **Scikit-Learn**    |\n",
    "|-----------------|----------|--------------------------|----------|-----------------|-------------------|-----------------|\n",
    "| Ecuación normal | Rapido   | No                       | Lento    | 0               | No                | N/A             |\n",
    "| SVD             | Rapido   | No                       | Lento    | 0               | No                | LinearRegresion |\n",
    "| Batch GD        | Lento    | No                       | Rapido   | 2               | Yes               | N/A             |\n",
    "| SGD             | Rapido   | Si                       | Rapido   | >=2             | Yes               | SGDRegressor    |\n",
    "| Mini-batch GD   | Rapido   | Si                       | Rapido   | >=2             | Yes               | N/A             |\n",
    "\n",
    "</br></br>\n",
    "Casi no hay diferencia después del entrenamiento: todos estos algoritmos terminan con modelos muy similares y hacen predicciones exactamente de la misma manera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfc0c9",
   "metadata": {},
   "source": [
    "## Regresión Polinómica\n",
    "\n",
    "**¿Qué pasa si tus datos son más complejos que una línea recta?** Sorprendentemente, puedes usar un modelo lineal para ajustar los datos no lineales. Una forma sencilla de hacer esto es agregar poderes de cada característica como nuevas características, y luego entrenar un modelo lineal en este conjunto extendido de características. Esta técnica se llama **regresión polinómica**.\n",
    "\n",
    "Echemos un vistazo a un ejemplo. \n",
    "En primer lugar, generaremos algunos datos no lineales (ver Figura 4-12), basados en una ecuación cuadrática simple, que es una ecuación de la forma **y = ax² +bx + c**, más algo de ruido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550a4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5a77a",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0412.png)\n",
    "\n",
    "_Figura 4-12. Conjunto de datos no lineales y ruidoso generado_\n",
    "\n",
    "Es evidente que una línea recta nunca se ajustará correctamente a estos datos. Entonces, usemos la clase `PolynomialFeatures` de Scikit-Learn para transformar nuestros datos de entrenamiento, agregando el cuadrado (polinomio de segundo grado) de cada característica en el conjunto de entrenamiento como una nueva característica (en este caso solo hay una característica):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce471cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75275929])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef09e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75275929,  0.56664654])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b67d7c",
   "metadata": {},
   "source": [
    "`X_poly` ahora contiene la característica original de `X` más el cuadrado de esta característica. \n",
    "\n",
    "Ahora podemos ajustar un modelo de `LinearRegression` a estos datos de entrenamiento extendidos (Figura 4-13):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11ca058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.78134581]), array([[0.93366893, 0.56456263]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a295c8b",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0413.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6769c90",
   "metadata": {},
   "source": [
    "No nada está mal: las estimaciones del modelo **y^ = 0.56(x1)^2 + 0.93(x1) + 1.78** cuando, de hecho, la función original era y = 0.5(x1)^2 + 1.0(x1) + 2.0 + RuidoGausiano.\n",
    "\n",
    "Tenga en cuenta que cuando hay varias características, la regresión polinómica es capaz de encontrar relaciones entre características, algo que un modelo de regresión lineal simple no puede hacer. Esto es posible gracias al hecho de que PolynomialFeatures también agrega todas las combinaciones de características hasta el grado dado. Por ejemplo, si hubiera dos características a y b, `PolynomialFeatures` con `degree=3` no solo agregaría las características _a2, a3, b2 y b3_, sino también las combinaciones _ab, a2b y ab2_.\n",
    "\n",
    "#### ------------------------------ ADVERTENCIA ---------------------------------\n",
    "`PolynomialFeatures(degree=d)`¡Transforma una matriz que contiene _n_ características en una matriz que contiene _(n + d)! / d! n!_ características, donde _n!_ es el factorial de _n_, igual a _1 × 2 × 3 × ⋯ ×n_. \n",
    "\n",
    "¡Cuidado con la explosión combinatoria del número de características!\n",
    "#### ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9174b47",
   "metadata": {},
   "source": [
    "## Curvas de aprendizaje\n",
    "\n",
    "Si realiza una regresión polinómica de alto grado, es probable que ajuste los datos de entrenamiento mucho mejor que con la regresión lineal simple. Por ejemplo, la Figura 4-14 aplica un modelo polinómico de 300 grados a los datos de entrenamiento anteriores, y compara el resultado con un modelo lineal puro y un modelo cuadrático (polinomio de segundo grado). Observa cómo el modelo polinómico de 300 grados se mueve para acercarse lo más posible a las instancias de entrenamiento.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0414.png)\n",
    "\n",
    "_Figura 4-14. Regresión polinómica de alto grado_\n",
    "\n",
    "Este modelo de regresión polinómica de alto grado está sobreajustando severamente los datos de entrenamiento, mientras que el modelo lineal lo está encajando. El modelo que mejor se generalizará en este caso es el modelo cuadrático, que tiene sentido porque los datos se generaron utilizando un modelo cuadrático. Pero en general no sabrás qué función generó los datos, así que ¿cómo puedes decidir qué tan complejo debe ser tu modelo? ¿Cómo puedes saber que tu modelo está sobreajustando o no ajustando los datos?\n",
    "\n",
    "En el capítulo 2, utilizó la validación cruzada para obtener una estimación del rendimiento de la generalización de un modelo. Si un modelo funciona bien en los datos de entrenamiento, pero se generaliza mal de acuerdo con las métricas de validación cruzada, entonces su modelo se está ajustando en exceso. Si funciona mal en ambos, entonces no se ajusta bien. Esta es una forma de saber cuándo un modelo es demasiado simple o demasiado complejo.\n",
    "\n",
    "Otra forma de saberlo es observar las curvas de aprendizaje, que son gráficos del error de entrenamiento y del error de validación del modelo en función de la iteración de entrenamiento: simplemente evalúe el modelo a intervalos regulares durante el entrenamiento tanto en el conjunto de entrenamiento como en el conjunto de validación. y trazar los resultados. Si el modelo no se puede entrenar de forma incremental (es decir, si no admite `parcial_fit()` o `warm_start`), entonces debes entrenarlo varias veces en subconjuntos gradualmente más grandes del conjunto de entrenamiento.\n",
    "\n",
    "Scikit-Learn tiene una útil función `learning_curve()` para ayudar con esto: entrena y evalúa el modelo mediante validación cruzada. De forma predeterminada, vuelve a entrenar el modelo en subconjuntos crecientes del conjunto de entrenamiento, pero si el modelo admite el aprendizaje incremental, puede configurar  `exploit_incremental_learning=True` al llamar a `learning_curve()` y, en su lugar, entrenará el modelo de forma incremental. La función devuelve los tamaños del conjunto de entrenamiento en los que evaluó el modelo y las puntuaciones de entrenamiento y validación que midió para cada tamaño y para cada pliegue de validación cruzada. Usemos esta función para observar las curvas de aprendizaje del modelo de regresión lineal simple (ver Figura 4-15):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b13e8ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4e14f",
   "metadata": {},
   "source": [
    "Este modelo no es apto. Para ver por qué, primero echemos un vistazo al error de entrenamiento. Cuando solo hay una o dos instancias en el conjunto de entrenamiento, el modelo puede encajar perfectamente, por lo que la curva comienza en cero. Pero a medida que se agregan nuevas instancias al conjunto de entrenamiento, se hace imposible que el modelo se ajuste perfectamente a los datos de entrenamiento, tanto porque los datos son ruidosos como porque no son lineales en absoluto. Por lo tanto, el error en los datos de entrenamiento aumenta hasta que alcanza una meseta, momento en el que agregar nuevas instancias al conjunto de entrenamiento no hace que el error promedio sea mucho mejor o peor. Ahora veamos el error de validación. Cuando el modelo se entrena en muy pocas instancias de entrenamiento, es incapaz de generalizar correctamente, por lo que el error de validación es inicialmente bastante grande. Luego, a medida que se muestra al modelo más ejemplos de entrenamiento, aprende y, por lo tanto, el error de validación disminuye lentamente. Sin embargo, una vez más, una línea recta no puede hacer un buen trabajo al modelar los datos, por lo que el error termina en una meseta, muy cerca de la otra curva.\n",
    "\n",
    "Estas curvas de aprendizaje son típicas de un modelo que no se ajusta bien. Ambas curvas han alcanzado una meseta; son cercanas y bastante altas.\n",
    "\n",
    "\n",
    "#### -------------------------------------- TIP ---------------------------------------\n",
    "Si su modelo no se ajusta bien a los datos de entrenamiento, añadir más ejemplos de entrenamiento no ayudará. Necesitas usar un modelo mejor o idear mejores características.\n",
    "#### ----------------------------------------------------------------------------------\n",
    "\n",
    "Ahora veamos las curvas de aprendizaje de un modelo polinómico de décimo grado en los mismos datos (Figura 4-16):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb4c4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=10, include_bias=False),\n",
    "    LinearRegression())\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97438ce7",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0416.png)\n",
    "\n",
    "_Figura 4-16. Curvas de aprendizaje para el modelo polinomial de décimo grado_\n",
    "\n",
    "Estas curvas de aprendizaje se parecen un poco a las anteriores, pero hay dos diferencias muy importantes:\n",
    "\n",
    "- El error en los datos de entrenamiento es mucho menor que antes.\n",
    "\n",
    "- Hay un espacio entre las curvas. Esto significa que el modelo se desempeña significativamente mejor en los datos de entrenamiento que en los datos de validación, que es el sello distintivo de un modelo de sobreajuste. Sin embargo, si usaras un conjunto de entrenamiento mucho más grande, las dos curvas seguirían acercándose.\n",
    "\n",
    "#### ------------------------------------ TIP ----------------------------------------\n",
    "Una forma de mejorar un modelo de sobreajuste es alimentarlo con más datos de entrenamiento hasta que el error de validación llegue al error de entrenamiento.\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "#### ----------------- LA COMPENSACIÓN DE SESGO/VARIANZA--------------\n",
    "Un resultado teórico importante de la estadística y el aprendizaje automático es el hecho de que el error de generalización de un modelo se puede expresar como la suma de tres errores muy diferentes:\n",
    "\n",
    "* **sesgo**: Esta parte del error de generalización se debe a suposiciones erróneas, como asumir que los datos son lineales cuando en realidad son cuadráticos. Lo más probable es que un modelo de alto sesgo no se ajuste a los datos de entrenamiento\n",
    "\n",
    "* **varianza**: Esta parte se debe a la excesiva sensibilidad del modelo a pequeñas variaciones en los datos de entrenamiento. Es probable que un modelo con muchos grados de libertad (como un modelo polinómico de alto grado) tenga una alta varianza y, por lo tanto, se ajuste demasiado a los datos de entrenamiento.\n",
    "\n",
    "* **error irreducible**: Esta parte se debe a la ruidosidad de los datos en sí. La única manera de reducir esta parte del error es limpiar los datos (por ejemplo, arreglar las fuentes de datos, como los sensores rotos, o detectar y eliminar valores atípicos).\n",
    "\n",
    "Aumentar la complejidad de un modelo normalmente aumentará su variación y reducirá su sesgo. Por el contrario, reducir la complejidad de un modelo aumenta su sesgo y reduce su variación. Esta es la razón por la que se llama una compensación.\n",
    "#### -----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e889b9c",
   "metadata": {},
   "source": [
    "# Modelos lineales regularizados\n",
    "\n",
    "Como vio en los capítulos 1 y 2, una buena manera de reducir el sobreajuste es regularizar el modelo (es decir, restringirlo): cuantos menos grados de libertad tenga, más difícil será para él sobreadaptar los datos. Una forma sencilla de regularizar un modelo polinómico es reducir el número de grados polinómicos.\n",
    "\n",
    "Para un modelo lineal, la regularización se logra normalmente restringiendo los pesos del modelo. Ahora veremos la regresión de la cresta, la regresión del lazo y la regresión de la red elástica, que implementan tres formas diferentes de restringir los pesos.\n",
    "\n",
    "\n",
    "## Regresión de cresta\n",
    "\n",
    "La regresión de cresta (también llamada _regularización de Tikhonov_) es una versión regularizada de la regresión lineal: un término de regularización igual a\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/JyV5zD1/Captura-de-pantalla-2023-08-24-a-las-5-14-15.png\" alt=\"Captura-de-pantalla-2023-08-24-a-las-5-14-15\" border=\"0\"></a>\n",
    "\n",
    "se añade al MSE. Esto obliga al algoritmo de aprendizaje no solo a ajustarse a los datos, sino también a mantener los pesos del modelo lo más pequeños posible. Tenga en cuenta que el plazo de regularización solo debe añadirse a la función de coste durante la formación. Una vez que se entrene el modelo, desea utilizar el MSE no regularizado (o el RMSE) para evaluar el rendimiento del modelo.\n",
    "\n",
    "El hiperparámetro α controla cuánto quieres regularizar el modelo. Si α = 0, entonces la regresión de cresta es solo regresión lineal. Si α es muy grande, entonces todos los pesos terminan muy cerca de cero y el resultado es una línea plana que pasa por la media de los datos. \n",
    "La ecuación 4-8 presenta la función de costo de regresión de cresta.\n",
    "\n",
    "### Ecuación 4-8. Función de costo de regresión de Ridge\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/TqbLD9J/Captura-de-pantalla-2023-08-24-a-las-15-33-38.png\" alt=\"Captura-de-pantalla-2023-08-24-a-las-15-33-38\" border=\"0\"></a>\n",
    "\n",
    "Tenga en cuenta que el término de sesgo θ0 no está regularizado (la suma comienza en i = 1, no en 0). \n",
    "Si definimos w como el vector de pesos de característica (θ1 a θn), entonces el término de regularización es igual a α(∥ w ∥2)2 / m, donde ∥ w ∥2 representa la norma ℓ2 del vector de peso.⁠ Para el gradiente de descenso.\n",
    "\n",
    "\n",
    "#### -------------------------------- ADVERTENCIA ------------------------------------\n",
    "Es importante escalar los datos (por ejemplo, utilizando un `StandardScaler`) antes de realizar la regresión de cresta, ya que es sensible a la escala de las características de entrada. \n",
    "Esto es cierto para la mayoría de los modelos regularizados.\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "La Figura 4-17 muestra varios modelos de cresta que se entrenaron con datos lineales muy ruidosos utilizando diferentes valores de α. \n",
    "\n",
    "A la izquierda, se utilizan modelos de crestas planas, que conducen a predicciones lineales. \n",
    "\n",
    "A la derecha, los datos primero se expanden usando `PolynomialFeatures(degree = 10)`, luego se escalan usando un `StandardScaler` y, finalmente, los modelos de cresta se aplican a las características resultantes: esto es regresión polinómica con regularización de cresta. \n",
    "\n",
    "Observa cómo el aumento de α conduce a predicciones más planas (es decir, menos extremas, más razonables), lo que reduce la varianza del modelo pero aumenta su sesgo.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0417.png)\n",
    "\n",
    "_Figura 4-17. Modelos lineales (izquierda) y polinómicos (derecha), ambos con varios niveles de regularización de crestas_\n",
    "\n",
    "Al igual que con la regresión lineal, podemos realizar la regresión de cresta ya sea calculando una ecuación de forma cerrada o realizando un descenso de gradiente. Los pros y los contras son los mismos. La ecuación 4-9 muestra la solución de forma cerrada, donde **A** es la _matriz de identidad_ (n + 1) × (n + 1), excepto con un 0 en la celda superior izquierda, correspondiente al término de sesgo.\n",
    "\n",
    "### Ecuación 4-9. Solución de forma cerrada de regresión de cresa\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/n77pXft/Captura-de-pantalla-2023-08-24-a-las-15-39-44.png\" alt=\"Captura-de-pantalla-2023-08-24-a-las-15-39-44\" border=\"0\"></a>\n",
    "\n",
    "Aquí se explica cómo realizar la regresión de cresta con Scikit-Learn utilizando una solución de forma cerrada (una variante de la ecuación 4-9 que utiliza una técnica de factorización matricial de André-Louis Cholesky):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87811c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.82899748]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610c58a",
   "metadata": {},
   "source": [
    "Y usando el descenso del gradiente estocástico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "881ff491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.82830117])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n",
    "                        max_iter=1000, eta0=0.01, random_state=42)\n",
    "\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2836b",
   "metadata": {},
   "source": [
    "El hiperparámetro de penalización (`penalty`) establece el tipo de término de regularización que se utilizará. Especificar `\"l2\"` indica que desea que SGD agregue un término de regularización a la función de costo de MSE igual a alfa (`alpha`) multiplicado por el cuadrado de la norma ℓ2 del vector de peso. \n",
    "Esto es como la regresión de cresta, excepto que en este caso no hay división por m; es por eso que pasamos `alfa=0,1 / m`, para obtener el mismo resultado que `Ridge(alpha=0,1)`.\n",
    "\n",
    "\n",
    "#### ------------------------------------ TIP ----------------------------\n",
    "La clase `RidgeCV` también realiza una regresión de crestas, pero ajusta automáticamente los hiperparámetros mediante validación cruzada. Es más o menos equivalente a usar `GridSearchCV`, pero está optimizado para la regresión de crestas y se ejecuta mucho más rápido. Varios otros estimadores (en su mayoría lineales) también tienen variantes de CV eficientes, como `LassoCV` y `ElasticNetCV`.\n",
    "#### --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## Regresión de Lasso\n",
    "\n",
    "\n",
    "La _contracción mínima absoluta_ y la regresión del operador de selección (generalmente llamada simplemente regresión de lazo) es otra versión regularizada de la regresión lineal: al igual que la regresión de cresta, agrega un término de regularización a la función de costo, pero utiliza la norma ℓ1 del vector de peso en lugar del cuadrado de la norma �� Tenga en cuenta que la norma ℓ1 se multiplica por 2α, mientras que la norma ℓ2 se multiplicó por α / m en la regresión de crestas. \n",
    "\n",
    "Estos factores se eligieron para garantizar que el valor óptimo de α sea independiente del tamaño del conjunto de entrenamiento: diferentes normas conducen a diferentes factores (consulte el número 15657 de Scikit-Learn para obtener más detalles).\n",
    "\n",
    "### Ecuación 4-10. Función de coste de regresión de lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d022be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
