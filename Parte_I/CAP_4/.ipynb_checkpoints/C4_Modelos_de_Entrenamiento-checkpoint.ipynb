{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2047f37",
   "metadata": {},
   "source": [
    "# Capítulo 4: Modelos de entrenamiento\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Hasta ahora hemos tratado los modelos de aprendizaje automático y sus algoritmos de entrenamiento principalmente como cajas negras. \n",
    "\n",
    "Es posible que te haya sorprendido lo mucho que puedes hacer sin saber nada sobre lo que hay bajo el capó: optimizaste un sistema de regresión, mejoraste un clasificador de imágenes de dígitos e incluso construiste un clasificador de spam desde cero, todo sin saber cómo funcionan realmente. \n",
    "\n",
    "De hecho, en muchas situaciones no es necesario conocer los detalles de la implementación.\n",
    "\n",
    "Sin embargo, tener una buena comprensión de cómo funcionan las cosas puede ayudarte a encontrar rápidamente el modelo apropiado, el algoritmo de entrenamiento adecuado para usar y un buen conjunto de hiperparámetros para tu tarea. \n",
    "\n",
    "Comprender lo que hay \"bajo el capó\" también le ayudará a depurar los problemas y a realizar el análisis de errores de manera más eficiente. \n",
    "\n",
    "Por último, la mayoría de los temas tratados en este capítulo serán esenciales para comprender, construir y entrenar las redes neuronales (discutido en la Parte II de este libro).\n",
    "\n",
    "En este capítulo comenzaremos por ver el modelo de regresión lineal, uno de los modelos más simples que existen. Discutiremos dos formas muy diferentes de entrenarlo:\n",
    "\n",
    "* Usando una ecuación de \"**forma cerrada**\"⁠ que **calcule directamente** los parámetros del modelo que mejor se ajustan al modelo al conjunto de entrenamiento (es decir, los parámetros del modelo que minimizan la función de costo sobre el conjunto de entrenamiento).\n",
    "\n",
    "\n",
    "* Utilizando un enfoque de optimización **iterativo** llamado **descenso de gradiente** (GD) que ajusta **gradualmente** los parámetros del modelo para minimizar la función de costo sobre el conjunto de entrenamiento, eventualmente convergiendo con el mismo conjunto de parámetros que el primer método. Veremos algunas variantes de descenso de gradiente que usaremos una y otra vez cuando estudiemos las redes neuronales en la Parte II: **GD de lote, GD de minibate y GD estocástico**.\n",
    "\n",
    "\n",
    "A continuación, veremos la **regresión polinómica**, un modelo más complejo que puede adaptarse a conjuntos de datos **no lineales**. \n",
    "\n",
    "Dado que este modelo tiene **más parámetros que la regresión lineal**, es más propenso a **sobreajustar** los datos de entrenamiento. \n",
    "\n",
    "Exploraremos cómo detectar si este es el caso o no utilizando **curvas de aprendizaje**, y luego veremos varias **técnicas de regularización** que pueden reducir el riesgo de sobreadaptar el conjunto de entrenamiento.\n",
    "\n",
    "Por último, examinaremos dos **modelos más que se utilizan** comúnmente para las tareas de **clasificación**: **regresión logística y regresión softmax**.\n",
    "\n",
    "\n",
    "### ------------------------ ADVERTENCIA ------------------------\n",
    "\n",
    "Habrá bastantes ecuaciones matemáticas en este capítulo, utilizando nociones básicas de álgebra lineal y cálculo. \n",
    "\n",
    "Para entender estas ecuaciones, necesitarás saber qué son los vectores y las matrices; cómo transponerlos, multiplicarlos e invertirlos; y qué son las derivadas parciales. \n",
    "\n",
    "Si no estás familiarizado con estos conceptos, consulte los tutoriales introductorios de álgebra lineal y cálculo disponibles como cuadernos Jupyter en el material complementario en línea. \n",
    "\n",
    "Para aquellos que son realmente alérgicos a las matemáticas, aún así deben pasar por este capítulo y simplemente omitir las ecuaciones; con suerte, el texto será suficiente para ayudarle a entender la mayoría de los conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dc0d0",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "</bre/>\n",
    "En el capítulo 1 se analizó un modelo de **regresión simple** para comprobar la satisfacción con la vida:\n",
    "\n",
    "**<center>satif_vida = θ0 + θ1 × GDP_per_capita</center>**\n",
    "\n",
    "Es solo una función lineal (línea recta) con una sola característica de entrada y dos parámetros:\n",
    "\n",
    "- Característica: `GDP_per_capita`\n",
    "- Parámetros: `θ0` y `θ1`\n",
    "\n",
    "Un modelo lineal funciona haciendo una predicción calculando una suma ponderada de características de entrada más una constante (sesgo o término de intersección).\n",
    "\n",
    "\n",
    "### Ecuación 4-1: Predicción del modelo de Regresión Lineal\n",
    "</br>\n",
    "\n",
    "**<center>y = θ0 + θ1X1 + θ2X2 + ... + θnXn</center>**\n",
    "\n",
    "* **y** = valor predicho\n",
    "* **n** = número de características\n",
    "* **Xi** = valor de la primera característica\n",
    "* **θj** = parámetro del modelo j, incluido el término de sesgo θ0 y los pesos de características θ1, θ2, ..., θn.\n",
    "\n",
    "(Y esto se puede escribir de forma más concisa usando una forma vectorizada).\n",
    "\n",
    "\n",
    "### Ecuación 4-2: Predicción del modelo de Regresión Lineal (FORMA VECTORIAL)\n",
    "</br>\n",
    "\n",
    "**<center>y = hθ(X) = θ*X</center>**\n",
    "\n",
    "* **hθ** = función de hipótesis, que usa los parámetros del modelo θ\n",
    "* **θ** = número de características\n",
    "* **X** = vector de parámetros del modelo, que contiene el término de sesgo θ0 y los pesos de características θ1 a θn.\n",
    "* **θ*X** = producto punto de los vectores θ y X, que es igual a θ0x0 + θ1x1 + θ2x2 + ... θnxn\n",
    "\n",
    "</br></br>\n",
    "**---------------------- NOTA ------------------------**\n",
    "\n",
    "En el aprendizaje automático, los vectores a menudo se representan como vectores de columna (row-vector), que son matrices 2D con una sola columna.\n",
    "\n",
    "Si **θ** y **x** son vectores de columna, entonces la predicción es `y = θ^T*X`, donde `θ^T` es la transposición de `θ` (un vector fila en lugar de un vector columna) y `θ^T*X` es la multiplicación de la matriz de `θ^T` y `X`.\n",
    "\n",
    "Por supuesto, es la misma predicción, excepto que ahora se representa como una matriz de una sola celda en lugar de un valor escalar. \n",
    "\n",
    "**------------------------------------------------------**\n",
    "</br>\n",
    "\n",
    "Vale, ese es el modelo de regresión lineal, pero ¿cómo lo entrenamos? Bueno, recuerda que entrenar un modelo significa establecer sus parámetros para que el modelo se ajuste mejor al conjunto de entrenamiento. \n",
    "\n",
    "Para este propósito, primero necesitamos una medida de qué tan bien (o mal) se ajusta el modelo a los datos de entrenamiento. \n",
    "\n",
    "En el capítulo 2 vimos que la medida de rendimiento más común de un modelo de regresión es el **error cuadrado medio** de la raíz. \n",
    "\n",
    "Por lo tanto, para entrenar un modelo de regresión lineal, necesitamos encontrar el valor de θ que minimice el **RMSE**. \n",
    "\n",
    "En la práctica, es más sencillo minimizar el error medio al cuadrado (MSE) que el RMSE, y conduce al mismo resultado (porque el valor que minimiza una función positiva también minimiza su raíz cuadrada).\n",
    "\n",
    "**---------------------- ADVERTENCIA ------------------------**\n",
    "\n",
    "Los algoritmos de aprendizaje a menudo optimizarán una función de pérdida diferente durante el entrenamiento que la medida de rendimiento utilizada para evaluar el modelo final. Esto se debe generalmente a que la función es más fácil de optimizar y/o porque tiene términos adicionales necesarios solo durante el entrenamiento (por ejemplo, para la regularización). Una buena métrica de rendimiento está lo más cerca posible del objetivo comercial final. Una buena pérdida de entrenamiento es fácil de optimizar y está fuertemente correlacionada con la métrica. Por ejemplo, los clasificadores a menudo se entrenan utilizando una función de costo como la pérdida de registro (como verá más adelante en este capítulo), pero se evalúan utilizando precisión/retirada. La pérdida de registro es fácil de minimizar, y hacerlo generalmente mejorará la precisión/recall.\n",
    "\n",
    "**----------------------------------------------------------------**\n",
    "\n",
    "El MSE de una hipótesis de regresión lineal hθ en un set de entrenamiento ´X´ se calcula utilizando la ecuación 4-3.\n",
    "\n",
    "\n",
    "### Ecuación 4-3: Función de coste de MSE para un modelo de regresión lineal\n",
    "</br>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mrow>\n",
    "    <mtext>MSE</mtext>\n",
    "    <mrow>\n",
    "      <mo>(</mo>\n",
    "      <mi mathvariant=\"bold\">X</mi>\n",
    "      <mo>,</mo>\n",
    "      <msub><mi>h</mi> <mi mathvariant=\"bold\">θ</mi> </msub>\n",
    "      <mo>)</mo>\n",
    "    </mrow>\n",
    "    <mo>=</mo>\n",
    "    <mstyle scriptlevel=\"0\" displaystyle=\"true\">\n",
    "      <mfrac><mn>1</mn> <mi>m</mi></mfrac>\n",
    "    </mstyle>\n",
    "    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>\n",
    "    <msup><mrow><mo>(</mo><msup><mi mathvariant=\"bold\">θ</mi> <mo>⊺</mo> </msup><msup><mi mathvariant=\"bold\">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup>\n",
    "  </mrow>\n",
    "</math>\n",
    "\n",
    "MSE(X,hθ) = 1/m * SUM i to m (θT * X^i - y^i)^2\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/qsT73Cj/Captura-de-pantalla-2023-08-18-a-las-20-00-06.png\" alt=\"Captura-de-pantalla-2023-08-18-a-las-20-00-06\" border=\"0\"></a><br /><a target='_blank' href='https://imgbb.com/'></a><br />\n",
    "\n",
    "La mayoría de estas anotaciones se presentaron en el capítulo 2. La única diferencia es que escribimos hθ en lugar de solo h para dejar claro que el modelo está parametrizado por el vector θ. Para simplificar las anotaciones, solo escribiremos MSE(θ) en lugar de MSE(X, hθ).\n",
    "\n",
    "### Ecuación normal\n",
    "\n",
    "Para encontrar el valor de θ que minimiza el MSE, existe una solución de forma cerrada, en otras palabras, una ecuación matemática que da el resultado directamente. \n",
    "\n",
    "Esto se llama **ecuación normal** y **nunca vamos a calcular en problemas complicados** porque es increiblemente lento (imposible computacionalmente) y tendremos que saber como acercarnos a la solución sin calcular esto.\n",
    "\n",
    "\n",
    "### Ecuación 4-4: Ecuación Normal\n",
    "\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/51vHrfN/Captura-de-pantalla-2023-08-18-a-las-20-06-44.png\" alt=\"Captura-de-pantalla-2023-08-18-a-las-20-06-44\" border=\"0\"></a>\n",
    "\n",
    "Donde:\n",
    "\n",
    "* **θ** = valor de θ que minimiza la función de coste.\n",
    "* **y** = vector de valores onjetivo que contiene **y(1)** a **y(m)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b6db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos algunos datos con forma lineal para probar que la ecuación funciona\n",
    "# Conjunto de datos lineal generado al azar.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "m = 100  # número de instancias\n",
    "X = 2 * np.random.rand(m, 1)  # vector columna\n",
    "y = 4 + 3 * X + np.random.randn(m, 1)  # vector columna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42288497",
   "metadata": {},
   "source": [
    "![data_points_lineal](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca7379",
   "metadata": {},
   "source": [
    "Ahora vamos a calcular **θ^** usando la **Ecuación Normal**. \n",
    "\n",
    "Usamos la función `inv()` del módulo de álgebra lineal de NumPy  (`np.linalg`) para hacer la inversa de la matriz, y el método `dot()` para la multiplicación de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348d63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "X_b = add_dummy_feature(X)  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c2ce0",
   "metadata": {},
   "source": [
    "### -------------------------- NOTA --------------------------\n",
    "\n",
    "El operador `@` realiza la multiplicación de matrices. \n",
    "\n",
    "Si **A** y **B** son matrices NumPy, entonces **A @ B** es equivalente a `np.matmul(A, B)`. \n",
    "\n",
    "Muchas otras bibliotecas, como TensorFlow, PyTorch y JAX, también admiten el operador **@**. Sin embargo, no puede usar @ en arreglos puros de Python (es decir, listas de listas).\n",
    "\n",
    "### -------------------------------------------------------------\n",
    "\n",
    "La función que usamos para generar los datos es `y = 4+3x1 + ruido_gaussiano`.\n",
    "Veamos que encontró la ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0bd692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba551d4",
   "metadata": {},
   "source": [
    "Habríamos esperado **(θ0 = 4** y **θ1 = 3)** en lugar de **(θ0 = 4.215** y **θ1 = 2.770)**. \n",
    "\n",
    "Lo suficientemente cerca, pero el ruido hizo imposible recuperar los parámetros exactos de la función original. \n",
    "\n",
    "**Cuanto más pequeño y ruidoso sea el conjunto de datos, más difícil se vuelve**.\n",
    "\n",
    "Ahora podemos hacer predicciones usando **θ^**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = add_dummy_feature(X_new)  # agregar x0 = 1 a cada instancia\n",
    "y_predict = X_new_b @ theta_best\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57611120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizamos las predicciones del modelo de R.Lineal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cbc570",
   "metadata": {},
   "source": [
    "Realizar una regresión lineal con **Scickit-Learn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10290589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e77552",
   "metadata": {},
   "source": [
    "Observa que Scikit-Learn separa el término de bias (`intercept_`) de los pesos de características (`coef_`).\n",
    "\n",
    "La clase LinearRegression se basa en la función `scipy.linalg.lstsq()` (el nombre se refiere a \"mínimos cuadrados\"), a la que se puede llamar directamente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df62e8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f3da7",
   "metadata": {},
   "source": [
    "Esta función calcula **θ^ = X+y**, donde **X+** es la pseudoinversa de **X** (específicamente la inversa de Moore-Penrose).\n",
    "\n",
    "Puedes usar `np.linalg.pinv()` para hacer la pseudoinversa directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad75318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21810da",
   "metadata": {},
   "source": [
    "La pseudoinversa en sí misma se calcula utilizando una técnica de factorización de matriz estándar llamada descomposición de valores singulares (**SVD**) que puede descomponer la matriz del conjunto de entrenamiento **X** en la multiplicación de matrices de tres matrices U Σ V⊺ (`vernumpynumpy.linalg.svd()`). \n",
    "\n",
    "La pseudoinversa se calcula como:\n",
    "\n",
    "**<center>X+ = VΣ+U⊺</center>** \n",
    "\n",
    "Para calcular la matriz Σ+, el algoritmo toma Σ y establece en cero todos los valores más pequeños que un pequeño valor de umbral, luego reemplaza todos los valores distintos de cero con su inverso y, finalmente, transpone la matriz resultante. \n",
    "\n",
    "Este enfoque es más eficiente que calcular la ecuación Normal, además de que maneja bien los casos de borde: de hecho, la ecuación Normal puede no funcionar si la matriz **X⊺X** no es invertible (es decir, singular), como si m < n o si algunas características son redundantes, pero el pseudoinverse siempre está definido.\n",
    "\n",
    "\n",
    "### Complejidad computacional\n",
    "</br></br>\n",
    "La ecuación normal calcula la inversa de **X⊺ X**, que es una matriz (n + 1) × (n + 1) (donde n es el número de características). \n",
    "\n",
    "La complejidad computacional de invertir dicha matriz suele ser de _O(n2.4) a O(n3)_, dependiendo de la implementación. \n",
    "\n",
    "En otras palabras, si duplicas el número de características, multiplicas el tiempo de cálculo por aproximadamente 22,4 = 5,3 a 23 = 8.\n",
    "\n",
    "El enfoque SVD utilizado por la clase `LinearRegression` de Scikit-Learn se trata de _O(n2)_. \n",
    "\n",
    "**Si duplicas la cantidad de características, multiplica el tiempo de cálculo por aproximadamente 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37e98d",
   "metadata": {},
   "source": [
    "#### ---------------------------------- ADVERTENCIA ----------------------------------\n",
    "\n",
    "Tanto la **ecuación normal** como el enfoque **SVD** se **ralentizan** mucho cuando el número de características **crece** (por ejemplo, 100.000). \n",
    "\n",
    "En el lado positivo, **ambos son lineales con respecto al número de instancias** en el conjunto de entrenamiento (son _O(m)_), por lo que manejan grandes conjuntos de entrenamiento de manera eficiente, siempre que puedan caber en la memoria.\n",
    "### --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Además, una vez que hayas entrenado tu modelo de regresión lineal (usando la ecuación normal o cualquier otro algoritmo), **las predicciones son muy rápidas: la complejidad computacional es lineal con respecto tanto al número de instancias en las que desea hacer predicciones como al número de características**. En otras palabras, hacer predicciones sobre el doble de casos (o el doble de características) tomará aproximadamente el doble de tiempo.\n",
    "\n",
    "**<center>x2 características = x2 tiempo</center>**\n",
    "\n",
    "#### Ahora veremos una forma muy diferente de entrenar un modelo de regresión lineal, que es más adecuado para los casos en los que hay un gran número de características o demasiadas instancias de entrenamiento para encajar en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05772e8",
   "metadata": {},
   "source": [
    "## Descenso del gradiente\n",
    "\n",
    "</br></br>\n",
    "El _descenso de gradiente_ es un algoritmo de optimización genérico capaz de encontrar **soluciones óptimas** para una amplia gama de problemas. \n",
    "\n",
    "La idea general del descenso del gradiente es **ajustar los parámetros de forma iterativa** para minimizar una **función de coste*.\n",
    "\n",
    "Supongamos que estás perdido en las montañas en una densa niebla, y solo puedes sentir la pendiente del suelo debajo de tus pies. Una buena estrategia para llegar al fondo del valle rápidamente es ir cuesta abajo en dirección a la pendiente más empinada. \n",
    "Esto es exactamente lo que hace el descenso del gradiente: mide el gradiente local de la función de error con respecto al vector de parámetros θ, y va en la dirección del gradiente descendente. \n",
    "Una vez que el gradiente es cero (casi nunca pasa), ¡has alcanzado un mínimo!\n",
    "\n",
    "En la práctica, comienzas por llenar θ con valores aleatorios (esto se llama inicialización aleatoria). \n",
    "\n",
    "Luego lo mejoras gradualmente, dando un paso a la vez, cada paso tratando de disminuir la función de costo (por ejemplo, el MSE), hasta que el algoritmo converge al mínimo.\n",
    "\n",
    "![descenso_gradiente](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0403.png)\n",
    "\n",
    "(_En esta representación del descenso de gradiente, los parámetros del modelo se inicializan al azar y se ajustan repetidamente para minimizar la función de costo; el tamaño del paso de aprendizaje es proporcional a la pendiente de la función de costo, por lo que los pasos se reducen gradualmente a medida que el costo se acerca al mínimo_)\n",
    "\n",
    "</br></br>\n",
    "\n",
    "Un parámetro importante en el descenso de gradiente es el tamaño de los pasos, determinado por el hiperparámetro de la **tasa de aprendizaje** o learning rate.\n",
    "\n",
    "Si la tasa de aprendizaje es **demasiado pequeña**, entonces el algoritmo tendrá que pasar por **muchas iteraciones para converger**, lo que llevará mucho tiempo.\n",
    "\n",
    "![small_learning_rate](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0404.png)\n",
    "\n",
    "</br></br>\n",
    "\n",
    "Por otro lado, si la tasa de aprendizaje es **demasiado alta**, podrías saltar a través del valle y terminar en el otro lado, posiblemente incluso más alto de lo que estabas antes. \n",
    "Esto podría hacer que el algoritmo **diverja** (no converja), con valores cada vez más grandes, al no encontrar una buena solución.\n",
    "\n",
    "![big_learning_rate](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0405.png)\n",
    "\n",
    "Además, no todas las funciones de coste parecen cuencos bonitos y normales. Puede haber agujeros, crestas, mesetas y todo tipo de terreno irregular, lo que dificulta la convergencia al mínimo. \n",
    "\n",
    "La siguiente figura muestra los dos **desafíos principales** con el descenso en pendiente. \n",
    "\n",
    "Si la inicialización aleatoria inicia el algoritmo a la **izquierda**, entonces convergerá a un mínimo local, que no es tan bueno como el mínimo global. \n",
    "\n",
    "Si comienza a la **derecha**, entonces llevará mucho tiempo cruzar la meseta. Y si te detienes demasiado pronto, nunca alcanzarás el mínimo global.\n",
    "\n",
    "![desafios_descenso_gradiente](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0406.png)\n",
    "\n",
    "\n",
    "Afortunadamente, la función de costo de MSE para un modelo de regresión lineal resulta ser una función **convexa**, lo que significa que si eliges dos puntos en la curva, el segmento de línea que los une nunca está por debajo de la curva. Esto implica que no hay mínimos locales, solo un mínimo global. \n",
    "\n",
    "También es una función continua con una pendiente que nunca cambia abruptamente.⁠\n",
    "\n",
    "Estos dos hechos tienen una gran consecuencia: se garantiza que el descenso del gradiente se acerca arbitrariamente al mínimo global (si esperas lo suficiente y si la tasa de aprendizaje no es demasiado alta).\n",
    "\n",
    "Si bien la función de costo tiene la forma de un tazón, puede ser un tazón alargado si las características tienen escalas muy diferentes. \n",
    "\n",
    "La siguiente imagen muestra el descenso en gradiente en un conjunto de entrenamiento donde las características 1 y 2 tienen la misma escala (a la izquierda), y en un conjunto de entrenamiento donde la característica 1 tiene valores mucho más pequeños que la característica 2 (a la derecha).⁠\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0407.png)\n",
    "\n",
    "Como puedes ver, a la izquierda el algoritmo de descenso de gradiente va directamente hacia el mínimo, llegando así rápidamente, mientras que a la derecha primero va en una dirección casi ortogonal a la dirección del mínimo global, y termina con una larga marcha por un valle casi plano. \n",
    "\n",
    "Eventualmente alcanzará el mínimo, pero llevará mucho tiempo.\n",
    "\n",
    "#### ----------------------------- ADVERTENCIA -----------------------------\n",
    "\n",
    "Cuando uses el gradiente descendente, deberías asegurarte que todas las características tienen una escala similar (ej: usando `StandardScaler`), de lo contrario tardará mucho más en converger.\n",
    "#### -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844bb5a3",
   "metadata": {},
   "source": [
    "Este diagrama también ilustra el hecho de que **entrenar un modelo significa buscar una combinación de parámetros del modelo que minimice una función de costo** (sobre el conjunto de entrenamiento). \n",
    "\n",
    "Es una búsqueda en el espacio de parámetros del modelo. \n",
    "\n",
    "Cuantos más parámetros tenga un modelo, más dimensiones tenga este espacio y más difícil sea la búsqueda: buscar una aguja en un pajar de 300 dimensiones es mucho más complicado que en 3 dimensiones. \n",
    "\n",
    "Afortunadamente, dado que la función de costo es convexa en el caso de la regresión lineal, la aguja está simplemente en la parte inferior del tazón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d5375",
   "metadata": {},
   "source": [
    "## Descenso del gradiente por lotes (BATCH)\n",
    "\n",
    "Para implementar el descenso de gradiente, es necesario **calcular el gradiente de la función de costo con respecto a cada parámetro del modelo θj**. \n",
    "\n",
    "En otras palabras, necesitas calcular cuánto cambiará la función de costo si cambias θj solo un poco. Esto se llama derivada parcial. \n",
    "\n",
    "Es como preguntar: \"¿Cuál es la pendiente de la montaña bajo mis pies si miro hacia el este\"? y luego hacer la misma pregunta mirando hacia el norte (y así sucesivamente para todas las demás dimensiones, si puedes imaginar un universo con más de tres dimensiones). \n",
    "\n",
    "La ecuación 4-5 calcula la derivada parcial del MSE con respecto al parámetro θj, señalado ∂ MSE(θ) / ∂θj.\n",
    "\n",
    "### Ecuación 4-5: Derivadas parciales de la función de coste\n",
    "\n",
    "<a href=\"https://ibb.co/z51jft4\"><img src=\"https://i.ibb.co/Sx2WvZ3/Captura-de-pantalla-2023-08-20-a-las-21-19-22.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-19-22\" border=\"0\"></a>\n",
    "\n",
    "En lugar de calcular estas derivadas parciales individualmente, puedes usar la ecuación 4-6 para calcularlas todas de una sola vez. \n",
    "\n",
    "El vector de gradiente, señalado ∇θMSE(θ), contiene todas las derivadas parciales de la función de coste (una para cada parámetro del modelo).\n",
    "\n",
    "### Ecuación 4-6: Vector de gradiente de la función de coste\n",
    "\n",
    "<a href=\"https://ibb.co/zPx8DbB\"><img src=\"https://i.ibb.co/CV8JDvY/Captura-de-pantalla-2023-08-20-a-las-21-20-53.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-20-53\" border=\"0\"></a>\n",
    "</br>\n",
    "\n",
    "#### ---------------------- ADVERTENCIA -------------------\n",
    "</br>\n",
    "\n",
    "¡Ten en cuenta que esta fórmula implica cálculos sobre el conjunto completo de entrenamiento X, en cada paso de descenso de gradiente! \n",
    "\n",
    "Esta es la razón por la que el algoritmo se llama descenso de gradiente **por lotes**: **utiliza todo el lote de datos de entrenamiento en cada paso** (en realidad, el descenso de gradiente completo probablemente sería un mejor nombre). \n",
    "\n",
    "Como resultado, es **terriblemente lento en conjuntos de entrenamiento muy grandes** (en breve veremos algunos algoritmos de descenso de gradiente mucho más rápidos). \n",
    "\n",
    "Sin embargo, el descenso del gradiente se escala bien con el número de características; entrenar un modelo de regresión lineal cuando hay cientos de miles de características es mucho más rápido usando el descenso del gradiente que usando la ecuación normal o la descomposición de SVD.\n",
    "\n",
    "#### -----------------------------------------------------------\n",
    "\n",
    "Una vez que tengas el vector de gradiente, que apunta cuesta arriba, simplemente ve en la dirección opuesta para ir cuesta abajo. \n",
    "\n",
    "Esto significa restar ∇θMSE(θ) de θ. \n",
    "\n",
    "Aquí es donde entra en juego la tasa de aprendizaje η:⁠4 multiplica el vector de gradiente por η para determinar el tamaño del paso de descenso (Ecuación 4-7).\n",
    "\n",
    "### Ecuación 4-7: Paso (step) del descenso del gradiente\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/1MHnNPW/Captura-de-pantalla-2023-08-20-a-las-21-24-54.png\" alt=\"Captura-de-pantalla-2023-08-20-a-las-21-24-54\" border=\"0\"></a>\n",
    "\n",
    "Vamos a hacer una implementación básica de este algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b0b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_epochs = 1000\n",
    "m = len(X_b)  # número de instancias\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # parámetros del modelo inicializados aleatoriamente\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b6a17",
   "metadata": {},
   "source": [
    "No fue muy dificil! Cada iteración del conjunto de entrenamiento es llamada \"**época**\".\n",
    "\n",
    "Vamos a ver el resultado de `theta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47e8ecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb21860",
   "metadata": {},
   "source": [
    "Ey, ¡eso es exactamente lo que encontró la ecuación normal! \n",
    "\n",
    "El descenso en gradiente funcionó perfectamente. Pero, ¿y si hubieras usado una tasa de aprendizaje diferente (`eta`)? \n",
    "\n",
    "La figura siguiente muestra los primeros 20 pasos de descenso de gradiente utilizando tres tasas de aprendizaje diferentes. \n",
    "\n",
    "La línea en la parte inferior de cada gráfico representa el punto de partida aleatorio, luego cada época está representada por una línea cada vez más oscura.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0408.png)\n",
    "\n",
    "- A la **izquierda**, la tasa de aprendizaje es demasiado baja: el algoritmo finalmente llegará a la solución, pero llevará mucho tiempo. \n",
    "\n",
    "- En el **medio**, la tasa de aprendizaje se ve bastante bien: en solo unas pocas épocas, ya ha convergido hacia la solución. \n",
    "\n",
    "- A la **derecha**, la tasa de aprendizaje es demasiado alta: el algoritmo diverge, saltando por todas partes y en realidad se aleja cada vez más de la solución en cada paso.\n",
    "\n",
    "</br>\n",
    "\n",
    "Para **encontrar una buena tasa de aprendizaje**, puede utilizar la **búsqueda en grill** (`SearchGrill` en el capítulo 2). Sin embargo, es posible que desee **limitar el número de épocas** para que la búsqueda en la cuadrícula pueda eliminar los modelos que tardan demasiado en converger.\n",
    "\n",
    "Puede que te preguntes cómo establecer el **número de épocas**: \n",
    "\n",
    "Si es demasiado **bajo**, todavía estarás **lejos de la solución** óptima cuando el algoritmo se detenga; pero si es demasiado **alto**, **perderás el tiempo mientras los parámetros del modelo ya no cambian**. \n",
    "\n",
    "Una solución simple es establecer un número muy grande de épocas, pero interrumpir el algoritmo cuando el vector de gradiente se vuelve diminuto, es decir, cuando su norma se vuelve más pequeña que un número diminuto ε (llamado _tolerancia_), porque esto sucede cuando el descenso del gradiente (casi) ha alcanzado el mínimo.\n",
    "\n",
    "#### --------------------- TASA DE CONVERGENCIA -------------------------\n",
    "Cuando la función de costo es convexa y su pendiente no cambia abruptamente (como es el caso de la función de costo de MSE), el descenso del gradiente de lote con una tasa de aprendizaje fija eventualmente convergerá a la solución óptima, pero es posible que tenga que esperar un tiempo: puede tomar iteraciones O(1/ε) para alcanzar el óptimo dentro de un rango de Si divides la tolerancia entre 10 para tener una solución más precisa, entonces es posible que el algoritmo tenga que funcionar unas 10 veces más tiempo.\n",
    "#### ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e5640",
   "metadata": {},
   "source": [
    "## Descenso del gradiente estocástico\n",
    "</br></br>\n",
    "El principal problema con el descenso del gradiente por **lotes** (batch) es el hecho de que **utiliza todo el conjunto de entrenamiento** para calcular los gradientes en cada paso, lo que lo hace **muy lento cuando el conjunto de entrenamiento es grande**. \n",
    "\n",
    "En el extremo opuesto, el descenso del gradiente **estocástico** elige una **instancia aleatoria** en el conjunto de entrenamiento en cada paso y calcula los gradientes basados **solo en esa única instancia**. \n",
    "\n",
    "Obviamente, trabajar en una sola instancia a la vez hace que el algoritmo sea mucho más rápido porque tiene muy pocos datos para manipular en cada iteración. \n",
    "\n",
    "También hace posible entrenar en grandes conjuntos de entrenamiento, ya que solo una instancia debe estar en la memoria en cada iteración (el GD estocástico se puede implementar como un algoritmo fuera del núcleo; véase el capítulo 1).\n",
    "\n",
    "Por otro lado, debido a su naturaleza estocástica (es decir, aleatoria), este algoritmo es **mucho menos regular** que el descenso del gradiente de lotes: en lugar de disminuir suavemente hasta que alcance el mínimo, la función de costo rebotará hacia arriba y hacia abajo, disminuyendo solo en promedio. \n",
    "\n",
    "Con el tiempo terminará muy cerca del mínimo, pero una vez que llegue allí, **continuará rebotando, sin asentarse** (ver figura siguiente). \n",
    "\n",
    "Una vez que el algoritmo se detenga, los valores finales de los parámetros serán buenos, pero **no óptimos**.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0409.png)\n",
    "\n",
    "(_Con el descenso de gradiente estocástico, cada paso de entrenamiento es mucho más rápido, pero también mucho más estocástico que cuando se usa el descenso de gradiente por lotes_)\n",
    "\n",
    "Cuando la función de costo es muy irregular, esto en realidad puede ayudar al algoritmo a saltar de los mínimos locales, por lo que el descenso del gradiente estocástico tiene una mejor oportunidad de encontrar el mínimo global que el descenso del gradiente por lotes.\n",
    "\n",
    "Por lo tanto, la aleatoriedad es buena para escapar del \n",
    "óptimo local, pero mala porque significa que el algoritmo nunca puede establecerse al mínimo. \n",
    "\n",
    "**Una solución a este dilema es reducir gradualmente la tasa de aprendizaje. Los pasos comienzan a lo grande (lo que ayuda a progresar rápidamente y a escapar de los mínimos locales), luego se hacen cada vez más pequeños, lo que permite que el algoritmo se establezca en el mínimo global.** \n",
    "Este proceso es similar al recocido simulado, un algoritmo inspirado en el proceso en la metalurgia del recocido, donde el metal fundido se enfría lentamente. \n",
    "\n",
    "La función que determina la tasa de aprendizaje en cada iteración se llama **programa de aprendizaje.** \n",
    "\n",
    "Si la tasa de aprendizaje se reduce demasiado rápido, puede quedar atrapado en un mínimo local, o incluso terminar congelado a mitad del mínimo. \n",
    "\n",
    "Si la tasa de aprendizaje se reduce demasiado lentamente, puede saltar alrededor del mínimo durante mucho tiempo y terminar con una solución subóptima si detiene el entrenamiento demasiado pronto.\n",
    "\n",
    "Este código implementa el descenso del gradiente estocástico utilizando un programa de aprendizaje simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550c4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index : random_index + 1]\n",
    "        yi = y[random_index : random_index + 1]\n",
    "        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n",
    "        eta = learning_schedule(epoch * m + iteration)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e32385",
   "metadata": {},
   "source": [
    "Por convención, iteramos por rondas de m iteraciones; cada ronda se llama una época, como antes. \n",
    "\n",
    "Mientras que el código de descenso de gradiente por lotes iteró 1000 veces a través de todo el conjunto de entrenamiento, este código pasa por el conjunto de entrenamiento solo 50 veces y alcanza una solución bastante buena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baaa2c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21076011],\n",
       "       [2.74856079]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafd3c0",
   "metadata": {},
   "source": [
    "Tenga en cuenta que, dado que las instancias se eligen al azar, algunas instancias pueden elegirse varias veces por época, mientras que otras pueden no elegirse en absoluto. \n",
    "\n",
    "Si quieres estar seguro de que el algoritmo pasa por cada instancia en cada época, otro enfoque es barajar el conjunto de entrenamiento (asegurándose de barajar las características de entrada y las etiquetas de forma conjunta), luego pasar por él instancia por instancia, luego barajarlo de nuevo, y así sucesivamente. Sin embargo, este enfoque es más complejo y, en general, no mejora el resultado.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0410.png)\n",
    "\n",
    "Figura 4-10 (_Los primeros 20 pasos del descenso del gradiente estocástico_)\n",
    "\n",
    "\n",
    "\n",
    "#### ----------------------------- ADVERTENCIA --------------------------------\n",
    "\n",
    "Cuando se utiliza el descenso del gradiente estocástico, las instancias de entrenamiento deben ser independientes y distribuidas de manera idéntica (IID) para garantizar que los parámetros se tiren hacia el óptimo global, en promedio. \n",
    "Una forma sencilla de garantizar esto es barajar las instancias durante el entrenamiento (por ejemplo, elegir cada instancia al azar o barajar el conjunto de entrenamiento al comienzo de cada época). \n",
    "Si no baraja las instancias, por ejemplo, si las instancias están ordenadas por etiqueta, entonces SGD comenzará optimizando para una etiqueta, luego la siguiente, y así sucesivamente, y no se ajustará cerca del mínimo global.\n",
    "#### --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Para realizar una regresión lineal utilizando GD estocástico con Scikit-Learn, puede utilizar la clase `SGDRegressor`, que por defecto es la optimización de la función de costo de MSE. \n",
    "\n",
    "El siguiente código se ejecuta durante un máximo de 1000 épocas (`max_iter`) o hasta que la pérdida cae en menos de 10-5 (`tol`) durante 100 épocas (`n_iter_no_change`). \n",
    "\n",
    "Comienza con una tasa de aprendizaje de 0,01 (`eta0`), utilizando el horario de aprendizaje predeterminado (diferente del que usamos). Por último, no utiliza ninguna regularización (`penalty=None`; más detalles sobre esto en breve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7461ce2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n",
    "                       n_iter_no_change=100, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() porque fit() espera etiquetas de 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93495f",
   "metadata": {},
   "source": [
    "Una vez más, encuentras una solución bastante cercana a la devuelta por la Normalequation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102b95c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21278812]), array([2.77270267]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1087fc",
   "metadata": {},
   "source": [
    "#### ------------------------------------ TIP ------------------------------------\n",
    "\n",
    "Todos los estimadores de Scikit-Learn se pueden entrenar usando el método `fit()`, pero algunos estimadores también tienen un método `part_fit()` al que se puede llamar para ejecutar una única ronda de entrenamiento en una o más instancias (ignora hiperparámetros como `max_iter` o `tol`). . \n",
    "\n",
    "Llamar repetidamente a `part_fit()` entrenará gradualmente el modelo. Esto resulta útil cuando necesita más control sobre el proceso de formación. \n",
    "\n",
    "Otros modelos tienen en su lugar un hiperparámetro `warm_start` (y algunos tienen ambos): si estableces `warm_start=True`, llamar al método `fit()` en un modelo entrenado no restablecerá el modelo; simplemente continuará entrenando donde lo dejó, respetando hiperparámetros como `max_iter` y `tol`. \n",
    "\n",
    "Tenga en cuenta que `fit()` restablece el contador de iteraciones utilizado por el programa de aprendizaje, mientras que `parcial_fit()` no lo hace.\n",
    "#### ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957940b",
   "metadata": {},
   "source": [
    "## Descenso de gradiente por Mini-Lotes (minibatches)\n",
    "</br></br>\n",
    "El último algoritmo de descenso de gradiente que veremos se llama descenso de gradiente de mini lotes. Es sencillo una vez que conoces el descenso del gradiente estocástico y por lotes: en cada paso, en lugar de calcular los gradientes basados en el conjunto de entrenamiento completo (como en el GD por lotes) o basados en una sola instancia (como en el GD estocástico), el GD de minibatería calcula los gradientes en **pequeños conjuntos aleatorios de instancias**.\n",
    "\n",
    "El progreso del algoritmo en el espacio de parámetros es menos errático que con el GD estocástico, especialmente con minilotes bastante grandes. Como resultado, el GD en minilote terminará caminando un poco más cerca del mínimo que el GD estocástico, pero puede ser más difícil para él escapar de los mínimos locales (en el caso de problemas que sufren de mínimos locales, a diferencia de la regresión lineal con la función de costo de MSE). La figura 4-11 muestra los caminos tomados por los tres algoritmos de descenso de gradiente en el espacio de parámetros durante el entrenamiento. Todos terminan cerca del mínimo, pero el camino del lote GD en realidad se detiene en el mínimo, mientras que tanto el GD estocástico como el GD de minilote continúan caminando. Sin embargo, no olvides que el GD por lotes toma mucho tiempo para dar cada paso, y el GD estocástico y el GD de mini-lote también alcanzarían el mínimo si usaras un buen horario de aprendizaje.\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0411.png)\n",
    "\n",
    "<center>(Figura 4-11. Rutas de descenso de gradiente en el espacio de parámetros)</center>\n",
    "\n",
    "La tabla 4-1 compara los algoritmos que hemos discutido hasta ahora para la regresión lineal⁠ (recuerde que m es el número de instancias de entrenamiento y n es el número de características).\n",
    "\n",
    "_Tabla 4-1. Comparación de algoritmos para regresión lineal_\n",
    "\n",
    "| **Algoritmo**       | **m grande** | **Soporte fuera del núcleo** | **n grande** | **Hiperparámetros** | **Requiere escalado** | **Scikit-Learn**    |\n",
    "|-----------------|----------|--------------------------|----------|-----------------|-------------------|-----------------|\n",
    "| Ecuación normal | Rapido   | No                       | Lento    | 0               | No                | N/A             |\n",
    "| SVD             | Rapido   | No                       | Lento    | 0               | No                | LinearRegresion |\n",
    "| Batch GD        | Lento    | No                       | Rapido   | 2               | Yes               | N/A             |\n",
    "| SGD             | Rapido   | Si                       | Rapido   | >=2             | Yes               | SGDRegressor    |\n",
    "| Mini-batch GD   | Rapido   | Si                       | Rapido   | >=2             | Yes               | N/A             |\n",
    "\n",
    "</br></br>\n",
    "Casi no hay diferencia después del entrenamiento: todos estos algoritmos terminan con modelos muy similares y hacen predicciones exactamente de la misma manera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfc0c9",
   "metadata": {},
   "source": [
    "## Regresión Polinómica\n",
    "\n",
    "**¿Qué pasa si tus datos son más complejos que una línea recta?** Sorprendentemente, puedes usar un modelo lineal para ajustar los datos no lineales. Una forma sencilla de hacer esto es agregar poderes de cada característica como nuevas características, y luego entrenar un modelo lineal en este conjunto extendido de características. Esta técnica se llama **regresión polinómica**.\n",
    "\n",
    "Echemos un vistazo a un ejemplo. \n",
    "En primer lugar, generaremos algunos datos no lineales (ver Figura 4-12), basados en una ecuación cuadrática simple, que es una ecuación de la forma **y = ax² +bx + c**, más algo de ruido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550a4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5a77a",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0412.png)\n",
    "\n",
    "_Figura 4-12. Conjunto de datos no lineales y ruidoso generado_\n",
    "\n",
    "Es evidente que una línea recta nunca se ajustará correctamente a estos datos. Entonces, usemos la clase `PolynomialFeatures` de Scikit-Learn para transformar nuestros datos de entrenamiento, agregando el cuadrado (polinomio de segundo grado) de cada característica en el conjunto de entrenamiento como una nueva característica (en este caso solo hay una característica):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce471cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75275929])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef09e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75275929,  0.56664654])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b67d7c",
   "metadata": {},
   "source": [
    "`X_poly` ahora contiene la característica original de `X` más el cuadrado de esta característica. \n",
    "\n",
    "Ahora podemos ajustar un modelo de `LinearRegression` a estos datos de entrenamiento extendidos (Figura 4-13):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11ca058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.78134581]), array([[0.93366893, 0.56456263]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a295c8b",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0413.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6769c90",
   "metadata": {},
   "source": [
    "No nada está mal: las estimaciones del modelo **y^ = 0.56(x1)^2 + 0.93(x1) + 1.78** cuando, de hecho, la función original era y = 0.5(x1)^2 + 1.0(x1) + 2.0 + RuidoGausiano.\n",
    "\n",
    "Tenga en cuenta que cuando hay varias características, la regresión polinómica es capaz de encontrar relaciones entre características, algo que un modelo de regresión lineal simple no puede hacer. Esto es posible gracias al hecho de que PolynomialFeatures también agrega todas las combinaciones de características hasta el grado dado. Por ejemplo, si hubiera dos características a y b, `PolynomialFeatures` con `degree=3` no solo agregaría las características _a2, a3, b2 y b3_, sino también las combinaciones _ab, a2b y ab2_.\n",
    "\n",
    "#### ------------------------------ ADVERTENCIA ---------------------------------\n",
    "`PolynomialFeatures(degree=d)`¡Transforma una matriz que contiene _n_ características en una matriz que contiene _(n + d)! / d! n!_ características, donde _n!_ es el factorial de _n_, igual a _1 × 2 × 3 × ⋯ ×n_. \n",
    "\n",
    "¡Cuidado con la explosión combinatoria del número de características!\n",
    "#### ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9174b47",
   "metadata": {},
   "source": [
    "## Curvas de aprendizaje\n",
    "\n",
    "Si realiza una regresión polinómica de alto grado, es probable que ajuste los datos de entrenamiento mucho mejor que con la regresión lineal simple. Por ejemplo, la Figura 4-14 aplica un modelo polinómico de 300 grados a los datos de entrenamiento anteriores, y compara el resultado con un modelo lineal puro y un modelo cuadrático (polinomio de segundo grado). Observa cómo el modelo polinómico de 300 grados se mueve para acercarse lo más posible a las instancias de entrenamiento.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0414.png)\n",
    "\n",
    "_Figura 4-14. Regresión polinómica de alto grado_\n",
    "\n",
    "Este modelo de regresión polinómica de alto grado está sobreajustando severamente los datos de entrenamiento, mientras que el modelo lineal lo está encajando. El modelo que mejor se generalizará en este caso es el modelo cuadrático, que tiene sentido porque los datos se generaron utilizando un modelo cuadrático. Pero en general no sabrás qué función generó los datos, así que ¿cómo puedes decidir qué tan complejo debe ser tu modelo? ¿Cómo puedes saber que tu modelo está sobreajustando o no ajustando los datos?\n",
    "\n",
    "En el capítulo 2, utilizó la validación cruzada para obtener una estimación del rendimiento de la generalización de un modelo. Si un modelo funciona bien en los datos de entrenamiento, pero se generaliza mal de acuerdo con las métricas de validación cruzada, entonces su modelo se está ajustando en exceso. Si funciona mal en ambos, entonces no se ajusta bien. Esta es una forma de saber cuándo un modelo es demasiado simple o demasiado complejo.\n",
    "\n",
    "Otra forma de saberlo es observar las curvas de aprendizaje, que son gráficos del error de entrenamiento y del error de validación del modelo en función de la iteración de entrenamiento: simplemente evalúe el modelo a intervalos regulares durante el entrenamiento tanto en el conjunto de entrenamiento como en el conjunto de validación. y trazar los resultados. Si el modelo no se puede entrenar de forma incremental (es decir, si no admite `parcial_fit()` o `warm_start`), entonces debes entrenarlo varias veces en subconjuntos gradualmente más grandes del conjunto de entrenamiento.\n",
    "\n",
    "Scikit-Learn tiene una útil función `learning_curve()` para ayudar con esto: entrena y evalúa el modelo mediante validación cruzada. De forma predeterminada, vuelve a entrenar el modelo en subconjuntos crecientes del conjunto de entrenamiento, pero si el modelo admite el aprendizaje incremental, puede configurar  `exploit_incremental_learning=True` al llamar a `learning_curve()` y, en su lugar, entrenará el modelo de forma incremental. La función devuelve los tamaños del conjunto de entrenamiento en los que evaluó el modelo y las puntuaciones de entrenamiento y validación que midió para cada tamaño y para cada pliegue de validación cruzada. Usemos esta función para observar las curvas de aprendizaje del modelo de regresión lineal simple (ver Figura 4-15):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b13e8ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4e14f",
   "metadata": {},
   "source": [
    "Este modelo no es apto. Para ver por qué, primero echemos un vistazo al error de entrenamiento. Cuando solo hay una o dos instancias en el conjunto de entrenamiento, el modelo puede encajar perfectamente, por lo que la curva comienza en cero. Pero a medida que se agregan nuevas instancias al conjunto de entrenamiento, se hace imposible que el modelo se ajuste perfectamente a los datos de entrenamiento, tanto porque los datos son ruidosos como porque no son lineales en absoluto. Por lo tanto, el error en los datos de entrenamiento aumenta hasta que alcanza una meseta, momento en el que agregar nuevas instancias al conjunto de entrenamiento no hace que el error promedio sea mucho mejor o peor. Ahora veamos el error de validación. Cuando el modelo se entrena en muy pocas instancias de entrenamiento, es incapaz de generalizar correctamente, por lo que el error de validación es inicialmente bastante grande. Luego, a medida que se muestra al modelo más ejemplos de entrenamiento, aprende y, por lo tanto, el error de validación disminuye lentamente. Sin embargo, una vez más, una línea recta no puede hacer un buen trabajo al modelar los datos, por lo que el error termina en una meseta, muy cerca de la otra curva.\n",
    "\n",
    "Estas curvas de aprendizaje son típicas de un modelo que no se ajusta bien. Ambas curvas han alcanzado una meseta; son cercanas y bastante altas.\n",
    "\n",
    "\n",
    "#### -------------------------------------- TIP ---------------------------------------\n",
    "Si su modelo no se ajusta bien a los datos de entrenamiento, añadir más ejemplos de entrenamiento no ayudará. Necesitas usar un modelo mejor o idear mejores características.\n",
    "#### ----------------------------------------------------------------------------------\n",
    "\n",
    "Ahora veamos las curvas de aprendizaje de un modelo polinómico de décimo grado en los mismos datos (Figura 4-16):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb4c4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=10, include_bias=False),\n",
    "    LinearRegression())\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97438ce7",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0416.png)\n",
    "\n",
    "_Figura 4-16. Curvas de aprendizaje para el modelo polinomial de décimo grado_\n",
    "\n",
    "Estas curvas de aprendizaje se parecen un poco a las anteriores, pero hay dos diferencias muy importantes:\n",
    "\n",
    "- El error en los datos de entrenamiento es mucho menor que antes.\n",
    "\n",
    "- Hay un espacio entre las curvas. Esto significa que el modelo se desempeña significativamente mejor en los datos de entrenamiento que en los datos de validación, que es el sello distintivo de un modelo de sobreajuste. Sin embargo, si usaras un conjunto de entrenamiento mucho más grande, las dos curvas seguirían acercándose.\n",
    "\n",
    "#### ------------------------------------ TIP ----------------------------------------\n",
    "Una forma de mejorar un modelo de sobreajuste es alimentarlo con más datos de entrenamiento hasta que el error de validación llegue al error de entrenamiento.\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "#### ----------------- LA COMPENSACIÓN DE SESGO/VARIANZA--------------\n",
    "Un resultado teórico importante de la estadística y el aprendizaje automático es el hecho de que el error de generalización de un modelo se puede expresar como la suma de tres errores muy diferentes:\n",
    "\n",
    "* **sesgo**: Esta parte del error de generalización se debe a suposiciones erróneas, como asumir que los datos son lineales cuando en realidad son cuadráticos. Lo más probable es que un modelo de alto sesgo no se ajuste a los datos de entrenamiento\n",
    "\n",
    "* **varianza**: Esta parte se debe a la excesiva sensibilidad del modelo a pequeñas variaciones en los datos de entrenamiento. Es probable que un modelo con muchos grados de libertad (como un modelo polinómico de alto grado) tenga una alta varianza y, por lo tanto, se ajuste demasiado a los datos de entrenamiento.\n",
    "\n",
    "* **error irreducible**: Esta parte se debe a la ruidosidad de los datos en sí. La única manera de reducir esta parte del error es limpiar los datos (por ejemplo, arreglar las fuentes de datos, como los sensores rotos, o detectar y eliminar valores atípicos).\n",
    "\n",
    "Aumentar la complejidad de un modelo normalmente aumentará su variación y reducirá su sesgo. Por el contrario, reducir la complejidad de un modelo aumenta su sesgo y reduce su variación. Esta es la razón por la que se llama una compensación.\n",
    "#### -----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e889b9c",
   "metadata": {},
   "source": [
    "# Modelos lineales regularizados\n",
    "\n",
    "Como vio en los capítulos 1 y 2, una buena manera de reducir el sobreajuste es regularizar el modelo (es decir, restringirlo): cuantos menos grados de libertad tenga, más difícil será para él sobreadaptar los datos. Una forma sencilla de regularizar un modelo polinómico es reducir el número de grados polinómicos.\n",
    "\n",
    "Para un modelo lineal, la regularización se logra normalmente restringiendo los pesos del modelo. Ahora veremos la regresión de la cresta, la regresión del lazo y la regresión de la red elástica, que implementan tres formas diferentes de restringir los pesos.\n",
    "\n",
    "\n",
    "## Regresión de cresta\n",
    "\n",
    "La regresión de cresta (también llamada _regularización de Tikhonov_) es una versión regularizada de la regresión lineal: un término de regularización igual a\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/JyV5zD1/Captura-de-pantalla-2023-08-24-a-las-5-14-15.png\" alt=\"Captura-de-pantalla-2023-08-24-a-las-5-14-15\" border=\"0\"></a>\n",
    "\n",
    "se añade al MSE. Esto obliga al algoritmo de aprendizaje no solo a ajustarse a los datos, sino también a mantener los pesos del modelo lo más pequeños posible. Tenga en cuenta que el plazo de regularización solo debe añadirse a la función de coste durante la formación. Una vez que se entrene el modelo, desea utilizar el MSE no regularizado (o el RMSE) para evaluar el rendimiento del modelo.\n",
    "\n",
    "El hiperparámetro α controla cuánto quieres regularizar el modelo. Si α = 0, entonces la regresión de cresta es solo regresión lineal. Si α es muy grande, entonces todos los pesos terminan muy cerca de cero y el resultado es una línea plana que pasa por la media de los datos. \n",
    "La ecuación 4-8 presenta la función de costo de regresión de cresta.\n",
    "\n",
    "### Ecuación 4-8. Función de costo de regresión de Ridge\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/TqbLD9J/Captura-de-pantalla-2023-08-24-a-las-15-33-38.png\" alt=\"Captura-de-pantalla-2023-08-24-a-las-15-33-38\" border=\"0\"></a>\n",
    "\n",
    "Tenga en cuenta que el término de sesgo θ0 no está regularizado (la suma comienza en i = 1, no en 0). \n",
    "Si definimos w como el vector de pesos de característica (θ1 a θn), entonces el término de regularización es igual a α(∥ w ∥2)2 / m, donde ∥ w ∥2 representa la norma ℓ2 del vector de peso.⁠ Para el gradiente de descenso.\n",
    "\n",
    "\n",
    "#### -------------------------------- ADVERTENCIA ------------------------------------\n",
    "Es importante escalar los datos (por ejemplo, utilizando un `StandardScaler`) antes de realizar la regresión de cresta, ya que es sensible a la escala de las características de entrada. \n",
    "Esto es cierto para la mayoría de los modelos regularizados.\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "La Figura 4-17 muestra varios modelos de cresta que se entrenaron con datos lineales muy ruidosos utilizando diferentes valores de α. \n",
    "\n",
    "A la izquierda, se utilizan modelos de crestas planas, que conducen a predicciones lineales. \n",
    "\n",
    "A la derecha, los datos primero se expanden usando `PolynomialFeatures(degree = 10)`, luego se escalan usando un `StandardScaler` y, finalmente, los modelos de cresta se aplican a las características resultantes: esto es regresión polinómica con regularización de cresta. \n",
    "\n",
    "Observa cómo el aumento de α conduce a predicciones más planas (es decir, menos extremas, más razonables), lo que reduce la varianza del modelo pero aumenta su sesgo.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0417.png)\n",
    "\n",
    "_Figura 4-17. Modelos lineales (izquierda) y polinómicos (derecha), ambos con varios niveles de regularización de crestas_\n",
    "\n",
    "Al igual que con la regresión lineal, podemos realizar la regresión de cresta ya sea calculando una ecuación de forma cerrada o realizando un descenso de gradiente. Los pros y los contras son los mismos. La ecuación 4-9 muestra la solución de forma cerrada, donde **A** es la _matriz de identidad_ (n + 1) × (n + 1), excepto con un 0 en la celda superior izquierda, correspondiente al término de sesgo.\n",
    "\n",
    "### Ecuación 4-9. Solución de forma cerrada de regresión de cresa\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/n77pXft/Captura-de-pantalla-2023-08-24-a-las-15-39-44.png\" alt=\"Captura-de-pantalla-2023-08-24-a-las-15-39-44\" border=\"0\"></a>\n",
    "\n",
    "Aquí se explica cómo realizar la regresión de cresta con Scikit-Learn utilizando una solución de forma cerrada (una variante de la ecuación 4-9 que utiliza una técnica de factorización matricial de André-Louis Cholesky):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87811c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.82899748]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610c58a",
   "metadata": {},
   "source": [
    "Y usando el descenso del gradiente estocástico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "881ff491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.82830117])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n",
    "                        max_iter=1000, eta0=0.01, random_state=42)\n",
    "\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2836b",
   "metadata": {},
   "source": [
    "El hiperparámetro de penalización (`penalty`) establece el tipo de término de regularización que se utilizará. Especificar `\"l2\"` indica que desea que SGD agregue un término de regularización a la función de costo de MSE igual a alfa (`alpha`) multiplicado por el cuadrado de la norma ℓ2 del vector de peso. \n",
    "Esto es como la regresión de cresta, excepto que en este caso no hay división por m; es por eso que pasamos `alfa=0,1 / m`, para obtener el mismo resultado que `Ridge(alpha=0,1)`.\n",
    "\n",
    "\n",
    "#### ------------------------------------ TIP ----------------------------\n",
    "La clase `RidgeCV` también realiza una regresión de crestas, pero ajusta automáticamente los hiperparámetros mediante validación cruzada. Es más o menos equivalente a usar `GridSearchCV`, pero está optimizado para la regresión de crestas y se ejecuta mucho más rápido. Varios otros estimadores (en su mayoría lineales) también tienen variantes de CV eficientes, como `LassoCV` y `ElasticNetCV`.\n",
    "#### --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## Regresión de Lasso\n",
    "\n",
    "\n",
    "La _contracción mínima absoluta_ y la regresión del operador de selección (generalmente llamada simplemente regresión de lazo) es otra versión regularizada de la regresión lineal: al igual que la regresión de cresta, agrega un término de regularización a la función de costo, pero utiliza la norma ℓ1 del vector de peso en lugar del cuadrado de la norma �� Tenga en cuenta que la norma ℓ1 se multiplica por 2α, mientras que la norma ℓ2 se multiplicó por α / m en la regresión de crestas. \n",
    "\n",
    "Estos factores se eligieron para garantizar que el valor óptimo de α sea independiente del tamaño del conjunto de entrenamiento: diferentes normas conducen a diferentes factores (consulte el número 15657 de Scikit-Learn para obtener más detalles).\n",
    "\n",
    "### Ecuación 4-10. Función de coste de regresión de lasso\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/6nvNFbc/Captura-de-pantalla-2023-09-04-a-las-5-31-51.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-5-31-51\" border=\"0\"></a>\n",
    "\n",
    "La figura 4-18 muestra lo mismo que la figura 4-17, pero reemplaza los modelos de cresta por modelos de lazo y utiliza diferentes valores α.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0418.png)\n",
    "\n",
    "(_Figura 4-18. Modelos lineales (izquierda) y polinómicos (derecha), ambos utilizando varios niveles de regularización de lazo_)\n",
    "\n",
    "Una característica importante de la regresión del lazo es que tiende a eliminar los pesos de las características menos importantes (es decir, establecerlas en cero). Por ejemplo, la línea discontinua en el gráfico de la derecha en la Figura 4-18 (con α = 0,01) se ve aproximadamente cúbica: todos los pesos para las características polinómicas de alto grado son iguales a cero. En otras palabras, la regresión de lazo realiza automáticamente la selección de características y genera un modelo disperso con pocos pesos de características distintos de cero.\n",
    "\n",
    "Puedes hacerte una idea de por qué este es el caso mirando la Figura 4-19: los ejes representan dos parámetros del modelo, y los contornos de fondo representan diferentes funciones de pérdida. En el gráfico superior izquierdo, los contornos representan la pérdida de ℓ1 (|θ1| + |θ2|), que cae linealmente a medida que te acercas a cualquier eje. \n",
    "\n",
    "Por ejemplo, si inicializa los parámetros del modelo a θ1 = 2 y θ2 = 0,5, el descenso de gradiente de ejecución disminuirá ambos parámetros por igual (como se representa por la línea amarilla discontinua); por lo tanto, θ2 alcanzará a 0 primero (ya que estaba más cerca de 0 para empezar). \n",
    "\n",
    "Después de eso, el descenso del gradiente rodará por el canalón hasta que alcance θ1 = 0 (con un poco de rebote, ya que los gradientes de ℓ1 nunca se acercan a 0: son -1 o 1 para cada parámetro). En el gráfico superior derecho, los contornos representan la función de costo de la regresión del lazo (es decir, una función de costo MSE más una pérdida de ℓ1). \n",
    "\n",
    "Los pequeños círculos blancos muestran el camino que toma el descenso del gradiente para optimizar algunos parámetros del modelo que se inicializaron alrededor de θ1 = 0,25 y θ2 = -1: observe una vez más cómo el camino alcanza rápidamente θ2 = 0, luego rueda por el canalón y termina rebotando alrededor del óptimo global (representado por el cuadrado rojo). \n",
    "\n",
    "Si aumentamos α, el óptimo global se movería a la izquierda a lo largo de la línea amarilla discontinua, mientras que si disminuimos α, el óptimo global se movería a la derecha (en este ejemplo, los parámetros óptimos para el MSE no regularizado son θ1 = 2 y θ2 = 0,5).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0419.png)\n",
    "\n",
    "(_Figura 4-19. Regularización de la lazo frente a la cresta_)\n",
    "\n",
    "Las dos parcelas inferiores muestran lo mismo, pero con una penalización de ℓ2. \n",
    "\n",
    "En el gráfico inferior izquierdo, puedes ver que la pérdida de ℓ2 disminuye a medida que nos acercamos al origen, por lo que el descenso de gradiente solo toma un camino recto hacia ese punto. En el gráfico inferior derecho, los contornos representan la función de costo de la regresión de cresta (es decir, una función de costo de MSE más una pérdida de ℓ2). \n",
    "\n",
    "Como puede ver, los gradientes se hacen más pequeños a medida que los parámetros se acercan al óptimo global, por lo que el descenso del gradiente se ralentiza naturalmente. \n",
    "\n",
    "Esto limita el rebote, lo que ayuda a que la cresta converja más rápido que la regresión de lazo. \n",
    "\n",
    "También tenga en cuenta que los parámetros óptimos (representados por el cuadrado rojo) se acercan cada vez más al origen cuando aumenta α, pero nunca se eliminan por completo.\n",
    "\n",
    "### --------------------------- TIP -------------------------------\n",
    "Para evitar que el descenso del gradiente rebote alrededor del óptimo al final cuando se utiliza la regresión de lazo, debe reducir gradualmente la tasa de aprendizaje durante el entrenamiento. Todavía rebotará alrededor del óptimo, pero los pasos se harán cada vez más pequeños, por lo que convergerá.\n",
    "### ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "La función de costo de lazo no es diferenciable en θi = 0 (para i = 1, 2, ⋯, n), pero el descenso de gradiente todavía funciona si se utiliza un vector de subgradiente g⁠11 en su lugar cuando cualquier θi = 0. La ecuación 4-11 muestra una ecuación vectorial de subgradiente que puedes usar para el descenso de gradiente con la función de costo de lazo.\n",
    "\n",
    "### Ecuación 4-11 Vector subgradiente de regresión de lass\n",
    "\n",
    "<a href=\"https://ibb.co/XtCN8yM\"><img src=\"https://i.ibb.co/NKyQS1b/Captura-de-pantalla-2023-09-04-a-las-5-51-41.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-5-51-41\" border=\"0\"></a>\n",
    "\n",
    "Aquí hay un pequeño ejemplo de Scikit-Learn usando la clase `Lasso`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83d022be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.77621741])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])\n",
    "\n",
    "#También podríamos usar:\n",
    "#SGDRegressor(penalty=\"l1\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25dbde1",
   "metadata": {},
   "source": [
    "## Regresión neta elástica\n",
    "\n",
    "La regresión de la red elástica es un punto intermedio entre la regresión de cresta y la regresión de lazo. El término de regularización es una suma ponderada de los términos de regularización de ridge y lasso, y puedes controlar la relación de mezcla r. \n",
    "Cuando r = 0, la red elástica es equivalente a la regresión de la cresta, y cuando r = 1, es equivalente a la regresión de lazo (Ecuación 4-12).\n",
    "\n",
    "### Ecuación 4-12. Función de coste neto elástico\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/K795Z9M/Captura-de-pantalla-2023-09-04-a-las-6-54-37.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-6-54-37\" border=\"0\"></a>\n",
    "\n",
    "Entonces, ¿cuándo debería usar la regresión neta elástica, o la regresión de cresta, lazo o la regresión lineal simple (es decir, sin ninguna regularización)? \n",
    "\n",
    "Casi siempre es preferible tener al menos un poco de regularización, por lo que generalmente debe evitar la regresión lineal simple. \n",
    "\n",
    "Ridge es un buen valor predeterminado, pero si sospechas que solo unas pocas características son útiles, deberías preferir el lazo o la red elástica porque tienden a reducir el peso de las características inútiles a cero, como se discutió anteriormente. \n",
    "\n",
    "En general, se prefiere la red elástica sobre el lazo porque el lazo puede comportarse de forma errática cuando el número de características es mayor que el número de instancias de entrenamiento o cuando varias características están fuertemente correlacionadas.\n",
    "\n",
    "Aquí un ejemplo que usa scikit-learn `ElasticNet(l1_ratio corresponde a mezclar el ratio r)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d493a6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.78114505])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45849741",
   "metadata": {},
   "source": [
    "## Parada temprana\n",
    "\n",
    "Una forma muy diferente de regularizar los algoritmos de aprendizaje iterativo, como el descenso de gradiente, es detener el entrenamiento tan pronto como el error de validación alcance un mínimo. Esto se llama parada temprana. \n",
    "La figura 4-20 muestra un modelo complejo (en este caso, un modelo de regresión polinómica de alto grado) que se está entrenando con el descenso del gradiente por lotes en el conjunto de datos cuadrático que usamos anteriormente. \n",
    "A medida que pasan las épocas, el algoritmo aprende, y su error de predicción (RMSE) en el conjunto de entrenamiento cae, junto con su error de predicción en el conjunto de validación. Sin embargo, después de un tiempo, el error de validación deja de disminuir y comienza a volver a subir. \n",
    "Esto indica que el modelo ha comenzado a sobreadaptar los datos de entrenamiento. Con la parada temprana, simplemente dejas de entrenar tan pronto como el error de validación alcance el mínimo. \n",
    "\n",
    "Es una técnica de regularización tan simple y eficiente que Geoffrey Hinton la llamó un \"hermoso almuerzo gratis\".\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0420.png)\n",
    "\n",
    "### ----------------------------- TIP -------------------------------------\n",
    "Con el descenso de gradiente estocástico y mini-lote, las curvas no son tan suaves, y puede ser difícil saber si has alcanzado el mínimo o no. Una solución es detenerse solo después de que el error de validación haya estado por encima del mínimo durante algún tiempo (cuando esté seguro de que el modelo no lo hará mejor), y luego revertir los parámetros del modelo hasta el punto en que el error de validación estuvo al mínimo.\n",
    "### -----------------------------------------------------------------------\n",
    "\n",
    "Aquí hay una implementación básica de la parada temprana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89d12eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3k/rj832x716_563czpglqh2mmw0000gn/T/ipykernel_2986/2086719212.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# split the quadratic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = [...]  # split the quadratic dataset\n",
    "\n",
    "preprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n",
    "                              StandardScaler())\n",
    "X_train_prep = preprocessing.fit_transform(X_train)\n",
    "X_valid_prep = preprocessing.transform(X_valid)\n",
    "sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\n",
    "n_epochs = 500\n",
    "best_valid_rmse = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.partial_fit(X_train_prep, y_train)\n",
    "    y_valid_predict = sgd_reg.predict(X_valid_prep)\n",
    "    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n",
    "    if val_error < best_valid_rmse:\n",
    "        best_valid_rmse = val_error\n",
    "        best_model = deepcopy(sgd_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7eb211",
   "metadata": {},
   "source": [
    "Este código primero agrega las características polinómicas y escala todas las características de entrada, tanto para el conjunto de entrenamiento como para el conjunto de validación (el código supone que ha dividido el conjunto de entrenamiento original en un conjunto de entrenamiento más pequeño y un conjunto de validación). Luego crea un modelo `SGDRegressor` sin regularización y con una pequeña tasa de aprendizaje. En el ciclo de entrenamiento, llama a `partial_fit()` en lugar de `fit()`, para realizar un aprendizaje incremental. En cada época, mide el RMSE en el conjunto de validación. Si es inferior al RMSE más bajo visto hasta ahora, guarda una copia del modelo en la variable `best_model`. En realidad, esta implementación no detiene el entrenamiento, pero le permite volver al mejor modelo después del entrenamiento. Tenga en cuenta que el modelo se copia utilizando `copy.deepcopy()`, porque copia tanto los hiperparámetros del modelo como los parámetros aprendidos. Por el contrario, `sklearn.base.clone()` solo copia los hiperparámetros del modelo.\n",
    "\n",
    "\n",
    "# Regresión logística\n",
    "\n",
    "Como se discutió en el capítulo 1, algunos algoritmos de regresión se pueden utilizar para la clasificación (y viceversa). La regresión logística (también llamada regresión de logit) se utiliza comúnmente para estimar la probabilidad de que una instancia pertenezca a una clase en particular (por ejemplo, ¿cuál es la probabilidad de que este correo electrónico sea spam?). Si la probabilidad estimada es mayor que un umbral dado (típicamente el 50%), entonces el modelo predice que la instancia pertenece a esa clase (llamada clase positiva, etiquetada como \"1\", y de lo contrario predice que no lo hace (es decir, pertenece a la clase negativa, etiquetada como \"0\"\"). Esto lo convierte en un clasificador binario.\n",
    "\n",
    "\n",
    "## Estimación de probabilidades\n",
    "\n",
    "Entonces, ¿cómo funciona la regresión logística? Al igual que un modelo de regresión lineal, un modelo de regresión logística calcula una suma ponderada de las características de entrada (más un término de sesgo), pero en lugar de generar el resultado directamente como lo hace el modelo de regresión lineal, genera la logística de este resultado (ver Ecuación 4-13).\n",
    "\n",
    "### Ecuación 4-13. Probabilidad estimada del modelo de regresión logística (forma vectorizada)\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/fHdg3rf/Captura-de-pantalla-2023-09-04-a-las-20-04-21.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-20-04-21\" border=\"0\"></a>\n",
    "\n",
    "La logística, con la que se señala σ(·), es una función sigmoide (es decir, en forma de S) que produce un número entre 0 y 1. Se define como se muestra en la ecuación 4-14 y la Figura 4-21.\n",
    "\n",
    "### Ecuación 4-14. Función logística\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/02MbQpc/Captura-de-pantalla-2023-09-04-a-las-20-05-12.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-20-05-12\" border=\"0\"></a>\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0421.png)\n",
    "\n",
    "Una vez que el modelo de regresión logística ha estimado la probabilidad **p^ = hθ(x)** que una instancia x pertenece a la clase positiva, puede hacer su predicción ŷ fácilmente (ver Ecuación 4-15).\n",
    "\n",
    "### Ecuación 4-15. Predicción del modelo de regresión logística utilizando un umbral de probabilidad del 50 %\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/MggtrMD/Captura-de-pantalla-2023-09-04-a-las-20-06-51.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-20-06-51\" border=\"0\"></a>\n",
    "\n",
    "Tenga en cuenta que σ(t) < 0,5 cuando t < 0, y σ(t) ≥ 0,5 cuando t ≥ 0, por lo que un modelo de regresión logística que utiliza el umbral predeterminado del 50% de probabilidad predice 1 si θ⊺ x es positivo y 0 si es negativo.\n",
    "\n",
    "#### ----------------------------- NOTA --------------------------------\n",
    "La puntuación t a menudo se llama logit. El nombre proviene del hecho de que la función logit, definida como logit(p) = log(p / (1 - p)), es la inversa de la función logística. \n",
    "\n",
    "De hecho, si calculas el logit de la probabilidad estimada p, encontrarás que el resultado es t. \n",
    "\n",
    "El logit también se llama log-odds, ya que es el log de la relación entre la probabilidad estimada para la clase positiva y la probabilidad estimada para la clase negativa.\n",
    "#### ---------------------------------------------------------------------\n",
    "\n",
    "## Función de formación y coste\n",
    "\n",
    "Ahora ya sabes cómo un modelo de regresión logística estima las probabilidades y hace predicciones. Pero, ¿cómo se entrena? El objetivo del entrenamiento es establecer el parámetro vectorθ para que el modelo estime altas probabilidades para instancias positivas (y = 1) y bajas probabilidades para instancias negativas (y = 0). Esta idea es capturada por la función de costo que se muestra en la ecuación 4-16 para una sola instancia de entrenamiento x.\n",
    "\n",
    "### Ecuación 4-16. Función de coste de una sola instancia de formación\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/dkdySsF/Captura-de-pantalla-2023-09-04-a-las-20-09-16.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-20-09-16\" border=\"0\"></a>\n",
    "\n",
    "Esta función de costo tiene sentido porque -log(t) crece muy grande cuando t se acerca a 0, por lo que el costo será grande si el modelo estima una probabilidad cercana a 0 para una instancia positiva, y también será grande si el modelo estima una probabilidad cercana a 1 para una instancia negativa. Por otro lado, -log(t) está cerca de 0 cuando t está cerca de 1, por lo que el costo estará cerca de 0 si la probabilidad estimada está cerca de 0 para una instancia negativa o cerca de 1 para una instancia positiva, que es precisamente lo que queremos.\n",
    "\n",
    "La función de costo en todo el conjunto de entrenamiento es el costo promedio en todas las instancias de entrenamiento. Se puede escribir en una sola expresión llamada pérdida de registro, que se muestra en la ecuación 4-17.\n",
    "\n",
    "### Ecuación 4-17. Función de costo de regresión logística (p pérdida de registro)\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/2PYRqwW/Captura-de-pantalla-2023-09-04-a-las-20-09-53.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-20-09-53\" border=\"0\"></a>\n",
    "\n",
    "#### ---------------------------------- ADVERTENCIA ------------------------------\n",
    "La pérdida de troncos no solo se sacó de un sombrero. Se puede demostrar matemáticamente (usando la inferencia bayesiana) que la minimización de esta pérdida dará como resultado el modelo con la máxima probabilidad de ser óptimo, suponiendo que las instancias sigan una distribución gaussiana alrededor de la media de su clase. Cuando usas la pérdida de registro, esta es la suposición implícita que estás haciendo. Cuanto más errónea sea esta suposición, más sesgada será el modelo. Del mismo modo, cuando usamos el MSE para entrenar modelos de regresión lineal, asumimos implícitamente que los datos eran puramente lineales, además de algo de ruido gaussiano. Por lo tanto, si los datos no son lineales (por ejemplo, si son cuadráticos) o si el ruido no es gaussiano (por ejemplo, si los valores atípicos no son exponencialmente raros), entonces el modelo estará sesgado.\n",
    "#### -----------------------------------------------------------------------------------\n",
    "\n",
    "La mala noticia es que no hay una ecuación de forma cerrada conocida para calcular el valor de θ que minimice esta función de costo (no hay equivalente de la ecuación Normal). Pero la buena noticia es que esta función de costo es convexa, por lo que el descenso de gradiente (o cualquier otro algoritmo de optimización) está garantizado para encontrar el mínimo global (si la tasa de aprendizaje no es demasiado grande y esperas lo suficiente). Las derivadas parciales de la función de coste con respecto al parámetro del modelo jth θj están dadas por la ecuación 4-18.\n",
    "\n",
    "### Ecuación 4-18. Derivados parciales de la función de costo logístico\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/5vq4VH3/Captura-de-pantalla-2023-09-04-a-las-20-11-16.png\" alt=\"Captura-de-pantalla-2023-09-04-a-las-20-11-16\" border=\"0\"></a>\n",
    "\n",
    "Esta ecuación se parece mucho a la ecuación 4-5: para cada instancia calcula el error de predicción y lo multiplica por el valor de la característica jth, y luego calcula el promedio en todas las instancias de entrenamiento. Una vez que tenga el vector de gradiente que contiene todas las derivadas parciales, puede usarlo en el algoritmo de descenso de gradiente por lotes. Eso es todo: ahora sabes cómo entrenar un modelo de regresión logística. Para el GD estocástico, tomarías una instancia a la vez, y para el GD de mini lotes, usarías un mini lote a la vez.\n",
    "\n",
    "\n",
    "## Límites de la dicisión (Decision Boundaries)\n",
    "\n",
    "Podemos usar el conjunto de datos del iris para ilustrar la regresión logística. Este es un famoso conjunto de datos que contiene la longitud y el ancho del sépalo y los pétalos de 150 flores de iris de tres especies diferentes: Iris setosa, Iris versicolor e Iris virginica (ver Figura 4-22).\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0422.png)\n",
    "\n",
    "Intentemos construir un clasificador para detectar el tipo Iris virginica basado solo en la característica de ancho de pétalo. El primer paso es cargar los datos y echar un vistazo rápido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34122bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename',\n",
       " 'data_module']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)\n",
    "list(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c73bc075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f35760c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target.head(3)  # notar que las instancias no están barajadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e7b32e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80da42",
   "metadata": {},
   "source": [
    "A continuación, dividiremos los datos y entrenaremos un modelo de regresión logística en el conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f94b91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data[[\"petal width (cm)\"]].values\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450d556",
   "metadata": {},
   "source": [
    "Echemos un vistazo a las probabilidades estimadas del modelo para las flores con anchos de pétalos que varían de 0 cm a 3 cm (Figura 4-23):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba9df0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n",
    "         label=\"Not Iris virginica proba\")\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\n",
    "plt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n",
    "         label=\"Decision boundary\")\n",
    "[...] # beautify the figure: add grid, labels, axis, legend, arrows, and samples\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9f3da",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0423.png)\n",
    "\n",
    "(_Figura 4-23. Probabilidades estimadas y límite de decisión_)\n",
    "\n",
    "El ancho de los pétalos de las flores de Iris virginica (representadas como triángulos) varía de 1,4 cm a 2,5 cm, mientras que las otras flores de iris (representadas por cuadrados) generalmente tienen un ancho de pétalos más pequeño, que oscila entre 0,1 cm y 1,8 cm. \n",
    "\n",
    "Observe que hay un poco de superposición. \n",
    "Por encima de aproximadamente 2 cm, el clasificador tiene mucha confianza en que la flor es Iris virginica (da una alta probabilidad para esa clase), mientras que por debajo de 1 cm tiene mucha confianza en que no es una Iris virginica (alta probabilidad para la clase \"No Iris\"). virginica”). \n",
    "\n",
    "Entre estos extremos, el clasificador no está seguro. Sin embargo, si le pide que prediga la clase (usando el método `predict()` en lugar del método `predict_proba()`), devolverá la clase que sea más probable. \n",
    "Por lo tanto, existe un límite de decisión en torno a 1,6 cm donde ambas probabilidades son iguales al 50%: si el ancho del pétalo es mayor que 1,6 cm el clasificador predecirá que la flor es una Iris virginica, y en caso contrario predecirá que no lo es. (incluso si no tiene mucha confianza):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1422424c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6516516516516517"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd870f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20d185",
   "metadata": {},
   "source": [
    "La figura 4-24 muestra el mismo conjunto de datos, pero esta vez muestra dos características: ancho y largo de los pétalos. Una vez entrenado, el clasificador de regresión logística puede, basándose en estas dos características, estimar la probabilidad de que una nueva flor sea anIris virginica. \n",
    "\n",
    "La línea discontinua representa los puntos en los que el modelo estima una probabilidad del 50 %: este es el límite de decisión del modelo. \n",
    "\n",
    "Tenga en cuenta que es un límite lineal.⁠ \n",
    "Cada línea paralela representa los puntos en los que el modelo genera una probabilidad específica, desde el 15 % (abajo a la izquierda) hasta el 90 % (arriba a la derecha). Todas las flores más allá de la línea superior derecha tienen más del 90 % de probabilidades de ser Iris virginica, según el modelo.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0424.png)\n",
    "\n",
    "(_Figura 4-24. Límite de decisión lineal_)\n",
    "\n",
    "#### ----------------------------------- NOTA ----------------------------------------\n",
    "El hiperparámetro que controla la fuerza de regularización de un modelo Scikit-Learn `LogisticRegression` no es `alfa` (como en otros modelos lineales), sino su inverso: `C`. \n",
    "Cuanto mayor es el valor de `C`, menos regularizado está el modelo.\n",
    "#### -----------------------------------------------------------------------------------\n",
    "\n",
    "Al igual que los otros modelos lineales, los modelos de regresión logística se pueden regularizar usando penalizaciones ℓ1 o ℓ2. Scikit-Learn en realidad añade una penalización de ℓ2 por defecto.\n",
    "\n",
    "\n",
    "## Regresión Softmax\n",
    "\n",
    "El modelo de regresión logística se puede generalizar para admitir múltiples clases directamente, sin tener que entrenar y combinar múltiples clasificadores binarios (como se discute en el capítulo 3). Esto se llama regresión softmax, o regresión logística multinomial.\n",
    "\n",
    "La idea es simple: cuando se le da una instancia **x**, el modelo de regresión softmax primero calcula una puntuación sk(**x**) para cada clase k, luego estima la probabilidad de cada clase aplicando la función softmax (también llamada exponencial normalizada) a las puntuaciones. La ecuación para calcular sk(x) debería parecer familiar, ya que es igual que la ecuación para la predicción de regresión lineal (ver Ecuación 4-19).\n",
    "\n",
    "### Ecuación 4-19. Puntuación Softmax para la clase k\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/1rWCQdL/Captura-de-pantalla-2023-09-06-a-las-3-57-11.png\" alt=\"Captura-de-pantalla-2023-09-06-a-las-3-57-11\" border=\"0\"></a>\n",
    "\n",
    "Tenga en cuenta que cada clase tiene su propio vector de parámetros dedicado θ(k). Todos estos vectores se almacenan normalmente como filas en una matriz de parámetros Θ.\n",
    "\n",
    "Una vez que hayas calculado la puntuación de cada clase para la instancia x, puedes estimar la probabilidad **pK^** que la instancia pertenece a la clase k ejecutando las puntuaciones a través de la función softmax (Ecuación 4-20). La función calcula la exponencial de cada puntuación, luego la normaliza (dividiéndola por la suma de todas las exponenciales). Las puntuaciones generalmente se llaman logits o log-odds (aunque en realidad son log-odds no normalizados).\n",
    "\n",
    "### Ecuación 4-20. Función Softmax\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/R6213kk/Captura-de-pantalla-2023-09-06-a-las-4-02-01.png\" alt=\"Captura-de-pantalla-2023-09-06-a-las-4-02-01\" border=\"0\"></a>\n",
    "\n",
    "En esta ecuación:\n",
    "\n",
    "- _**K**_ es el número de clases.\n",
    "- _**s(x)**_ es un vector que contiene las puntuaciones de cada clase para la instancia _x_.\n",
    "- _**σ(s(x))k**_ es la probabilidad estimada de que la instancia x pertenezca a la clase k, dadas las puntuaciones de cada clase para esa instancia.\n",
    "\n",
    "Al igual que el clasificador de regresión logística, por defecto, el clasificador de regresión softmax predice la clase con la probabilidad estimada más alta (que es simplemente la clase con la puntuación más alta), como se muestra en la Ecuación 4-21.\n",
    "\n",
    "\n",
    "### Ecuación 4-21. Predicción del clasificador de regresión Softmax\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/qDNhrcG/Captura-de-pantalla-2023-09-06-a-las-4-05-36.png\" alt=\"Captura-de-pantalla-2023-09-06-a-las-4-05-36\" border=\"0\"></a>\n",
    "\n",
    "El operador argmax devuelve el valor de una variable que maximiza una función. En esta ecuación, devuelve el valor de k que maximiza la probabilidad estimada **σ(s(x))k**.\n",
    "\n",
    "#### --------------------------------------- TIP --------------------------------------\n",
    "El clasificador de regresión softmax predice solo una clase a la vez (es decir, es multiclase, no multisalida), por lo que debe usarse solo con clases mutuamente excluyentes, como diferentes especies de plantas. \n",
    "No puedes usarlo para reconocer a varias personas en una imagen.\n",
    "#### ----------------------------------------------------------------------------------\n",
    "\n",
    "Ahora que sabes cómo el modelo estima las probabilidades y hace predicciones, echemos un vistazo a la formación. El objetivo es tener un modelo que estime una alta probabilidad para la clase objetivo (y, en consecuencia, una baja probabilidad para las otras clases). \n",
    "Minimizar la función de costo que se muestra en la Ecuación 4-22, llamada entropía cruzada, debería conducir a este objetivo porque penaliza el modelo cuando estima una baja probabilidad para una clase objetivo. \n",
    "La entropía cruzada se utiliza con frecuencia para medir qué tan bien un conjunto de probabilidades de clase estimadas coincide con las clases objetivo.\n",
    "\n",
    "### Ecuación 4-22. Función de costo de entropía cruzada\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/3R3SFBN/Captura-de-pantalla-2023-09-06-a-las-4-09-27.png\" alt=\"Captura-de-pantalla-2023-09-06-a-las-4-09-27\" border=\"0\"></a>\n",
    "\n",
    "En esta ecuación, yk^(i) es la probabilidad objetivo de que la i-i-i instancia pertenezca a la clase k. En general, es igual a 1 o 0, dependiendo de si la instancia pertenece a la clase o no.\n",
    "\n",
    "Tenga en cuenta que cuando solo hay dos clases (K = 2), esta función de costo es equivalente a la función de costo de regresión logística (p pérdida de registro; véase la ecuación 4-17).\n",
    "\n",
    "\n",
    "#### ------------- ENTROPÍA CRUZADA (CROSS ENTROPY) -------------------\n",
    "La entropía cruzada se originó a partir de la teoría de la información de Claude Shannon. \n",
    "\n",
    "Supongamos que quieres transmitir información eficiente sobre el clima todos los días. \n",
    "Si hay ocho opciones (soleadas, lluviosas, etc.), podrías codificar cada opción usando 3 bits, porque 23 = 8. Sin embargo, si crees que estará soleado casi todos los días, sería mucho más eficiente codificar \"soleado\" en solo un bit (0) y las otras siete opciones en cuatro bits (empezando con un 1). \n",
    "La entropía cruzada mide el número promedio de bits que realmente envías por opción. \n",
    "Si su suposición sobre el clima es perfecta, la entropía cruzada será igual a la entropía del clima en sí (es decir, su imprevisibilidad intrínseca). \n",
    "Pero si su suposición es errónea (por ejemplo, si llueve a menudo), la entropía cruzada será mayor en una cantidad llamada divergencia Kullback-Leibler (KL).\n",
    "\n",
    "La entropía cruzada entre dos distribuciones de probabilidad p y q se define como H(p,q) = -Σx p(x) log q(x) (al menos cuando las distribuciones son discretas). Para más detalles, echa un vistazo a mi vídeo sobre el tema.\n",
    "\n",
    "La entropía cruzada entre dos distribuciones de probabilidad _p_ y _q_ se define como _**H(p,q) = -Σx p(x) log q(x)**_ (al menos cuando las distribuciones son discretas). \n",
    "\n",
    "Para más detalles, echa un vistazo a mi vídeo sobre el tema.\n",
    "\n",
    "https://www.youtube.com/watch?v=ErfnhcEV1O8\n",
    "\n",
    "#### ----------------------------------------------------------------------------------\n",
    "\n",
    "El vector de gradiente de esta función de coste con respecto a θ(k) está dado por la ecuación 4-23.\n",
    "\n",
    "### Ecuación 4-23. Vector de gradiente de entropía cruzada para la clase k\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/q1htDvR/Captura-de-pantalla-2023-09-06-a-las-4-17-04.png\" alt=\"Captura-de-pantalla-2023-09-06-a-las-4-17-04\" border=\"0\"></a>\n",
    "\n",
    "Ahora puede calcular el vector de gradiente para cada clase, luego usar el descenso de gradiente (o cualquier otro algoritmo de optimización) para encontrar la matriz de parámetrosΘ que minimiza la función de costo.\n",
    "\n",
    "Usemos la regresión softmax para clasificar las plantas de iris en las tres clases. El clasificador `LogisticRegression` de Scikit-Learn usa la regresión softmax automáticamente cuando lo entrenas en más de dos clases (suponiendo que uses `solver=\"lbfgs\"`, que es el valor predeterminado). También aplica la regularización ℓ2 de forma predeterminada, que puedes controlar usando el hiperparámetro `C`, como se mencionó anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a897f81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=30, random_state=42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "softmax_reg = LogisticRegression(C=30, random_state=42)\n",
    "softmax_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e4b2c",
   "metadata": {},
   "source": [
    "Así que la próxima vez que encuentres un iris con pétalos de 5 cm de largo y 2 cm de ancho, puedes pedirle a tu modelo que te diga qué tipo de iris es, y responderá Iris virginica (clase 2) con un 96 % de probabilidad (o Iris versicolor con un 4 % de probabilidad):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7551584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e72653ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.04, 0.96]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict_proba([[5, 2]]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4507135",
   "metadata": {},
   "source": [
    "La figura 4-25 muestra los límites de decisión resultantes, representados por los colores de fondo. Tenga en cuenta que los límites de decisión entre dos clases cualesquiera son lineales. La figura también muestra las probabilidades de la clase versicolor de Iris, representada por las líneas curvas (por ejemplo, la línea etiquetada con 0,30 representa el límite de probabilidad del 30%). Tenga en cuenta que el modelo puede predecir una clase que tiene una probabilidad estimada por debajo del 50 %. Por ejemplo, en el punto en que todos los límites de decisión se encuentran, todas las clases tienen una probabilidad estimada igual del 33 %.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0425.png)\n",
    "\n",
    "(_Figura 4-25. Límites de decisión de regresión Softmax_)\n",
    "\n",
    "\n",
    "En este capítulo, aprendiste varias formas de entrenar modelos lineales, tanto para la regresión como para la clasificación. Utilizó una ecuación de forma cerrada para resolver la regresión lineal, así como el descenso de gradiente, y aprendió cómo se pueden agregar varias sanciones a la función de costo durante el entrenamiento para regularizar el modelo. \n",
    "En el camino, también aprendiste a trazar curvas de aprendizaje y analizarlas, y a implementar la parada temprana. Finalmente, aprendiste cómo funcionan la regresión logística y la regresión softmax. ¡Hemos abierto las primeras cajas negras de aprendizaje automático! En los próximos capítulos abriremos muchos más, empezando por las máquinas vectoriales de soporte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce9da6",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "1. ¿Qué algoritmo de entrenamiento de regresión lineal puedes usar si tienes un conjunto de entrenamiento con millones de características?\n",
    "\n",
    "2. Supongamos que las características de su conjunto de entrenamiento tienen escalas muy diferentes. ¿Qué algoritmos podrían sufrir esto y cómo? ¿Qué puedes hacer al respecto?\n",
    "\n",
    "3. ¿Puede el descenso de gradiente quedarse atascado en un mínimo local al entrenar un modelo de regresión logística?\n",
    "\n",
    "4. ¿Todos los algoritmos de descenso de gradiente conducen al mismo modelo, siempre que los dejes correr el tiempo suficiente?\n",
    "\n",
    "5. Supongamos que usas el descenso de gradiente de lote y trazas el error de validación en cada época. Si notas que el error de validación aumenta constantemente, ¿qué es probable que esté pasando? ¿Cómo puedes arreglar esto?\n",
    "\n",
    "6. ¿Es una buena idea detener el descenso del gradiente de mini lotes inmediatamente cuando aumenta el error de validación?\n",
    "\n",
    "7. ¿Qué algoritmo de descenso de gradiente (entre los que discutimos) llegará más rápido a la proximidad de la solución óptima? ¿Qué convergerá realmente? ¿Cómo puedes hacer que los demás también converjan?\n",
    "\n",
    "8. Supongamos que está utilizando la regresión polinómica. Trazas las curvas de aprendizaje y notas que hay una gran brecha entre el error de entrenamiento y el error de validación. ¿Qué está pasando? ¿Cuáles son las tres formas de resolver esto?\n",
    "\n",
    "9. Supongamos que está utilizando la regresión de cresta y nota que el error de entrenamiento y el error de validación son casi iguales y bastante altos. ¿Dirías que el modelo sufre de alto sesgo o alta variación? ¿Deberías aumentar el hiperparámetro de regularización α o reducirlo?\n",
    "\n",
    "10. ¿Por qué querrías usar:\n",
    "- 1. ¿Regresión de crestas en lugar de regresión lineal simple (es decir, sin ninguna regularización)?\n",
    "- 2. ¿Lazo en lugar de regresión de cresta?\n",
    "- 3. ¿Regreso elástico en lugar de regresión de lazo?\n",
    "\n",
    "11. Supongamos que quieres clasificar las imágenes como exteriores/interiores y diurnas/noches. ¿Debería implementar dos clasificadores de regresión logística o un clasificador de regresión softmax?\n",
    "\n",
    "12. Implemente el descenso de gradiente por lotes con parada temprana para la regresión softmax sin usar Scikit-Learn, solo NumPy. Úsalo en una tarea de clasificación como el conjunto de datos del iris.\n",
    "\n",
    "Las soluciones a estos ejercicios están disponibles al final del cuaderno de este capítulo, en https://homl.info/colab3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7dc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
