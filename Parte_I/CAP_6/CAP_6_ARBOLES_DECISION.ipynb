{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de8e8b2",
   "metadata": {},
   "source": [
    "# Capítulo 6: Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65fbb5",
   "metadata": {},
   "source": [
    "Los árboles de decisión son algoritmos versátiles de aprendizaje automático que pueden realizar tareas de **clasificación** y **regresión**, e incluso tareas de múltiples salidas. \n",
    "\n",
    "Son algoritmos potentes, capaces de ajustar conjuntos de datos complejos. \n",
    "\n",
    "Por ejemplo, en el Capítulo 2 entrenó un modelo `DecisionTreeRegressor` en el conjunto de datos de vivienda de California, ajustándolo perfectamente (en realidad, sobreajustándolo).\n",
    "\n",
    "Los árboles de decisión también son los componentes fundamentales de los bosques aleatorios (véase el capítulo 7), que se encuentran entre los algoritmos de aprendizaje automático más potentes disponibles en la actualidad.\n",
    "\n",
    "\n",
    "En este capítulo comenzaremos discutiendo cómo **entrenar, visualizar y hacer predicciones con árboles de decisión**. Luego revisaremos el algoritmo de entrenamiento **CART** utilizado por Scikit-Learn, y exploraremos cómo regularizar los árboles y usarlos para tareas de regresión. \n",
    "\n",
    "Por último, discutiremos algunas de las limitaciones de los árboles de decisión.\n",
    "\n",
    "</br>\n",
    "\n",
    "## Crear y visualizar un árbol de decisiones\n",
    "\n",
    "\n",
    "Para comprender los árboles de decisión, construyamos uno y echemos un vistazo a cómo hace predicciones. \n",
    "\n",
    "El siguiente código entrena un DecisionTreeClassifier en el conjunto de datos del iris (consulte el Capítulo 4):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b19a076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y_iris = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f252c41",
   "metadata": {},
   "source": [
    "Puede visualizar el árbol de decisión entrenado usando primero la función `export_graphviz()` para generar un archivo de definición de gráfico llamado `iris_tree.dot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f0bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=\"iris_tree.dot\",\n",
    "        feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf8948",
   "metadata": {},
   "source": [
    "Después puedes usar `graphviz.Source.from_file()` para cargar y mostrar el archivo en Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70219d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"351pt\" height=\"314pt\"\n",
       " viewBox=\"0.00 0.00 351.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-310 347,-310 347,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M209.5,-306C209.5,-306 65.5,-306 65.5,-306 59.5,-306 53.5,-300 53.5,-294 53.5,-294 53.5,-235 53.5,-235 53.5,-229 59.5,-223 65.5,-223 65.5,-223 209.5,-223 209.5,-223 215.5,-223 221.5,-229 221.5,-235 221.5,-235 221.5,-294 221.5,-294 221.5,-300 215.5,-306 209.5,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) &lt;= 2.45</text>\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M105,-179.5C105,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 105,-111.5 105,-111.5 111,-111.5 117,-117.5 117,-123.5 117,-123.5 117,-167.5 117,-167.5 117,-173.5 111,-179.5 105,-179.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"58.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"middle\" x=\"58.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"58.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M110.09,-222.91C102.49,-211.65 94.23,-199.42 86.59,-188.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.39,-186 80.89,-179.67 83.59,-189.91 89.39,-186\"/>\n",
       "<text text-anchor=\"middle\" x=\"76.14\" y=\"-200.51\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M286,-187C286,-187 147,-187 147,-187 141,-187 135,-181 135,-175 135,-175 135,-116 135,-116 135,-110 141,-104 147,-104 147,-104 286,-104 286,-104 292,-104 298,-110 298,-116 298,-116 298,-175 298,-175 298,-181 292,-187 286,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) &lt;= 1.75</text>\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.91,-222.91C170.91,-214.01 177.33,-204.51 183.53,-195.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"186.44,-197.27 189.14,-187.02 180.64,-193.35 186.44,-197.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.9\" y=\"-207.86\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#4de88e\" stroke=\"black\" d=\"M196,-68C196,-68 99,-68 99,-68 93,-68 87,-62 87,-56 87,-56 87,-12 87,-12 87,-6 93,0 99,0 99,0 196,0 196,0 202,0 208,-6 208,-12 208,-12 208,-56 208,-56 208,-62 202,-68 196,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"147.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n",
       "<text text-anchor=\"middle\" x=\"147.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n",
       "<text text-anchor=\"middle\" x=\"147.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n",
       "<text text-anchor=\"middle\" x=\"147.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M190.81,-103.73C185.29,-94.97 179.45,-85.7 173.91,-76.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176.78,-74.89 168.48,-68.3 170.85,-78.63 176.78,-74.89\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#843de6\" stroke=\"black\" d=\"M331,-68C331,-68 238,-68 238,-68 232,-68 226,-62 226,-56 226,-56 226,-12 226,-12 226,-6 232,0 238,0 238,0 331,0 331,0 337,0 343,-6 343,-12 343,-12 343,-56 343,-56 343,-62 337,-68 331,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"284.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n",
       "<text text-anchor=\"middle\" x=\"284.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n",
       "<text text-anchor=\"middle\" x=\"284.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n",
       "<text text-anchor=\"middle\" x=\"284.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M241.82,-103.73C247.26,-94.97 253.01,-85.7 258.48,-76.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"261.52,-78.64 263.82,-68.3 255.57,-74.95 261.52,-78.64\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7fa708861a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "Source.from_file(\"iris_tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829088b",
   "metadata": {},
   "source": [
    "Graphviz es un paquete de software de visualización de gráficos de código abierto. También incluye una herramienta de línea de comandos de puntos para convertir archivos .dot a una variedad de formatos, como PDF o PNG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebc586",
   "metadata": {},
   "source": [
    "## Hacer predicciones\n",
    "\n",
    "Veamos cómo el árbol representado en la Figura 6-1 hace predicciones. Supongamos que encuentras una flor de iris y quieres clasificarla en función de sus pétalos. Comienzas en el nodo raíz (profundidad 0, en la parte superior): este nodo pregunta si la longitud del pétalo de la flor es inferior a 2,45 cm. Si lo es, entonces baja al nodo secundario izquierdo de la raíz (profundidad 1, izquierda). En este caso, es un nodo de hoja (es decir, no tiene ningún nodo hijo), por lo que no hace ninguna pregunta: simplemente mire la clase predicha para ese nodo, y el árbol de decisión predice que su flor es un Iris setosa (class=setosa).\n",
    "\n",
    "Ahora supongamos que encuentras otra flor, y esta vez la longitud del pétalo es superior a 2,45 cm. Vuelves a empezar por la raíz, pero ahora baja a su nodo hijo derecho (profundidad 1, derecha). Este no es un nodo de hoja, es un nodo dividido, por lo que hace otra pregunta: ¿el ancho del pétalo es menor de 1,75 cm? Si lo es, lo más probable es que tu flor sea un Iris versicolor (profundidad 2, izquierda). Si no, es probable que sea un Iris virginica (profundidad 2, derecha). Es así de simple.\n",
    "\n",
    "\n",
    "\n",
    "##### NOTA:\n",
    "\n",
    "**Una de las muchas cualidades de los árboles de decisión es que requieren muy poca preparación de datos. De hecho, no requieren escalado o centrado de características en absoluto.**\n",
    "\n",
    "\n",
    "El atributo de muestras de un nodo cuenta a cuántas instancias de entrenamiento se aplica. \n",
    "\n",
    "Por ejemplo, 100 instancias de entrenamiento tienen una longitud de pétalo superior a 2,45 cm (profundidad 1, derecha), y de esas 100, 54 tienen una anchura de pétalo inferior a 1,75 cm (profundidad 2, izquierda). \n",
    "\n",
    "El atributo de valor de un nodo le indica a cuántas instancias de entrenamiento de cada clase se aplica este nodo: por ejemplo, el nodo inferior derecho se aplica a 0 Iris setosa, 1 Iris versicolor y 45 Iris virginica. \n",
    "\n",
    "Finalmente, el atributo gini de un nodo mide su impureza Gini: un nodo es \"puro\" (gini=0) si todas las instancias de entrenamiento a las que aplica pertenecen a la misma clase. \n",
    "\n",
    "Por ejemplo, dado que el nodo izquierdo de profundidad 1 se aplica solo a las instancias de entrenamiento de Iris setosa, es puro y su impureza de Gini es 0. \n",
    "\n",
    "La ecuación 6-1 muestra cómo el algoritmo de entrenamiento calcula la impureza de Gini Gi del nodo i. \n",
    "\n",
    "El nodo izquierdo de profundidad 2 tiene una impureza de Gini igual a 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0,168.\n",
    "\n",
    "\n",
    "### Ecuación 6-1. Impureza de Gini\n",
    "\n",
    "<a href=\"https://ibb.co/f0ftgD8\"><img src=\"https://i.ibb.co/9Y62DZw/Captura-de-pantalla-2023-10-15-a-las-18-55-18.png\" alt=\"Captura-de-pantalla-2023-10-15-a-las-18-55-18\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22675c16",
   "metadata": {},
   "source": [
    "#### NOTA: \n",
    "\n",
    "**Scikit-Learn utiliza el algoritmo CART, que produce solo árboles binarios, lo que significa que los árboles donde los nodos divididos siempre tienen exactamente dos hijos (es decir, las preguntas solo tienen respuestas sí/no). Sin embargo, otros algoritmos, como ID3, pueden producir árboles de decisión con nodos que tienen más de dos hijos.**\n",
    "\n",
    "La Figura 6-2 muestra los límites de decisión de este árbol de decisión. La línea vertical gruesa representa el límite de decisión del nodo raíz (profundidad 0): longitud del pétalo = 2,45 cm. Dado que el área de la izquierda es pura (sólo Iris setosa), no se puede dividir más. Sin embargo, el área de la derecha es impura, por lo que el nodo derecho de profundidad 1 la divide en un ancho de pétalo = 1,75 cm (representado por la línea discontinua). \n",
    "\n",
    "Dado que `max_depth` se estableció en 2, el árbol de decisión se detiene allí. Si establece `max_depth` en 3, entonces los dos nodos de profundidad 2 agregarían cada uno otro límite de decisión (representado por las dos líneas de puntos verticales).\n",
    "\n",
    "<a href=\"https://ibb.co/6r3bqhS\"><img src=\"https://i.ibb.co/Nt8pGhq/Captura-de-pantalla-2023-10-15-a-las-19-06-40.png\" alt=\"Captura-de-pantalla-2023-10-15-a-las-19-06-40\" border=\"0\"></a><br /><a target='_blank' href='https://es.imgbb.com/'>bbfree</a><br />\n",
    "\n",
    "#### TIP:\n",
    "\n",
    "**La estructura de árbol, incluida toda la información que se muestra en la Figura 6-1, está disponible a través del atributo tree_ del clasificador. Escriba help(tree_clf.tree_) para obtener más detalles y consulte el cuaderno de este capítulo para ver un ejemplo.**\n",
    "\n",
    "\n",
    "## INTERPRETACIÓN DEL MODELO: CAJA BLANCA VS CAJA NEGRA\n",
    "\n",
    "\n",
    "Los árboles de decisión son intuitivos y sus decisiones son fáciles de interpretar. Estos modelos a menudo se llaman modelos de caja blanca. Por el contrario, como verás, los bosques aleatorios y las redes neuronales generalmente se consideran modelos de caja negra. Hacen grandes predicciones, y puedes comprobar fácilmente los cálculos que realizaron para hacer estas predicciones; sin embargo, generalmente es difícil explicar en términos simples por qué se hicieron las predicciones. Por ejemplo, si una red neuronal dice que una persona en particular aparece en una imagen, es difícil saber qué contribuyó a esta predicción: ¿Reconoció el modelo los ojos de esa persona? ¿Su boca? ¿Su nariz? ¿Sus zapatos? ¿O incluso en el sofá en el que estaban sentados? Por el contrario, los árboles de decisión proporcionan reglas de clasificación agradables y simples que incluso se pueden aplicar manualmente si es necesario (por ejemplo, para la clasificación de flores). El campo del aprendizaje automático interpretable tiene como objetivo crear sistemas de aprendizaje automático que puedan explicar sus decisiones de una manera que los humanos puedan entender. Esto es importante en muchos ámbitos, por ejemplo, para garantizar que el sistema no tome decisiones injustas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3dab3",
   "metadata": {},
   "source": [
    "## Estimación de las probabilidades de clase\n",
    "\n",
    "Un árbol de decisiones también puede estimar la probabilidad de que una instancia pertenezca a una clase k en particular. \n",
    "\n",
    "Primero atraviesa el árbol para encontrar el nodo de hoja para esta instancia, y luego devuelve la proporción de instancias de entrenamiento de clase k en este nodo. \n",
    "\n",
    "Por ejemplo, supongamos que has encontrado una flor cuyos pétalos tienen 5 cm de largo y 1,5 cm de ancho. \n",
    "\n",
    "El nodo de la hoja correspondiente es el nodo izquierdo de profundidad-2, por lo que el árbol de decisión produce las siguientes probabilidades: 0 % para Iris setosa (0/54), 90,7 % para Iris versicolor (49/54) y 9,3 % para Iris virginica (5/54). Y si le pides que prediga la clase, produce Iris versicolor (clase 1) porque tiene la mayor probabilidad. Vamos a comprobar esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4613e7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.907, 0.093]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e84227e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef04444",
   "metadata": {},
   "source": [
    "¡Perfecto! Tenga en cuenta que las probabilidades estimadas serían idénticas en cualquier otro lugar del rectángulo inferior derecho de la Figura 6-2, por ejemplo, si los pétalos tuvieran 6 cm de largo y 1,5 cm de ancho (aunque parece obvio que lo más probable es que sea un Iris virginica en este caso)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc25ba",
   "metadata": {},
   "source": [
    "## El algoritmo de entrenamiento CART\n",
    "\n",
    "Scikit-Learn utiliza el algoritmo del árbol de clasificación y regresión (CART) para entrenar los árboles de decisión (también llamados árboles de \"crecimiento\"). \n",
    "\n",
    "El algoritmo funciona dividiendo primero el conjunto de entrenamiento en dos subconjuntos utilizando una sola característica k y un umbral tk (por ejemplo, \"longitud del óte ≤ 2,45 cm\"). \n",
    "\n",
    "¿Cómo elige k y tk? Busca el par (k,tk) que produce los subconjuntos más puros, ponderados por su tamaño. \n",
    "\n",
    "La ecuación 6-2 da la función de costo que el algoritmo intenta minimizar.\n",
    "\n",
    "<a href=\"https://ibb.co/ckkCXvR\"><img src=\"https://i.ibb.co/n66n804/Captura-de-pantalla-2023-10-15-a-las-19-18-57.png\" alt=\"Captura-de-pantalla-2023-10-15-a-las-19-18-57\" border=\"0\"></a>\n",
    "\n",
    "Una vez que el algoritmo CART ha dividido con éxito el conjunto de entrenamiento en dos, divide los subconjuntos usando la misma lógica, luego los subsubconjuntos, y así sucesivamente, de forma recursiva. Deja de repetirse una vez que alcanza la profundidad máxima (definida por el hiperparámetro max_profundidad), o si no puede encontrar una división que reduzca la impureza. Algunos otros hiperparámetros (descritos en un momento) controlan condiciones de parada adicionales:\n",
    "\n",
    "`min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `and max_leaf_nodes`.\n",
    "\n",
    "#### ADVERTENCIA!:\n",
    "\n",
    "Como puede ver, el algoritmo CART es un algoritmo codicioso: busca con avidez una división óptima en el nivel superior, luego repite el proceso en cada nivel posterior. No comprueba si la división conducirá o no a la impureza más baja posible varios niveles. Un algoritmo codicioso a menudo produce una solución que es razonablemente buena pero que no garantiza que sea óptima.\n",
    "\n",
    "Desafortunadamente, se sabe que encontrar el árbol óptimo es un problema NP-completo.⁠1 Requiere tiempo O(exp(m)), lo que hace que el problema sea intratable incluso para pequeños conjuntos de entrenamiento. Esta es la razón por la que debemos conformarnos con una solución \"razonablemente buena\" al entrenar los árboles de decisión.\n",
    "\n",
    "\n",
    "## Complejidad computacional\n",
    "\n",
    "Hacer predicciones requiere atravesar el árbol de decisiones desde la raíz hasta una hoja. Los árboles de decisión generalmente están aproximadamente equilibrados, por lo que atravesar el árbol de decisiones requiere pasar aproximadamente por los nodos O(log2(m)), donde log2(m) es el logaritmo binario de m, igual a log(m) / log(2). \n",
    "\n",
    "Dado que cada nodo solo requiere comprobar el valor de una característica, la complejidad general de la predicción es O(log2(m)), independiente del número de características. \n",
    "\n",
    "Así que las predicciones son muy rápidas, incluso cuando se trata de grandes conjuntos de entrenamiento.\n",
    "\n",
    "El algoritmo de entrenamiento compara todas las características (o menos si se establece `max_feature`) en todas las muestras en cada nodo. La comparación de todas las características de todas las muestras en cada nodo da como resultado una complejidad de entrenamiento de O(n × m log2(m)).\n",
    "\n",
    "## ¿Impureza o entropía de Gini?\n",
    "\n",
    "De forma predeterminada, la clase `DecisionTreeClassifier` utiliza la medida de impureza de Gini, pero puede seleccionar la medida de impureza de entropía estableciendo el hiperparámetro de `criterion` en `\"entropy\"`. El concepto de entropía se originó en la termodinámica como una medida del desorden molecular: la entropía tiende a cero cuando las moléculas están quietas y bien ordenadas. Posteriormente, la entropía se extendió a una amplia variedad de dominios, incluida la teoría de la información de Shannon, donde mide el contenido de información promedio de un mensaje, como vimos en el capítulo 4. La entropía es cero cuando todos los mensajes son idénticos. En el aprendizaje automático, la entropía se utiliza con frecuencia como medida de impureza: la entropía de un conjunto es cero cuando contiene instancias de una sola clase. La ecuación 6-3 muestra la definición de la entropía del iésimo nodo. Por ejemplo, el nodo izquierdo de profundidad 2 en la Figura 6-1 tiene una entropía igual a –(49/54) log2 (49/54) – (5/54) log2 (5/54) ≈ 0,445.\n",
    "\n",
    "<a href=\"https://ibb.co/KzTdDJL\"><img src=\"https://i.ibb.co/S7Fbczd/Captura-de-pantalla-2023-10-15-a-las-19-22-51.png\" alt=\"Captura-de-pantalla-2023-10-15-a-las-19-22-51\" border=\"0\"></a>\n",
    "\n",
    "Entonces, ¿deberías usar la impureza o la entropía de Gini? La verdad es que la mayoría de las veces no hace una gran diferencia: conducen a árboles similares. \n",
    "\n",
    "La impureza de Gini es un poco más rápida de calcular, por lo que es un buen valor predeterminado. Sin embargo, cuando difieren, la impureza de Gini tiende a aislar la clase más frecuente en su propia rama del árbol, mientras que la entropía tiende a producir árboles ligeramente más equilibrados.\n",
    "\n",
    "## Hiperparámetros de regularización\n",
    "\n",
    "Los árboles de decisión hacen muy pocas suposiciones sobre los datos de entrenamiento (a diferencia de los modelos lineales, que asumen que los datos son lineales, por ejemplo). \n",
    "\n",
    "Si se deja sin restricciones, la estructura del árbol se adaptará a los datos de entrenamiento, ajustándolo muy de cerca, de hecho, lo más probable es que se ajuste demasiado. \n",
    "\n",
    "Tal modelo a menudo se llama modelo no paramétrico, no porque no tenga ningún parámetro (a menudo tiene mucho), sino porque el número de parámetros no se determina antes del entrenamiento, por lo que la estructura del modelo es libre de adherirse estrechamente a los datos. \n",
    "\n",
    "Por el contrario, un modelo paramétrico, como un modelo lineal, tiene un número predeterminado de parámetros, por lo que su grado de libertad es limitado, lo que reduce el riesgo de sobreajuste (pero aumenta el riesgo de ajuste insuficiente).\n",
    "\n",
    "\n",
    "Para evitar un ajuste excesivo de los datos de entrenamiento, es necesario restringir la libertad del árbol de decisiones durante el entrenamiento. Como ya sabes, esto se llama regularización. Los hiperparámetros de regularización dependen del algoritmo utilizado, pero generalmente se puede al menos restringir la profundidad máxima del árbol de decisión. \n",
    "\n",
    "En Scikit-Learn, esto está controlado por el hiperparámetro `max_depth`. El valor predeterminado es `None`, lo que significa ilimitado. \n",
    "Reducir `max_depth` regularizará el modelo y, por lo tanto, reducirá el riesgo de sobreajuste.\n",
    "\n",
    "La clase `DecisionTreeClassifier` tiene algunos otros parámetros que restringen de manera similar la forma del árbol de decisión:\n",
    "\n",
    "- `max_feature`\n",
    "Número máximo de características que se evalúan para su división en cada nodo\n",
    "\n",
    "- `max_leaf_nodes`\n",
    "Número máximo de nodos de hoja\n",
    "\n",
    "- `min_samples_split`\n",
    "Número mínimo de muestras que debe tener un nodo antes de que se pueda dividir\n",
    "\n",
    "- `min_samples_leaf`\n",
    "Número mínimo de muestras que se debe crear un nodo de hoja\n",
    "\n",
    "- `min_weight_fraction_leaf`\n",
    "Igual que min_samples_leaf, pero expresado como una fracción del número total de instancias ponderadas\n",
    "\n",
    "Incrementando los hiperparámetros `min_*` o reduciendo `max_*` va a regularizar el modelo.\n",
    "\n",
    "\n",
    "#### NOTA:\n",
    "\n",
    "**Otros algoritmos funcionan primero entrenando el árbol de decisiones sin restricciones, luego podando (eliminando) nodos innecesarios. Un nodo cuyos hijos son todos nodos de hoja se considera innecesario si la mejora de la pureza que proporciona no es estadísticamente significativa. Las pruebas estadísticas estándar, como la prueba χ2 (prueba de chi cuadrado), se utilizan para estimar la probabilidad de que la mejora sea puramente el resultado del azar (que se llama hipótesis nula). Si esta probabilidad, llamada valor p, es más alta que un umbral dado (normalmente 5%, controlado por un hiperparámetro), entonces el nodo se considera innecesario y sus hijos se eliminan. La poda continúa hasta que se hayan podado todos los nodos innecesarios**.\n",
    "\n",
    "\n",
    "Pruebemos la regularización en el conjunto de datos de lunas, introducido en el capítulo 5. Entrenaremos un árbol de decisiones sin regularización, y otro con `min_samples_leaf=5`. Aquí está el código; la Figura 6-3 muestra los límites de decisión de cada árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36264f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(min_samples_leaf=5, random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)\n",
    "\n",
    "tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf2 = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\n",
    "tree_clf1.fit(X_moons, y_moons)\n",
    "tree_clf2.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4fe46",
   "metadata": {},
   "source": [
    "![limites](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0603.png)\n",
    "\n",
    "(_Figura 6-3. Límites de decisión de un árbol no regularizado (izquierda) y un árbol regularizado (derecha)_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae41442",
   "metadata": {},
   "source": [
    "El modelo no regularizado de la izquierda está claramente sobreadaptado, y el modelo regularizado de la derecha probablemente generalizará mejor. Podemos verificar esto evaluando ambos árboles en un conjunto de pruebas generado utilizando una semilla aleatoria diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45dda13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11dca4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf1.score(X_moons_test, y_moons_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d68523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf2.score(X_moons_test, y_moons_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e4214e",
   "metadata": {},
   "source": [
    "De hecho, el segundo árbol tiene una mejor precisión en el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2374173c",
   "metadata": {},
   "source": [
    "## REGRESIÓN\n",
    "\n",
    "Los árboles de decisión también son capaces de realizar tareas de regresión. Construyamos un árbol de regresión usando la clase DecisionTreeRegressor de Scikit-Learn, entrenándolo en un conjunto de datos cuadrático ruidoso con `max_depth=2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842480f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "X_quad = np.random.rand(200, 1) - 0.5  # una sola característica aleatoria\n",
    "y_quad = X_quad ** 2 + 0.025 * np.random.randn(200, 1)\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X_quad, y_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6c7a1",
   "metadata": {},
   "source": [
    "El árbol resultante está representado en la Figura 6-4.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0604.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d507fc",
   "metadata": {},
   "source": [
    "Este árbol se parece mucho al árbol de clasificación que construiste antes. La principal diferencia es que en lugar de predecir una clase en cada nodo, predice un valor. Por ejemplo, supongamos que quieres hacer una predicción para una nueva instancia con x1 = 0,2. El nodo raíz pregunta si x1 ≤ 0,197. Como no lo es, el algoritmo va al nodo secundario derecho, que pregunta si x1 ≤ 0,772. Ya que lo es, el algoritmo va al nodo hijo izquierdo. Este es un nodo de hoja, y predice el value=0.111. Esta predicción es el valor objetivo promedio de las 110 instancias de entrenamiento asociadas con este nodo de hoja, y da como resultado un error al cuadrado medio igual a 0,015 sobre estas 110 instancias.\n",
    "\n",
    "Las predicciones de este modelo están representadas a la izquierda en la Figura 6-5. Si estableces max_depth=3, obtienes las predicciones representadas a la derecha. Observe cómo el valor predicho para cada región es siempre el valor objetivo promedio de las instancias de esa región. El algoritmo divide cada región de manera que hace que la mayoría de las instancias de entrenamiento se acerquen lo más posible a ese valor predicho.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0605.png)\n",
    "\n",
    "(_Figura 6-5. Predicciones de dos modelos de regresión del árbol de decisiones_)\n",
    "\n",
    "El algoritmo CART funciona como se describió anteriormente, excepto que en lugar de tratar de dividir el conjunto de entrenamiento de una manera que minimice la impureza, ahora intenta dividir el conjunto de entrenamiento de una manera que minimice el MSE. La ecuación 6-4 muestra la función de coste que el algoritmo intenta minimizar.\n",
    "\n",
    "### Ecuación 6-4. Función de coste CART para la regresión\n",
    "\n",
    "<a href=\"https://ibb.co/MB0NYXv\"><img src=\"https://i.ibb.co/2Z4KDb9/Captura-de-pantalla-2023-10-24-a-las-3-56-16.png\" alt=\"Captura-de-pantalla-2023-10-24-a-las-3-56-16\" border=\"0\"></a>\n",
    "\n",
    "Al igual que las tareas de clasificación, los árboles de decisión tienden a sobreajustarse cuando se trata de tareas de regresión. Sin ninguna regularización (es decir, utilizando los hiperparámetros predeterminados), se obtienen las predicciones a la izquierda en la Figura 6-6. Obviamente, estas predicciones están sobreajustando muy mal el conjunto de entrenamiento. Simplemente establecer `min_samples_leaf=10` da como resultado un modelo mucho más razonable, representado a la derecha en la Figura 6-6.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0606.png)\n",
    "\n",
    "(_Figura 6-6. Predicciones de un árbol de regresión no regularizado (izquierda) y un árbol regularizado (derecha)_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95f8d6",
   "metadata": {},
   "source": [
    "## Sensibilidad a la orientación del eje\n",
    "\n",
    "Esperemos que a estas alturas ya estés convencido de que los árboles de decisión tienen mucho a su alcance: son relativamente fáciles de entender e interpretar, fáciles de usar, versátiles y potentes. Sin embargo, tienen algunas limitaciones. En primer lugar, como habrás notado, a los árboles de decisión les encantan los límites de decisión ortogonales (todas las divisiones son perpendiculares a un eje), lo que los hace sensibles a la orientación de los datos. Por ejemplo, la Figura 6-7 muestra un conjunto de datos simple separable linealmente: a la izquierda, un árbol de decisiones puede dividirlo fácilmente, mientras que a la derecha, después de que el conjunto de datos se gire 45°, el límite de la decisión parece innecesariamente enrevesado. Aunque ambos árboles de decisión se ajustan perfectamente al conjunto de entrenamiento, es muy probable que el modelo de la derecha no se generalice bien.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0607.png)\n",
    "\n",
    "Una forma de limitar este problema es escalar los datos y luego aplicar una transformación de análisis de componentes principales. Miraremos el PCA en detalle en el capítulo 8, pero por ahora solo necesita saber que gira los datos de una manera que reduce la correlación entre las características, lo que a menudo (no siempre) hace que las cosas sean más fáciles para los árboles.\n",
    "\n",
    "Creemos una pequeña canalización que escale los datos y los rote usando PCA, luego entrenemos un DecisionTreeClassifier con esos datos. La Figura 6-8 muestra los límites de decisión de ese árbol: como puede ver, la rotación permite ajustar bastante bien el conjunto de datos utilizando solo una característica, z1, que es una función lineal de la longitud y el ancho del pétalo original.\n",
    "\n",
    "Aquí está el código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141b12d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca_pipeline = make_pipeline(StandardScaler(), PCA())\n",
    "X_iris_rotated = pca_pipeline.fit_transform(X_iris)\n",
    "tree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf_pca.fit(X_iris_rotated, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d87f24",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0608.png)\n",
    "\n",
    "(_Figura 6-8. Límites de decisión de un árbol en el conjunto de datos de iris escalado y rotado por PCA_)\n",
    "\n",
    "## Los árboles de decisión tienen una gran variación\n",
    "\n",
    "De manera más general, el principal problema con los árboles de decisión es que tienen una variación bastante alta: los pequeños cambios en los hiperparámetros o en los datos pueden producir modelos muy diferentes. De hecho, dado que el algoritmo de entrenamiento utilizado por Scikit-Learn es estocástico, selecciona al azar el conjunto de características para evaluar en cada nodo, incluso el reentrenamiento del mismo árbol de decisiones en exactamente los mismos datos puede producir un modelo muy diferente, como el representado en la Figura 6-9 (a menos que establezca el hiperparámetro random_state). Como puedes ver, se ve muy diferente del árbol de decisiones anterior (Figura 6-2).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0609.png)\n",
    "\n",
    "Afortunadamente, al promediar las predicciones sobre muchos árboles, es posible reducir la varianza significativamente. Tal conjunto de árboles se llama bosque aleatorio, y es uno de los tipos de modelos más poderosos disponibles hoy en día, como verás en el próximo capítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf5490",
   "metadata": {},
   "source": [
    "* ¿Cuál es la profundidad aproximada de un árbol de decisiones entrenado (sin restricciones) en un conjunto de entrenamiento con un millón de instancias?\n",
    "\n",
    "* ¿La impureza de Gini de un nodo es generalmente más baja o más alta que la de sus padres? ¿Es generalmente más bajo/más alto, o siempre más bajo/más alto?\n",
    "\n",
    "* Si un árbol de decisiones se está ajustando demasiado al conjunto de entrenamiento, ¿es una buena idea intentar disminuirmaxmax_depth?\n",
    "\n",
    "* Si un árbol de decisiones no se ajusta al conjunto de entrenamiento, ¿es una buena idea intentar escalar las características de entrada?\n",
    "\n",
    "* Si se tarda una hora en entrenar un árbol de decisiones en un conjunto de entrenamiento que contiene un millón de instancias, ¿cuánto tiempo llevará aproximadamente entrenar otro árbol de decisiones en un conjunto de entrenamiento que contiene diez millones de instancias? Sugerencia: considere la complejidad computacional del algoritmo CART.\n",
    "\n",
    "* Si se tarda una hora en entrenar un árbol de decisiones en un conjunto de entrenamiento determinado, ¿cuánto tiempo tomará aproximadamente si duplicas el número de características?\n",
    "\n",
    "* Entrena y afina un árbol de decisiones para el conjunto de datos de lunas siguiendo estos pasos:\n",
    "\n",
    "- Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.\n",
    "- Use train_test_split() to split the dataset into a training set and a test set.\n",
    "- Utilice la búsqueda de cuadrícula con validación cruzada (con la ayuda de la clase GridSearchCV) para encontrar buenos valores de hiperparámetros para aDecisionTreeClassifier. Sugerencia: prueba varios valores formaxmax_leaf_nodes.\n",
    "- Entrénelo en el conjunto de entrenamiento completo utilizando estos hiperparámetros y mide el rendimiento de tu modelo en el conjunto de pruebas. Deberías obtener aproximadamente un 85 % a un 87 % de precisión.\n",
    "\n",
    "Cultiva un bosque siguiendo estos pasos:\n",
    "\n",
    "- Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn’s ShuffleSplit class for this.\n",
    "- Entrena un árbol de decisiones en cada subconjunto, utilizando los mejores valores de hiperparámetros encontrados en el ejercicio anterior. Evalúe estos 1.000 árboles de decisión en el conjunto de pruebas. Dado que fueron entrenados en conjuntos más pequeños, estos árboles de decisión probablemente tendrán un rendimiento peor que el primer árbol de decisión, logrando solo alrededor del 80 % de precisión.\n",
    "- Now comes the magic. For each test set instance, generate the predictions of the 1,000 decision trees, and keep only the most frequent prediction (you can use SciPy’s mode() function for this). This approach gives you majority-vote predictions over the test set.\n",
    "- Evalúe estas predicciones en el conjunto de pruebas: debe obtener una precisión ligeramente mayor que su primer modelo (alrededor de 0,5 a 1,5 % más alto). ¡Enhorabuena, has entrenado a un clasificador de bosques al azar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf260112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
