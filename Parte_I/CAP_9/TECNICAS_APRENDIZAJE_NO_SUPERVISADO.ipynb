{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8190b44",
   "metadata": {},
   "source": [
    "# TÉCNICAS DE APRENDIZAJE NO SUPERVISADAS\n",
    "\n",
    "Aunque la mayoría de las aplicaciones del aprendizaje automático hoy en día se basan en el aprendizaje supervisado (y, como resultado, aquí es donde van la mayoría de las inversiones), la gran mayoría de los datos disponibles no están etiquetados: tenemos las características de entrada **X**, pero no tenemos las etiquetas **y**. El científico informático Yann LeCun dijo que \"si la inteligencia fuera un pastel, el aprendizaje no supervisado sería el pastel, el aprendizaje supervisado sería la guinda del pastel, y el aprendizaje de refuerzo sería la guinda del pastel\". En otras palabras, hay un gran potencial en el aprendizaje no supervisado en el que apenas hemos empezado a hundir nuestros dientes.\n",
    "\n",
    "Digamos que desea crear un sistema que tome algunas fotos de cada artículo en una línea de producción y detecte qué artículos son defectuosos. Puedes crear fácilmente un sistema que tome fotos automáticamente, y esto podría darte miles de fotos todos los días. A continuación, puede construir un conjunto de datos razonablemente grande en solo unas pocas semanas. Pero espera, ¡no hay etiquetas! Si desea entrenar a un clasificador binario regular que prediga si un artículo es defectuoso o no, tendrá que etiquetar cada imagen como \"defectuosa\" o \"normal\". Esto generalmente requerirá que los expertos humanos se sienten y revisen manualmente todas las imágenes. Esta es una tarea larga, costosa y tediosa, por lo que generalmente solo se hará en un pequeño subconjunto de las imágenes disponibles. Como resultado, el conjunto de datos etiquetado será bastante pequeño y el rendimiento del clasificador será decepcionante. Además, cada vez que la empresa realice algún cambio en sus productos, todo el proceso tendrá que comenzar de nuevo desde cero. ¿No sería genial si el algoritmo pudiera explotar los datos no etiquetados sin necesidad de que los humanos etiqueten cada imagen? Entra en el aprendizaje no supervisado.\n",
    "\n",
    "En el capítulo 8 analizamos la tarea de aprendizaje no supervisado más común: la reducción de la dimensionalidad. En este capítulo nos pondremos en cuenta algunas tareas más sin supervisión:\n",
    "\n",
    "\n",
    "- **Agrupamiento**\n",
    "El objetivo es agrupar instancias similares en grupos. La agrupación es una gran herramienta para el análisis de datos, la segmentación de clientes, los sistemas de recomendación, los motores de búsqueda, la segmentación de imágenes, el aprendizaje semisupervisado, la reducción de la dimensionalidad y más.\n",
    "\n",
    "\n",
    "- **Detección de anomalías** (también llamada detección de valores atípicos)\n",
    "El objetivo es aprender cómo se ven los datos \"normales\" y luego usarlos para detectar instancias anormales. Estas instancias se llaman anomalías, o valores atípicos, mientras que las instancias normales se llaman valores atípicos. La detección de anomalías es útil en una amplia variedad de aplicaciones, como la detección de fraude, la detección de productos defectuosos en la fabricación, la identificación de nuevas tendencias en las series temporales o la eliminación de valores atípicos de un conjunto de datos antes de entrenar otro modelo, lo que puede mejorar significativamente el rendimiento del modelo resultante.\n",
    "\n",
    "\n",
    "- **Estimación de la densidad**\n",
    "Esta es la tarea de estimar la función de densidad de probabilidad (PDF) del proceso aleatorio que generó el conjunto de datos. La estimación de la densidad se utiliza comúnmente para la detección de anomalías: es probable que las instancias ubicadas en regiones de muy baja densidad sean anomalías. También es útil para el análisis y la visualización de datos.\n",
    "\n",
    "¿Listo para un poco de pastel? Comenzaremos con dos algoritmos de agrupación, k-means y DBSCAN, luego discutiremos los modelos de mezcla gaussiana y veremos cómo se pueden utilizar para la estimación de la densidad, la agrupación y la detección de anomalías."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9a8fc",
   "metadata": {},
   "source": [
    "# Algoritmo de agrupación: K-means y DBSCAN\n",
    "\n",
    "\n",
    "Mientras disfrutas de una caminata por las montañas, te encuentras con una planta que nunca antes habías visto. Miras a tu alrededor y notas algunos más. No son idénticos, pero son lo suficientemente similares como para que sepas que lo más probable es que pertenezcan a la misma especie (o al menos al mismo género). Es posible que necesites un botánico que te diga qué especie es, pero ciertamente no necesitas un experto para identificar grupos de objetos de aspecto similar. Esto se llama _agrupación_: es la tarea de identificar instancias similares y asignarlas a _clústeres_ o grupos de instancias similares.\n",
    "\n",
    "Al igual que en la clasificación, cada instancia se asigna a un grupo. Sin embargo, a diferencia de la clasificación, la agrupación es una tarea sin supervisión. Considere la Figura 9-1: a la izquierda está el conjunto de datos del iris (introducido en el capítulo 4), donde la especie de cada instancia (es decir, su clase) se representa con un marcador diferente. Es un conjunto de datos etiquetado, para el que los algoritmos de clasificación como la regresión logística, los SVM o los clasificadores de bosques aleatorios son muy adecuados. A la derecha está el mismo conjunto de datos, pero sin las etiquetas, por lo que ya no puedes usar un algoritmo de clasificación. Aquí es donde intervienen los algoritmos de agrupación: muchos de ellos pueden detectar fácilmente el clúster inferior izquierdo. También es bastante fácil de ver con nuestros propios ojos, pero no es tan obvio que el grupo superior derecho esté compuesto por dos subgrupos distintos. Dicho esto, el conjunto de datos tiene dos características adicionales (longitud y ancho del sépalo) que no están representadas aquí, y los algoritmos de agrupación pueden hacer un buen uso de todas las características, por lo que, de hecho, identifican los tres grupos bastante bien (por ejemplo, utilizando un modelo de mezcla gaussiano, solo 5 instancias de 150 están asignadas al clúster equivocado).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0901.png)\n",
    "\n",
    "_Figura 9-1. Clasificación (izquierda) frente a agrupación (derecha)_\n",
    "\n",
    "\n",
    "La agrupación se utiliza en una amplia variedad de aplicaciones, incluyendo:\n",
    "\n",
    "- **Segmentación de clientes**\n",
    "Puede agrupar a sus clientes en función de sus compras y su actividad en su sitio web. Esto es útil para entender quiénes son sus clientes y qué necesitan, para que pueda adaptar sus productos y campañas de marketing a cada segmento. Por ejemplo, la segmentación de clientes puede ser útil en los sistemas de recomendación para sugerir contenido que otros usuarios del mismo grupo disfrutaron.\n",
    "\n",
    "\n",
    "- **Análisis de datos**\n",
    "Cuando se analiza un nuevo conjunto de datos, puede ser útil ejecutar un algoritmo de agrupación y luego analizar cada agrupación por separado.\n",
    "\n",
    "\n",
    "- **Reducción de la dimensionalidad**\n",
    "Una vez que se ha agrupado un conjunto de datos, generalmente es posible medir la afinidad de cada instancia con cada clúster; la afinidad es cualquier medida de lo bien que encaja una instancia en un clúster. El vector de características de cada instancia x se puede reemplazar con el vector de sus afinidades de clúster. Si hay grupos k, entonces este vector es k-dimensional. El nuevo vector suele ser de una dimensión mucho menor que el vector de características original, pero puede conservar suficiente información para su posterior procesamiento.\n",
    "\n",
    "\n",
    "- **Ingeniería de características**\n",
    "Las afinidades de los grupos a menudo pueden ser útiles como características adicionales. Por ejemplo, utilizamos k-means en el capítulo 2 para agregar características de afinidad geográfica del clúster al conjunto de datos de viviendas de California, y nos ayudaron a obtener un mejor rendimiento.\n",
    "\n",
    "\n",
    "- **Detección de anomalías (también llamada detección de valores atípicos)**\n",
    "Es probable que cualquier instancia que tenga una baja afinidad con todos los clústeres sea una anomalía. Por ejemplo, si ha agrupado a los usuarios de su sitio web en función de su comportamiento, puede detectar usuarios con un comportamiento inusual, como un número inusual de solicitudes por segundo.\n",
    "\n",
    "\n",
    "- **Aprendizaje semisupervisado**\n",
    "Si solo tienes unas pocas etiquetas, podrías realizar agrupaciones y propagar las etiquetas a todas las instancias del mismo clúster. Esta técnica puede aumentar en gran medida el número de etiquetas disponibles para un algoritmo de aprendizaje supervisado posterior y, por lo tanto, mejorar su rendimiento.\n",
    "\n",
    "\n",
    "- **Motores de búsqueda**\n",
    "Algunos motores de búsqueda te permiten buscar imágenes que sean similares a una imagen de referencia. Para construir un sistema de este tipo, primero aplicaría un algoritmo de agrupación a todas las imágenes de su base de datos; imágenes similares terminarían en el mismo clúster. Luego, cuando un usuario proporciona una imagen de referencia, todo lo que necesitaría hacer es usar el modelo de agrupación entrenado para encontrar el clúster de esta imagen, y luego podría simplemente devolver todas las imágenes de este clúster.\n",
    "\n",
    "\n",
    "- **Segmentación de imágenes**\n",
    "Al agrupar los píxeles de acuerdo con su color y luego reemplazar el color de cada píxel con el color medio de su grupo, es posible reducir considerablemente el número de diferentes colores en una imagen. La segmentación de imágenes se utiliza en muchos sistemas de detección y seguimiento de objetos, ya que facilita la detección del contorno de cada objeto.\n",
    "\n",
    "\n",
    "\n",
    "No hay una definición universal de lo que es un clúster: realmente depende del contexto, y diferentes algoritmos capturarán diferentes tipos de clústeres. Algunos algoritmos buscan instancias centradas en un punto en particular, llamado centroide. Otros buscan regiones continuas de instancias densamente empaquetadas: estos grupos pueden tomar cualquier forma. Algunos algoritmos son jerárquicos y buscan grupos de grupos. Y la lista continúa.\n",
    "\n",
    "En esta sección, examinaremos dos algoritmos de agrupación populares, k-means y DBSCAN, y exploraremos algunas de sus aplicaciones, como la reducción de la dimensionalidad no lineal, el aprendizaje semisupervisado y la detección de anomalías."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfceda",
   "metadata": {},
   "source": [
    "## K-MEANS\n",
    "\n",
    "\n",
    "Considere el conjunto de datos sin etiquetar representado en la Figura 9-2: puede ver claramente cinco blobs de instancias. El algoritmo k-means es un algoritmo simple capaz de agrupar este tipo de conjunto de datos de manera muy rápida y eficiente, a menudo en solo unas pocas iteraciones. Fue propuesto por Stuart Lloyd en Bell Labs en 1957 como una técnica para la modulación del código de pulso, pero solo se publicó fuera de la compañía en 1982.⁠1 En 1965, Edward W. Forgy había publicado prácticamente el mismo algoritmo, por lo que k-means a veces se conoce como el algoritmo Lloyd-Forgy.\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0902.png)\n",
    "\n",
    "_Figura 9-2. Un conjunto de datos sin etiquetar compuesto por cinco blobs de instancias_\n",
    "\n",
    "\n",
    "Entrenemos a un agrupador de k-means en este conjunto de datos. Intentará encontrar el centro de cada blob y asignará cada instancia al blob más cercano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63faa6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs([...])  # hacer los blobs: y contiene el ID del cluster\n",
    "                          # esto es lo que queremos predecir\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96857279",
   "metadata": {},
   "source": [
    "Tenga en cuenta que tiene que especificar el número de clústeres k que el algoritmo debe encontrar. En este ejemplo, es bastante obvio al mirar los datos que k debe establecerse en 5, pero en general no es tan fácil. Discutiremos esto en breve.\n",
    "\n",
    "Cada instancia se asignará a uno de los cinco grupos. En el contexto de la agrupación, la etiqueta de una instancia es el índice del clúster al que el algoritmo asigna esta instancia; Esto no debe confundirse con las etiquetas de clase en la clasificación, que se utilizan como objetivos (recuerde que la agrupación en clústeres es una tarea de aprendizaje no supervisada). La instancia de `KMeans` conserva las etiquetas previstas de las instancias en las que se entrenó, disponibles a través de la variable de instancia `labels_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e636478",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n",
    "#array([4, 0, 1, ..., 2, 1, 0], dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred is kmeans.labels_\n",
    "#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_\n",
    "#array([[-2.80389616,  1.80117999],\n",
    "#       [ 0.20876306,  2.25551336],\n",
    "#       [-2.79290307,  2.79641063],\n",
    "#       [-1.46679593,  2.28585348],\n",
    "#       [-2.80037642,  1.30082566]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ba1ec",
   "metadata": {},
   "source": [
    "Puede asignar fácilmente nuevas instancias al clúster cuyo centroide sea más cercano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34048072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_new = np.array([[0,2], [3,2], [-3,3], [-3,2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445de537",
   "metadata": {},
   "source": [
    "Si trazas los límites de decisión del grupo, obtienes una teselación de Voronoi: véase la Figura 9-3, donde cada centroide se representa con una X.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0903.png)\n",
    "\n",
    "_Figura 9-3. k-medios límites de decisión (tesselación de Voronoi)_\n",
    "\n",
    "\n",
    "La gran mayoría de las instancias se asignaron claramente al clúster apropiado, pero algunas instancias probablemente se etiquetaron mal, especialmente cerca del límite entre el clúster superior izquierdo y el clúster central. De hecho, el algoritmo k-means no se comporta muy bien cuando las manchas tienen diámetros muy diferentes porque lo que le importa al asignar una instancia a un grupo es la distancia al centroide.\n",
    "\n",
    "En lugar de asignar cada instancia a un único clúster, lo que se denomina agrupación estricta, puede resultar útil asignar a cada instancia una puntuación por clúster, lo que se denomina agrupación suave. La puntuación puede ser la distancia entre la instancia y el centroide o una puntuación de similitud (o afinidad), como la función de base radial gaussiana que utilizamos en el Capítulo 2. En la clase `KMeans`, el método `transform()` mide la distancia desde cada instancia. a cada centroide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.transform(X_new).round(2)\n",
    "#array([[2.81, 0.33, 2.9 , 1.49, 2.89],\n",
    "#       [5.81, 2.8 , 5.85, 4.48, 5.84],\n",
    "#       [1.21, 3.29, 0.29, 1.69, 1.71],\n",
    "#       [0.73, 3.22, 0.36, 1.55, 1.22]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029f2dc",
   "metadata": {},
   "source": [
    "En este ejemplo, la primera instancia en `X_new` está ubicada a una distancia de aproximadamente 2,81 del primer centroide, 0,33 del segundo centroide, 2,90 del tercer centroide, 1,49 del cuarto centroide y 2,89 del quinto centroide. Si tiene un conjunto de datos de alta dimensión y lo transforma de esta manera, terminará con un conjunto de datos de k dimensiones: esta transformación puede ser una técnica de reducción de dimensionalidad no lineal muy eficiente. Alternativamente, puedes usar estas distancias como funciones adicionales para entrenar otro modelo, como en el Capítulo 2.\n",
    "\n",
    "### El algoritmo k-means\n",
    "\n",
    "Entonces, ¿cómo funciona el algoritmo? Bueno, supongamos que te dieron los centroides. Podrías etiquetar fácilmente todas las instancias del conjunto de datos asignando cada una de ellas al clúster cuyo centroide es el más cercano. Por el contrario, si se le dieran todas las etiquetas de instancia, podría localizar fácilmente el centroide de cada clúster calculando la media de las instancias en ese clúster. Pero no se te dan ni las etiquetas ni los centroides, así que ¿cómo puedes proceder? Comience colocando los centroides al azar (por ejemplo, eligiendo k instancias al azar del conjunto de datos y utilizando sus ubicaciones como centroides). Luego etiquete las instancias, actualice los centroides, etiquete las instancias, actualice los centroides, y así sucede hasta que los centroides dejen de moverse. Se garantiza que el algoritmo converge en un número finito de pasos (generalmente bastante pequeños). Eso se debe a que la distancia media al cuadrado entre las instancias y sus centroides más cercanos solo puede bajar en cada paso, y como no puede ser negativa, se garantiza que converge.\n",
    "\n",
    "Puedes ver el algoritmo en acción en la Figura 9-4: los centroides se inicializan al azar (arriba a la izquierda), luego las instancias se etiquetan (arriba a la derecha), luego los centroides se actualizan (centro a la izquierda), las instancias se vuelven a etiquetar (centro a la derecha), y así suceden. Como puede ver, en solo tres iteraciones el algoritmo ha alcanzado una agrupación que parece estar cerca de la óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca306aca",
   "metadata": {},
   "source": [
    "#### NOTA\n",
    "\n",
    "La complejidad computacional del algoritmo es generalmente lineal con respecto al número de instancias m, el número de grupos k y el número de dimensiones n. Sin embargo, esto solo es cierto cuando los datos tienen una estructura de agrupación. Si no lo hace, entonces en el peor de los casos la complejidad puede aumentar exponencialmente con el número de instancias. En la práctica, esto rara vez sucede, y k-means es generalmente uno de los algoritmos de agrupación más rápidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13643cfb",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0904.png)\n",
    "\n",
    "_Figura 9-4. El algoritmo k-means_\n",
    "\n",
    "\n",
    "Aunque se garantiza que el algoritmo converge, puede no converger a la solución correcta (es decir, puede converger a un óptimo local): si lo hace o no depende de la inicialización del centroide. La figura 9-5 muestra dos soluciones subóptimas a las que el algoritmo puede converger si no tienes suerte con el paso de inicialización aleatoria.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0905.png)\n",
    "\n",
    "_Figura 9-5. Soluciones subóptimas debido a las desafortunadas inicializaciones de centroides_\n",
    "\n",
    "\n",
    "Echemos un vistazo a algunas formas en las que puede mitigar este riesgo mejorando la inicialización del centroide.\n",
    "\n",
    "\n",
    "### Métodos de inicialización de Centroid\n",
    "\n",
    "\n",
    "Si sabe aproximadamente dónde deberían estar los centroides (por ejemplo, si ejecutó otro algoritmo de agrupamiento anteriormente), puede configurar el hiperparámetro `init` en una matriz NumPy que contiene la lista de centroides y establecer `n_init` en `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b81bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394efd6",
   "metadata": {},
   "source": [
    "Otra solución es ejecutar el algoritmo varias veces con diferentes inicializaciones aleatorias y conservar la mejor solución. El número de inicializaciones aleatorias está controlado por el hiperparámetro `n_init`: de forma predeterminada es igual a `10`, lo que significa que todo el algoritmo descrito anteriormente se ejecuta 10 veces cuando llamas a `fit()`, y Scikit-Learn mantiene la mejor solución. \n",
    "\n",
    "Pero ¿cómo sabe exactamente cuál es la mejor solución? ¡Utiliza una métrica de rendimiento! Esa métrica se llama inercia del modelo, que es la suma de las distancias al cuadrado entre las instancias y sus centroides más cercanos. \n",
    "\n",
    "Es aproximadamente igual a 219,4 para el modelo de la izquierda en la Figura 9-5, 258,6 para el modelo de la derecha en la Figura 9-5 y sólo 211,6 para el modelo de la Figura 9-3. \n",
    "\n",
    "La clase `KMeans` ejecuta el algoritmo n_init veces y mantiene el modelo con la inercia más baja. En este ejemplo, se seleccionará el modelo de la Figura 9-3 (a menos que tengamos mucha mala suerte con `n_init` inicializaciones aleatorias consecutivas). \n",
    "\n",
    "Si tienes curiosidad, se puede acceder a la inercia de un modelo a través de la variable de instancia `inertia_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_\n",
    "#211.59853725816836"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3506e",
   "metadata": {},
   "source": [
    "The `score()` method returns the negative inertia (it’s negative because a predictor’s `score()` method must always respect Scikit-Learn’s “greater is better” rule: if a predictor is better than another, its `score()` method should return a greater score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd80f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.score(X)\n",
    "#-211.5985372581684"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d16a6",
   "metadata": {},
   "source": [
    "Una mejora importante para el algoritmo _k-means, k-means++_, fue propuesta en un artículo de 2006 de David Arthur y Sergei Vassilvitskii.⁠2 Introdujeron un paso de inicialización más inteligente que tiende a seleccionar centroides que están distantes entre sí, y esta mejora hace que el algoritmo k-means sea mucho menos probable que converja a una solución subóptima. \n",
    "\n",
    "El documento mostró que el cálculo adicional requerido para el paso de inicialización más inteligente vale la pena porque permite reducir drásticamente el número de veces que es necesario ejecutar el algoritmo para encontrar la solución óptima. El algoritmo de inicialización _k-means++_ funciona así:\n",
    "\n",
    "1- Tome un centroide **c(1)**, elegido uniformemente al azar del conjunto de datos.\n",
    "\n",
    "\n",
    "2- Toma un nuevo centroide **c(i)**, eligiendo una instancia **x(i)** con probabilidad <a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/5jy6VJ8/Captura-de-pantalla-2023-11-20-a-las-0-13-14.png\" alt=\"Captura-de-pantalla-2023-11-20-a-las-0-13-14\" border=\"0\"></a> , donde D(x(i)) es la distancia entre la instancia x(i) y el centroide más cercano que ya se eligió. Esta distribución de probabilidad garantiza que las instancias más alejadas de los centroides ya elegidos tengan muchas más probabilidades de ser seleccionadas como centroides.\n",
    "\n",
    "\n",
    "3- Repita el paso anterior hasta que se hayan elegido todos los k centroides.\n",
    "\n",
    "The KMeans class uses this initialization method by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57212bf6",
   "metadata": {},
   "source": [
    "### K-means acelerados y k-means de mini-lote\n",
    "\n",
    "Otra mejora en el algoritmo k-means se propuso en un artículo de 2003 de Charles Elkan.⁠3 En algunos conjuntos de datos grandes con muchos grupos, el algoritmo se puede acelerar evitando muchos cálculos de distancia innecesarios. Elkan logró esto explotando la desigualdad del triángulo (es decir, que una línea recta es siempre la distancia más corta entre dos puntos⁠ y haciendo un seguimiento de los límites inferiores y superiores para las distancias entre las instancias y los centroides. Sin embargo, el algoritmo de Elkan no siempre acelera el entrenamiento, y a veces incluso puede ralentizar el entrenamiento significativamente; depende del conjunto de datos. Aún así, si quieres probarlo, establece `algorithm=\"elkan\"`\n",
    "\n",
    "Otra variante importante del algoritmo k-means fue propuesta en un artículo de 2010 de David Sculley.⁠5 En lugar de usar el conjunto de datos completo en cada iteración, el algoritmo es capaz de usar mini-lotes, moviendo los centroides solo ligeramente en cada iteración. Esto acelera el algoritmo (normalmente en un factor de tres a cuatro) y hace posible agrupar enormes conjuntos de datos que no caben en la memoria. Scikit-Learn implementa este algoritmo en la clase `MiniBatchKMeans`, que puedes usar al igual que la clase `KMeans`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55098940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)\n",
    "minibatch_kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca288ca0",
   "metadata": {},
   "source": [
    "Si el conjunto de datos no cabe en la memoria, la opción más simple es usar la clase `memmap`, como hicimos para el PCA incremental en el capítulo 8. Alternativamente, puede pasar un mini-batch a la vez al método `partial_fit()`, pero esto requerirá mucho más trabajo, ya que tendrá que realizar múltiples inicializaciones y seleccionar la mejor usted mismo.\n",
    "\n",
    "Aunque el algoritmo k-means de mini-lote es mucho más rápido que el algoritmo normal de k-means, su inercia suele ser ligeramente peor. Puedes ver esto en la Figura 9-6: el gráfico de la izquierda compara las inercias de los mini-lotes k-means y los modelos regulares de k-means entrenados en el conjunto de datos anterior de cinco blobs utilizando varios números de grupos k. La diferencia entre las dos curvas es pequeña, pero visible. En la trama de la derecha, puedes ver que los k-medios de mini-lote son aproximadamente 3,5 veces más rápidos que los k-medios normales en este conjunto de datos.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0906.png)\n",
    "\n",
    "_Figura 9-6. Los k-medios de mini-lote tienen una inercia más alta que los k-medios (izquierda), pero es mucho más rápido (derecha), especialmente a medida que aumenta k_\n",
    "\n",
    "\n",
    "### Encontrar el número óptimo de grupos\n",
    "\n",
    "\n",
    "Hasta ahora, hemos establecido el número de clústeres k en 5 porque era obvio al mirar los datos que este era el número correcto de clústeres. Pero en general, no será tan fácil saber cómo establecer k, y el resultado podría ser bastante malo si lo estableces en el valor equivocado. Como se puede ver en la Figura 9-7, para este conjunto de datos, la configuración de k a 3 u 8 da como resultado modelos bastante malos.\n",
    "\n",
    "Podrías estar pensando que podrías elegir el modelo con la inercia más baja. Desafortunadamente, no es tan sencillo. La inercia para k=3 es de aproximadamente 654,2, que es mucho mayor que para k=5 (211,6). Pero con k=8, la inercia es de solo 119.1. La inercia no es una buena métrica de rendimiento cuando se intenta elegir k porque sigue bajando a medida que aumentamos k. De hecho, cuantos más cúmulos haya, más cerca estará cada instancia de su centroide más cercano y, por lo tanto, menor será la inercia. Traemos la inercia en función de k. Cuando hacemos esto, la curva a menudo contiene un punto de inflexión llamado codo (ver Figura 9-8).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0907.png)\n",
    "\n",
    "_Figura 9-7. Malas opciones para el número de grupos: cuando k es demasiado pequeño, los grupos separados se fusionan (izquierda), y cuando k es demasiado grande, algunos grupos se cortan en múltiples trozos (derecha)_\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0908.png)\n",
    "\n",
    "_Figura 9-8. Trazando la inercia en función del número de grupos k_\n",
    "\n",
    "\n",
    "Como puede ver, la inercia disminuye muy rápidamente a medida que aumentamos k hasta 4, pero luego disminuye mucho más lentamente a medida que seguimos aumentando k. Esta curva tiene aproximadamente la forma de un brazo, y hay un codo en k = 4. Por lo tanto, si no lo supiéramos mejor, podríamos pensar que 4 era una buena opción: cualquier valor más bajo sería dramático, mientras que cualquier valor más alto no ayudaría mucho, y podríamos estar dividiendo grupos perfectamente buenos por la mitad sin una buena razón.\n",
    "\n",
    "Esta técnica para elegir el mejor valor para el número de grupos es bastante gruesa. Un enfoque más preciso (pero también más costoso desde el punto de la computación) es usar la puntuación de silueta, que es el coeficiente de silueta medio en todas las instancias. El coeficiente de silueta de una instancia es igual a (b - a) / max(a, b), donde a es la distancia media a las otras instancias en el mismo clúster (es decir, la distancia media dentro del clúster) y b es la distancia media más cercana al clúster (es decir, la distancia media a las instancias del siguiente clúster más cercano, definida como la que minimiza b, excluyendo el propio clúster de la instancia). El coeficiente de silueta puede variar entre -1 y +1. Un coeficiente cercano a +1 significa que la instancia está bien dentro de su propio clúster y lejos de otros grupos, mientras que un coeficiente cercano a 0 significa que está cerca de un límite de clúster; finalmente, un coeficiente cercano a -1 significa que la instancia puede haber sido asignada al clúster equivocado.\n",
    "\n",
    "Para calcular la puntuación de la silueta, puede usar la función Scikit-Learn'silhouette `silhouette_score()`, dándole todas las instancias del conjunto de datos y las etiquetas que se les asignaron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X, kmeans.labels_)\n",
    "\n",
    "#0.655517642572828"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432db5a",
   "metadata": {},
   "source": [
    "Comparemos las puntuaciones de la silueta para diferentes números de grupos (ver Figura 9-9).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0909.png)\n",
    "\n",
    "_Figura 9-9. Seleccionando el número de grupos k usando la puntuación de la silueta_\n",
    "\n",
    "Como puedes ver, esta visualización es mucho más rica que la anterior: aunque confirma que k = 4 es una muy buena opción, también destaca el hecho de que k = 5 también es bastante buena, y mucho mejor que k = 6 o 7. Esto no era visible al comparar las inercias.\n",
    "\n",
    "Se obtiene una visualización aún más informativa cuando trazamos el coeficiente de silueta de cada instancia, ordenado por los grupos a los que están asignados y por el valor del coeficiente. Esto se llama diagrama de silueta (ver Figura 9-10). Cada diagrama contiene una forma de cuchillo por grupo. La altura de la forma indica el número de instancias en el clúster, y su ancho representa los coeficientes de silueta ordenados de las instancias en el clúster (más ancho es mejor).\n",
    "\n",
    "Las líneas discontinuas verticales representan la puntuación media de la silueta para cada número de grupos. Cuando la mayoría de las instancias de un clúster tienen un coeficiente más bajo que esta puntuación (es decir, si muchas de las instancias se detienen en la línea discontinua, terminando a la izquierda de ella), entonces el clúster es bastante malo, ya que esto significa que sus instancias están demasiado cerca de otros grupos. Aquí podemos ver que cuando k = 3 o 6, tenemos grupos malos. Pero cuando k = 4 o 5, los grupos se ven bastante bien: la mayoría de los casos se extienden más allá de la línea discontinua, a la derecha y más cerca de 1.0. Cuando k = 4, el grupo en el índice 1 (el segundo desde la parte inferior) es bastante grande. Cuando k = 5, todos los grupos tienen tamaños similares. Por lo tanto, a pesar de que la puntuación general de la silueta de k = 4 es ligeramente mayor que la de k = 5, parece una buena idea usar k = 5 para obtener grupos de tamaños similares.\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0910.png)\n",
    "\n",
    "_Figura 9-10. Analizando los diagramas de silueta para varios valores de k_\n",
    "\n",
    "\n",
    "### Límites de k-means\n",
    "\n",
    "\n",
    "A pesar de sus muchos méritos, sobre todo por ser rápido y escalable, k-means no es perfecto. Como vimos, es necesario ejecutar el algoritmo varias veces para evitar soluciones subóptimas, además de que es necesario especificar el número de clústeres, lo que puede ser bastante complicado. Además, los k-medios no se comportan muy bien cuando los grupos tienen diferentes tamaños, diferentes densidades o formas no esféricas. Por ejemplo, la Figura 9-11 muestra cómo k-means agrupa un conjunto de datos que contiene tres grupos elipsoidales de diferentes dimensiones, densidades y orientaciones.\n",
    "\n",
    "Como puedes ver, ninguna de estas soluciones es buena. La solución de la izquierda es mejor, pero aún así corta el 25 % del grupo central y lo asigna al grupo de la derecha. La solución de la derecha es simplemente terrible, a pesar de que su inercia es menor. Por lo tanto, dependiendo de los datos, los diferentes algoritmos de agrupación pueden funcionar mejor. En este tipo de grupos elípticos, los modelos de mezcla gaussiana funcionan muy bien.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0911.png)\n",
    "\n",
    "_Figura 9-11. k-means no agrupa estas manchas elipsoidales correctamente_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ac99d",
   "metadata": {},
   "source": [
    "##### TIP\n",
    "\n",
    "Es importante escalar las características de entrada (ver Capítulo 2) antes de ejecutar k-means, o los grupos pueden estar muy estirados y k-means tendrán un mal rendimiento. \n",
    "Escalar las características no garantiza que todos los clústeres sean agradables y esféricos, pero generalmente ayuda a k-means.\n",
    "\n",
    "Ahora veamos algunas formas en las que podemos beneficiarnos de la agrupación. Usaremos k-means, pero siéntase libre de experimentar con otros algoritmos de agrupación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320e983",
   "metadata": {},
   "source": [
    "### Uso de agrupaciones para la segmentación de imágenes\n",
    "\n",
    "\n",
    "La __segmentación de imágenes__ es la tarea de dividir una imagen en múltiples segmentos. Hay varias variantes:\n",
    "\n",
    "\n",
    "- En la **segmentación del color**, los píxeles con un color similar se asignan al mismo segmento. Esto es suficiente en muchas aplicaciones. Por ejemplo, si desea analizar imágenes de satélite para medir la cantidad de área forestal total que hay en una región, la segmentación del color puede estar bien.\n",
    "\n",
    "\n",
    "- En la **segmentación semántica**, todos los píxeles que forman parte del mismo tipo de objeto se asignan al mismo segmento. Por ejemplo, en el sistema de visión de un coche autónomo, todos los píxeles que forman parte de la imagen de un peatón podrían asignarse al segmento de \"peatón\" (habría un segmento que contendría a todos los peatones).\n",
    "\n",
    "\n",
    "- En la **segmentación de instancias**, todos los píxeles que forman parte del mismo objeto individual se asignan al mismo segmento. En este caso, habría un segmento diferente para cada peatón.\n",
    "\n",
    "\n",
    "El estado del arte en la segmentación semántica o de instancias hoy en día se logra utilizando arquitecturas complejas basadas en redes neuronales convolucionales (véase el capítulo 14). En este capítulo nos vamos a centrar en la tarea de segmentación de color (mucho más simple), usando k-means.\n",
    "\n",
    "Comenzaremos importando el paquete Pillow (sucesor de la Python Imaging Library, PIL), que luego usaremos para cargar la imagen ladybug.png (ver la imagen de la parte superior izquierda en la Figura 9-12), suponiendo que se encuentre en `filepath`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "image = np.asarray(PIL.Image.open(filepath))\n",
    "image.shape\n",
    "\n",
    "#(533, 800, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4171e",
   "metadata": {},
   "source": [
    "La imagen se representa como una matriz 3D. El tamaño de la primera dimensión es la altura; la segunda es el ancho; y la tercera es el número de canales de color, en este caso rojo, verde y azul (RGB). En otras palabras, para cada píxel hay un vector 3D que contiene las intensidades de rojo, verde y azul como enteros de 8 bits sin signo entre 0 y 255. Algunas imágenes pueden tener menos canales (como las imágenes en escala de grises, que solo tienen uno), y algunas imágenes pueden tener más canales (como imágenes con un canal alfa adicional para transparencia, o imágenes de satélite, que a menudo contienen canales para frecuencias de luz adicionales (como el infrarrojo).\n",
    "\n",
    "El siguiente código reforma la matriz para obtener una larga lista de colores RGB y luego agrupa estos colores usando k-means con ocho grupos. Crea una matriz `segmented_img` que contiene el centro del grupo más cercano para cada píxel (es decir, el color medio del grupo de cada píxel) y, por último, reforma esta matriz a la forma de la imagen original. La tercera línea utiliza indexación NumPy avanzada; por ejemplo, si las primeras 10 etiquetas en `kmeans_.labels_` son iguales a 1, entonces los primeros 10 colores en `segmented_img` son iguales a `kmeans.cluster_centers_[1]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = image.reshape(-1, 3)\n",
    "kmeans = KMeans(n_clusters=8, random_state=42).fit(X)\n",
    "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "segmented_img = segmented_img.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98f187",
   "metadata": {},
   "source": [
    "Esto muestra la imagen que se muestra en la parte superior derecha de la Figura 9-12. Puedes experimentar con varios números de grupos, como se muestra en la figura. Cuando usas menos de ocho grupos, ten en cuenta que el llamativo color rojo de la mariquita no consigue un grupo propio: se fusiona con los colores del entorno. Esto se debe a que k-means prefiere grupos de tamaños similares. La mariquita es pequeña, mucho más pequeña que el resto de la imagen, por lo que a pesar de que su color es llamativo, k-means no le dedica un racimo.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0912.png)\n",
    "\n",
    "_Figura 9-12. Segmentación de imágenes usando k-means con varios números de grupos de colores_\n",
    "\n",
    "Eso no fue demasiado difícil, ¿verdad? Ahora echemos un vistazo a otra aplicación de agrupación.\n",
    "\n",
    "\n",
    "### Uso de la agrupación (clustering) para el aprendizaje semisupervisado\n",
    "\n",
    "\n",
    "Otro caso de uso para la agrupación es en el aprendizaje semisupervisado, cuando tenemos muchas instancias sin etiquetar y muy pocas instancias etiquetadas. En esta sección, utilizaremos el conjunto de datos de dígitos, que es un simple conjunto de datos similar al MNIST que contiene 1.797 imágenes en escala de grises de 8 × 8 que representan los dígitos del 0 al 9. Primero, carguemos y dividamos el conjunto de datos (ya está mezclado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32684426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "X_train, y_train = X_digits[:1400], y_digits[:1400]\n",
    "X_test, y_test = X_digits[1400:], y_digits[1400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ebbeb",
   "metadata": {},
   "source": [
    "Pretenderemos que solo tenemos etiquetas para 50 casos. \n",
    "\n",
    "Para obtener un rendimiento de referencia, entrenemos un modelo de regresión logística en estas 50 instancias etiquetadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ebab4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_labeled = 50\n",
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541d991",
   "metadata": {},
   "source": [
    "A continuación, podemos medir la precisión de este modelo en el conjunto de pruebas (tenga en cuenta que el conjunto de pruebas debe estar etiquetado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46108c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7481108312342569"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e344fa",
   "metadata": {},
   "source": [
    "La precisión del modelo es de solo el 74,8 %. Eso no es genial: de hecho, si intentas entrenar el modelo en el conjunto de entrenamiento completo, encontrarás que alcanzará una precisión de alrededor del 90,7 %. Veamos cómo podemos hacerlo mejor. En primer lugar, agrupe el conjunto de entrenamiento en 50 grupos. Luego, para cada grupo, encontraremos la imagen más cercana al centroide. Llamaremos a estas imágenes las imágenes representativas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2d562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
    "X_representative_digits = X_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7c9b9",
   "metadata": {},
   "source": [
    "La figura 9-13 muestra las 50 imágenes representativas.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0913.png)\n",
    "\n",
    "_Figura 9-13. Imágenes de cincuenta dígitos representativos (una por grupo)_\n",
    "\n",
    "Echemos un vistazo a cada imagen y etiquetémoslas manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8341e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits = np.array([1, 3, 6, 0, 7, 9, 2, ..., 5, 1, 9, 9, 3, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00e02d",
   "metadata": {},
   "source": [
    "Ahora tenemos un conjunto de datos con solo 50 instancias etiquetadas, pero en lugar de ser instancias aleatorias, cada una de ellas es una imagen representativa de su clúster. Veamos si el rendimiento es mejor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02453967",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_representative_digits, y_representative_digits)\n",
    "log_reg.score(X_test, y_test)\n",
    "\n",
    "#0.8488664987405542"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19419768",
   "metadata": {},
   "source": [
    "¡Guau! Pasamos de la precisión del 74,8% al 84,9%, aunque todavía solo estamos entrenando el modelo en 50 instancias. Dado que a menudo es costoso y doloroso etiquetar instancias, especialmente cuando tiene que ser hecho manualmente por expertos, es una buena idea etiquetar instancias representativas en lugar de solo instancias aleatorias.\n",
    "\n",
    "Pero tal vez podamos ir un paso más allá: ¿qué pasaría si propagamos las etiquetas a todas las demás instancias del mismo clúster? Esto se llama propagación de etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(X_train), dtype=np.int64)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4247bb",
   "metadata": {},
   "source": [
    "Ahora entrenémoslo de nuevo y veamos su rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04463409",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train_propagated)\n",
    "log_reg.score(X_test, y_test)\n",
    "\n",
    "#0.8942065491183879"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9d529",
   "metadata": {},
   "source": [
    "¡Tenemos otro aumento significativo en la precisión! Veamos si podemos hacerlo aún mejor ignorando el 1% de las instancias que están más alejadas de su centro de clúster: esto debería eliminar algunos valores atípicos. El siguiente código primero calcula la distancia desde cada instancia hasta su centro de clúster más cercano, luego para cada clúster establece las distancias más grandes del 1% en -1. Por último, crea un conjunto sin estas instancias marcadas con una distancia de -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_closest = 99\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "\n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee62172",
   "metadata": {},
   "source": [
    "Ahora entrenemos el modelo de nuevo en este conjunto de datos parcialmente propagado y veamos qué precisión tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "log_reg.score(X_test, y_test)\n",
    "\n",
    "#0.9093198992443325"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a767ab",
   "metadata": {},
   "source": [
    "¡Genial! Con solo 50 instancias etiquetadas (¡solo 5 ejemplos por clase en promedio!) obtuvimos una precisión del 90,9 %, que en realidad es ligeramente superior al rendimiento que obtuvimos en el conjunto de datos de dígitos completamente etiquetado (90,7%). Esto se debe en parte al hecho de que abandonamos algunos valores atípicos, y en parte a que las etiquetas propagadas son en realidad bastante buenas: su precisión es de alrededor del 97,5 %, como muestra el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> (y_train_partially_propagated == y_train[partially_propagated]).mean()\n",
    "\n",
    "#0.9755555555555555"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fe04c",
   "metadata": {},
   "source": [
    "##### TIP\n",
    "\n",
    "Scikit-Learn también ofrece dos clases que pueden propagar etiquetas automáticamente: `LabelSpreading` y `LabelPropagation` en el paquete `sklearn.semi_supervised`. \n",
    "\n",
    "Ambas clases construyen una matriz de similitud entre todas las instancias y propagan iterativamente etiquetas desde instancias etiquetadas a instancias similares sin etiquetar. También hay una clase muy diferente llamada `SelfTrainingClassifier` en el mismo paquete: le asignas un clasificador base (como `RandomForestClassifier`) y lo entrena en las instancias etiquetadas, luego lo usa para predecir etiquetas para las muestras sin etiquetar. Luego actualiza el conjunto de entrenamiento con las etiquetas en las que tiene más confianza y repite este proceso de entrenamiento y etiquetado hasta que ya no puede agregar etiquetas. Estas técnicas no son soluciones mágicas, pero ocasionalmente pueden darle un pequeño impulso a tu modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefddd8a",
   "metadata": {},
   "source": [
    "##### APRENDIZAJE ACTIVO\n",
    "\n",
    "Para continuar mejorando su modelo y su conjunto de entrenamiento, el siguiente paso podría ser hacer algunas rondas de aprendizaje activo, que es cuando un experto humano interactúa con el algoritmo de aprendizaje, proporcionando etiquetas para casos específicos cuando el algoritmo las solicita. Hay muchas estrategias diferentes para el aprendizaje activo, pero una de las más comunes se llama muestreo de incertidumbre. Así es como funciona:\n",
    "\n",
    "- 1. El modelo está entrenado en las instancias etiquetadas reunidas hasta ahora, y este modelo se utiliza para hacer predicciones sobre todas las instancias no etiquetadas.\n",
    "\n",
    "\n",
    "- 2. Los casos para los que el modelo es más incierto (es decir, donde su probabilidad estimada es más baja) se dan al experto para su etiquetado.\n",
    "\n",
    "\n",
    "- 3. Se itera este proceso hasta que la mejora del rendimiento deje de valer la pena el esfuerzo de etiquetado.\n",
    "\n",
    "\n",
    "Otras estrategias de aprendizaje activo incluyen el etiquetado de las instancias que darían lugar al mayor cambio de modelo o a la mayor caída en el error de validación del modelo, o las instancias en las que diferentes modelos no están de acuerdo (por ejemplo, un SVM y un bosque aleatorio)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db048b",
   "metadata": {},
   "source": [
    "Antes de pasar a los modelos de mezcla gaussiana, echemos un vistazo a DBSCAN, otro algoritmo de agrupación popular que ilustra un enfoque muy diferente basado en la estimación de la densidad local. Este enfoque permite al algoritmo identificar grupos de formas arbitrarias.\n",
    "\n",
    "\n",
    "\n",
    "## DBSCAN\n",
    "\n",
    "El algoritmo de agrupación espacial basada en la densidad de aplicaciones con ruido (DBSCAN) define los clústeres como regiones continuas de alta densidad. Así es como funciona:\n",
    "\n",
    "\n",
    "* Para cada instancia, el algoritmo cuenta cuántas instancias se encuentran dentro de una pequeña distancia ε (epsilon) de ella. Esta región se llama el barrio ε de la instancia.\n",
    "\n",
    "\n",
    "* Si una instancia tiene al menos instancias `min_samples` en su vecindario ε (incluida ella misma), entonces se considera una instancia central. En otras palabras, las instancias centrales son aquellas que están ubicadas en regiones densas.\n",
    "\n",
    "\n",
    "* Todas las instancias en el vecindario de una instancia central pertenecen al mismo clúster. Este vecindario puede incluir otras instancias centrales; por lo tanto, una larga secuencia de instancias centrales vecinas forma un solo clúster.\n",
    "\n",
    "\n",
    "* Cualquier instancia que no sea una instancia central y que no tenga una en su vecindario se considera una anomalía.\n",
    "\n",
    "\n",
    "Este algoritmo funciona bien si todos los grupos están bien separados por regiones de baja densidad. La clase DBSCAN en Scikit-Learn es tan sencilla de usar como cabría esperar. Probémoslo en el conjunto de datos de las lunas, presentado en el Capítulo 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2fe637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(eps=0.05)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.05)\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aee44c",
   "metadata": {},
   "source": [
    "Las etiquetas de todas las instancias ahora están disponibles en la variable de instancia `labels_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7576b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  1,  1,  0,  0,  0,  1,  0,  1,  1,  2,  0,  1,  0,  1,\n",
       "        1,  0,  2,  1,  0,  2,  0,  1,  1,  2,  2,  1,  0,  3,  1,  1,  1,\n",
       "        2,  0,  1,  1,  2,  3,  0,  1, -1,  1,  0,  3,  2, -1,  2,  0,  1,\n",
       "       -1, -1,  0,  1,  1,  0,  0,  0,  3,  1,  1,  2,  1,  3,  1, -1,  0,\n",
       "        3,  1,  2,  1,  2,  1,  1,  0,  0,  1,  0,  0,  1,  1,  0,  1,  0,\n",
       "        3,  1, -1,  1,  1,  1,  0,  1,  1,  1,  0,  3,  0,  1,  1,  2,  1,\n",
       "       -1,  0,  1,  1,  0,  1,  1,  1,  0,  0,  1,  0,  2,  0,  0,  2,  2,\n",
       "        1,  2,  2,  1,  1,  2,  2,  0,  1,  2,  0,  1, -1, -1,  2,  1,  1,\n",
       "        1,  2,  1, -1,  1,  1,  0,  1,  0,  3,  1,  0,  0,  1,  1, -1,  0,\n",
       "        0, -1,  1,  1,  1,  0,  1,  3,  3,  0,  0,  0,  1,  1,  2,  0,  2,\n",
       "        3,  0,  0,  1,  0,  1,  0,  2,  0,  2,  0,  0,  0,  1,  2,  0, -1,\n",
       "        0,  1,  0,  0,  2,  3,  1,  1,  0,  0,  1,  0,  3,  0,  1,  1,  0,\n",
       "       -1,  1,  0,  1,  0,  0,  3,  0,  1,  3,  1,  3,  0,  1, -1,  0,  1,\n",
       "        0,  1,  0,  1,  1,  0,  0, -1,  2,  1,  0, -1,  0,  1,  2, -1,  1,\n",
       "        1, -1,  0,  0,  2,  1, -1,  1,  1,  2,  0, -1,  0,  0,  2,  1,  0,\n",
       "        0,  1,  0,  1,  1,  1, -1,  1,  1, -1,  1,  0,  0,  2,  1,  3,  1,\n",
       "        1,  1,  0, -1, -1,  0,  1,  1,  1,  0,  1,  1,  0,  1,  0,  3,  1,\n",
       "        0,  2,  2,  1,  1,  0,  2,  0,  1,  0,  1,  1, -1,  1,  0,  1,  1,\n",
       "        1,  2,  2,  0,  1,  0, -1, -1,  0,  1,  1,  2,  1,  0,  1,  1, -1,\n",
       "        0,  0,  0,  1,  1,  0, -1,  0,  0,  2,  2,  2,  1,  0,  0,  0,  1,\n",
       "        2,  0,  1,  2,  0,  1,  0,  1,  1,  0,  2,  0,  0,  0,  2,  0,  1,\n",
       "        1, -1,  1,  0,  0,  2,  1,  2,  1,  2,  1,  0,  2,  0,  1,  1, -1,\n",
       "        1,  0,  1,  1,  1,  1,  1,  1,  1,  2,  0,  1,  0,  1,  1,  3,  0,\n",
       "        3,  0,  1,  1,  0,  1,  1,  1,  0,  1,  0, -1,  1, -1,  1,  1,  0,\n",
       "        0,  2,  2,  0,  2,  3,  1,  2,  1,  1,  0,  1,  0,  0,  1,  2, -1,\n",
       "        1,  0,  1,  1,  0,  0,  1,  1,  0,  1,  2,  0,  0,  0,  3,  2,  2,\n",
       "        1,  0,  1,  0,  0,  2,  1,  0,  0,  1,  3,  2,  1,  1,  0,  1,  1,\n",
       "        0,  1,  0,  0,  0,  1,  2,  0,  1,  0,  0,  1,  0,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  2,  0,  0,  0,  1,  3,  1, -1,  1,  1,  0,  1,  1,  1,\n",
       "       -1,  2,  3,  1,  0,  1,  0,  1,  2, -1,  1,  0,  1,  1,  1,  1,  0,\n",
       "        0,  0,  0,  1,  1,  1,  0,  1,  1,  1,  1,  1, -1,  1,  2,  0,  1,\n",
       "        1, -1,  2, -1, -1,  0,  1,  1,  1,  1,  1,  2,  1,  0,  1,  0,  1,\n",
       "        1,  1,  1,  1,  1,  1,  2,  1,  0,  0,  0,  2,  1,  1,  0,  3,  1,\n",
       "        1,  0,  0,  1,  2,  2,  0,  0,  0, -1,  1,  0,  2,  3,  0,  1,  1,\n",
       "        0,  1,  1,  1,  0,  1,  2, -1,  0,  0,  0,  1,  1,  0,  1,  3,  0,\n",
       "        1,  3,  2,  0,  0,  1,  1,  1,  1,  1,  1, -1,  0,  2,  2,  0,  1,\n",
       "        1,  3,  0,  2,  0,  0,  1,  1, -1,  3,  0, -1, -1,  0,  1,  1,  1,\n",
       "       -1,  0,  2,  0,  1,  3,  2,  0,  1,  1,  0,  0,  2,  1,  1, -1,  2,\n",
       "        0,  1,  0,  4,  0,  3,  2,  0,  2,  3,  3,  2,  1,  2,  3,  1,  3,\n",
       "        2,  1,  0,  1,  0,  0,  1,  1,  1,  0,  2,  1,  0,  3,  2,  2,  1,\n",
       "        2,  0,  0,  2,  1,  1,  1,  1,  0,  0,  1, -1,  0,  2,  0,  2,  1,\n",
       "        0,  1,  2,  0,  3,  1,  1, -1,  0,  2,  2,  0,  0, -1,  0,  0,  2,\n",
       "        0,  3,  1,  0,  1,  1, -1,  3,  1,  1, -1,  3,  1,  1,  2,  0,  3,\n",
       "        2,  0,  1,  1,  1,  0,  2,  2,  2,  0,  0,  2,  1,  1,  2,  1,  1,\n",
       "        0,  0,  2,  3, -1,  1,  0,  0,  1,  3,  0,  0,  2,  1,  0, -1,  1,\n",
       "        0,  1,  1,  1,  0,  0,  1,  1,  1,  1,  1,  1,  0,  0,  0,  3,  1,\n",
       "        3,  0,  2,  1,  0,  0,  0,  2,  1,  0,  1,  0,  3,  3,  0,  0,  0,\n",
       "        0, -1,  0, -1,  1, -1,  2,  1, -1,  1,  2,  1,  0,  1,  2, -1,  1,\n",
       "        0,  2,  1,  3,  0,  1,  0,  0,  1,  0,  2,  0,  3,  4,  2,  1, -1,\n",
       "        2,  1,  0,  0,  0,  1, -1,  0,  1,  2,  0,  2,  1,  0,  1,  1, -1,\n",
       "        2,  0,  0,  1,  1,  1,  2,  1, -1,  1,  3, -1,  1,  0,  1, -1,  1,\n",
       "        0,  2,  3,  1,  2,  0,  0,  1,  1, -1,  1,  2,  1,  2,  1,  3,  1,\n",
       "        3,  2,  2,  0,  2,  1,  2,  0,  1,  1, -1,  1,  0,  0,  1,  3, -1,\n",
       "        2,  1,  1,  1,  1,  1,  0,  0,  1,  0,  2,  1,  2,  1,  1,  1,  3,\n",
       "        0,  1,  3,  1,  0,  0,  0,  1,  1,  1, -1, -1,  0,  1,  1,  0,  2,\n",
       "        1,  3,  3,  2,  3,  0,  0,  0,  3,  1,  0,  2,  0,  1,  1,  1,  3,\n",
       "        1,  3,  1,  0,  1,  0,  1,  1,  3,  3,  2,  1,  0,  1,  2,  1,  1,\n",
       "        0,  2,  1,  3, -1,  0,  0,  1,  3,  1,  2,  0,  0,  1,  2,  0,  1,\n",
       "        1,  1,  1,  0,  2,  0, -1, -1,  2,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7184b",
   "metadata": {},
   "source": [
    "Tenga en cuenta que algunas instancias tienen un índice de clúster igual a -1, lo que significa que el algoritmo las considera anomalías. Los índices de las instancias principales están disponibles en la variable de instancia `core_sample_indices_`, y las instancias centrales en sí están disponibles en la variable de instancia `components_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "255e8ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  26,  27,\n",
       "        28,  29,  30,  31,  32,  33,  35,  36,  38,  39,  40,  41,  43,\n",
       "        44,  45,  46,  48,  49,  50,  53,  54,  55,  56,  57,  58,  59,\n",
       "        60,  61,  62,  63,  64,  65,  67,  69,  70,  71,  72,  73,  74,\n",
       "        75,  77,  78,  79,  80,  81,  82,  83,  84,  85,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  98,  99, 100, 101, 103, 104, 105,\n",
       "       106, 107, 108, 109, 110, 112, 114, 115, 117, 118, 119, 122, 123,\n",
       "       124, 127, 128, 129, 130, 133, 134, 135, 136, 137, 138, 140, 141,\n",
       "       142, 144, 146, 147, 148, 149, 150, 152, 153, 156, 157, 158, 159,\n",
       "       160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
       "       173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
       "       189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202,\n",
       "       206, 207, 209, 210, 211, 212, 213, 215, 216, 217, 219, 220, 221,\n",
       "       222, 225, 226, 227, 229, 230, 231, 233, 234, 235, 237, 238, 240,\n",
       "       241, 242, 243, 245, 246, 247, 250, 251, 252, 253, 254, 255, 256,\n",
       "       258, 259, 260, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       274, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 289, 291,\n",
       "       292, 293, 294, 295, 296, 297, 298, 299, 300, 302, 303, 304, 305,\n",
       "       306, 307, 308, 309, 310, 311, 317, 318, 319, 320, 321, 323, 324,\n",
       "       326, 327, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341,\n",
       "       342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354,\n",
       "       355, 356, 357, 359, 361, 362, 363, 364, 365, 366, 368, 369, 370,\n",
       "       371, 372, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385,\n",
       "       386, 388, 389, 391, 393, 394, 395, 396, 398, 399, 400, 401, 403,\n",
       "       405, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
       "       421, 422, 423, 425, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n",
       "       436, 437, 438, 439, 440, 442, 443, 444, 445, 446, 447, 448, 450,\n",
       "       452, 453, 454, 456, 457, 458, 460, 462, 463, 464, 466, 467, 468,\n",
       "       469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 482, 483,\n",
       "       487, 488, 489, 490, 491, 492, 494, 495, 496, 497, 498, 499, 500,\n",
       "       501, 503, 504, 505, 508, 509, 510, 512, 513, 514, 516, 517, 520,\n",
       "       521, 523, 524, 525, 526, 527, 529, 532, 533, 535, 536, 537, 538,\n",
       "       539, 540, 541, 542, 543, 544, 546, 547, 548, 549, 550, 551, 552,\n",
       "       553, 554, 555, 557, 558, 559, 560, 561, 563, 564, 565, 566, 567,\n",
       "       568, 569, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 582,\n",
       "       584, 586, 587, 590, 591, 593, 594, 595, 596, 597, 598, 600, 601,\n",
       "       602, 604, 605, 607, 608, 609, 610, 611, 613, 614, 615, 616, 617,\n",
       "       618, 619, 621, 622, 625, 626, 627, 628, 630, 631, 632, 633, 634,\n",
       "       635, 636, 637, 638, 639, 641, 642, 643, 645, 646, 647, 648, 649,\n",
       "       651, 653, 654, 655, 657, 658, 659, 660, 661, 662, 663, 664, 665,\n",
       "       666, 667, 668, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679,\n",
       "       680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 692, 693,\n",
       "       694, 695, 696, 697, 698, 700, 701, 702, 703, 706, 707, 708, 709,\n",
       "       711, 712, 713, 714, 715, 717, 719, 721, 722, 723, 725, 726, 727,\n",
       "       728, 729, 732, 733, 734, 735, 736, 737, 738, 740, 741, 742, 743,\n",
       "       744, 745, 746, 747, 748, 750, 751, 753, 754, 756, 758, 759, 760,\n",
       "       761, 762, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
       "       776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
       "       789, 791, 792, 793, 794, 795, 796, 797, 798, 799, 801, 803, 805,\n",
       "       806, 808, 809, 810, 811, 812, 813, 815, 816, 817, 818, 820, 821,\n",
       "       822, 823, 824, 825, 827, 828, 830, 831, 833, 834, 835, 836, 837,\n",
       "       838, 840, 842, 845, 846, 847, 848, 850, 851, 852, 853, 854, 855,\n",
       "       856, 859, 860, 862, 863, 864, 867, 868, 869, 870, 871, 872, 873,\n",
       "       874, 875, 877, 878, 879, 880, 881, 882, 883, 884, 887, 888, 889,\n",
       "       890, 891, 892, 893, 895, 897, 898, 899, 901, 902, 903, 904, 905,\n",
       "       906, 907, 908, 909, 911, 912, 913, 914, 915, 916, 917, 918, 919,\n",
       "       922, 924, 925, 926, 930, 931, 932, 933, 934, 935, 936, 937, 938,\n",
       "       939, 940, 941, 942, 943, 944, 945, 947, 948, 949, 950, 952, 953,\n",
       "       954, 955, 956, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967,\n",
       "       968, 969, 970, 971, 972, 974, 976, 977, 978, 979, 980, 981, 982,\n",
       "       984, 985, 986, 987, 988, 989, 990, 991, 995, 996, 997, 998])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.core_sample_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee450cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48315214, -0.39288966],\n",
       "       [-0.00438427,  0.33551151],\n",
       "       [ 0.04152122,  0.14878884],\n",
       "       ...,\n",
       "       [ 0.90601996,  0.47667043],\n",
       "       [-1.03131559,  0.07990096],\n",
       "       [-0.17416662,  1.04049186]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83590311",
   "metadata": {},
   "source": [
    "Esta agrupación se representa en el gráfico de la izquierda de la Figura 9-14. Como puede ver, identificó bastantes anomalías, además de siete grupos diferentes. ¡Que decepcionante! Afortunadamente, si ampliamos la vecindad de cada instancia aumentando los eps a 0,2, obtenemos la agrupación de la derecha, que parece perfecta. Sigamos con este modelo.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0914.png)\n",
    "\n",
    "_Figura 9-14. Agrupación DBSCAN utilizando dos radios de vecindario diferentes_\n",
    "\n",
    "Sorprendentemente, la clase `DBSCAN` no tiene un método `predict()`, aunque tiene un método `fit_predict()`. \n",
    "\n",
    "En otras palabras, no puede predecir a qué clúster pertenece una nueva instancia. Esta decisión se tomó porque diferentes algoritmos de clasificación pueden ser mejores para diferentes tareas, por lo que los autores decidieron dejar que el usuario elija cuál usar. Además, no es difícil de implementar. Por ejemplo, entrenemos un `KNeighborsClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f55456c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6676be",
   "metadata": {},
   "source": [
    "Ahora, dadas algunas instancias nuevas, podemos predecir a qué grupos es más probable que pertenezcan e incluso estimar una probabilidad para cada grupo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cdaf150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
    "knn.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "324c51ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16, 0.84, 0.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.24, 0.  , 0.  , 0.76, 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3f963",
   "metadata": {},
   "source": [
    "Tenga en cuenta que solo entrenamos al clasificador en las instancias principales, pero también podríamos haber optado por entrenarlo en todas las instancias, o en todas, pero las anomalías: esta elección depende de la tarea final.\n",
    "\n",
    "El límite de decisión se representa en la Figura 9-15 (las cruces representan las cuatro instancias en `X_new`). Tenga en cuenta que, dado que no hay ninguna anomalía en el conjunto de entrenamiento, el clasificador siempre elige un grupo, incluso cuando ese grupo está lejos. Es bastante sencillo introducir una distancia máxima, en cuyo caso las dos instancias que están lejos de ambos grupos se clasifican como anomalías. Para hacer esto, use el método `kneighbors()` el `KNeighborsClassifier`. Dado un conjunto de instancias, devuelve las distancias y los índices de los k vecinos más cercanos en el conjunto de entrenamiento (dos matrices, cada una con k columnas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "481db4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  3, -1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n",
    "y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n",
    "y_pred[y_dist > 0.2] = -1\n",
    "y_pred.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c6345",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0915.png)\n",
    "\n",
    "_Figura 9-15. Límite de decisión entre dos grupos_\n",
    "\n",
    "En resumen, DBSCAN es un algoritmo muy simple pero poderoso capaz de identificar cualquier número de grupos de cualquier forma. Es resistente a los valores atípicos y tiene solo dos hiperparámetros (`eps` y `min_samples`). \n",
    "\n",
    "Sin embargo, si la densidad varía significativamente entre los grupos, o si no hay una región de densidad suficientemente baja alrededor de algunos grupos, DBSCAN puede tener dificultades para capturar todos los grupos correctamente. \n",
    "\n",
    "Además, su complejidad computacional es aproximadamente O(m2n), por lo que no se adapta bien a grandes conjuntos de datos.\n",
    "\n",
    "##### TIP\n",
    "\n",
    "También es posible que desee probar el DBSCAN jerárquico (HDBSCAN), que se implementa en el proyecto `scikit-learn-contrib`, ya que generalmente es mejor que DBSCAN para encontrar grupos de densidades variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c2d38",
   "metadata": {},
   "source": [
    "## Otros algoritmos de agrupación\n",
    "\n",
    "Scikit-Learn implementa varios algoritmos de agrupación más a los que deberías echar un vistazo. No puedo cubrirlos todos en detalle aquí, pero aquí hay una breve descripción general:\n",
    "\n",
    "- **Agrupación aglomerativa**\n",
    "\n",
    "    Se construye una jerarquía de grupos de abajo hacia arriba. Piensa en muchas burbujas diminutas flotando en el agua y uniéndose gradualmente entre sí hasta que haya un gran grupo de burbujas. Del mismo modo, en cada iteración, la agrupación aglomerativa conecta el par de clústeres más cercano (comenzando con instancias individuales). Si dibujaras un árbol con una rama por cada par de racimos que se fusionaron, obtendrás un árbol binario de racimos, donde las hojas son las instancias individuales. Este enfoque puede capturar grupos de varias formas; también produce un árbol de grupos flexible e informativo en lugar de obligarlo a elegir una escala de grupos en particular, y se puede utilizar con cualquier distancia de pares. Puede escalar bien a un gran número de instancias si proporciona una matriz de conectividad, que es una matriz escasa de m × m que indica qué pares de instancias son vecinas (por ejemplo, devuelta porsklearnsklearn.neighbors.kneighbors_graph()). Sin una matriz de conectividad, el algoritmo no se escala bien a grandes conjuntos de datos.\n",
    "\n",
    "\n",
    "- **abedul**\n",
    "\n",
    "    El algoritmo de reducción y agrupación iterativa equilibrada utilizando jerarquías (BIRCH) fue diseñado específicamente para conjuntos de datos muy grandes, y puede ser más rápido que los k-medios por lotes, con resultados similares, siempre y cuando el número de características no sea demasiado grande (<20). Durante el entrenamiento, construye una estructura de árbol que contiene la información suficiente para asignar rápidamente cada nueva instancia a un clúster, sin tener que almacenar todas las instancias en el árbol: este enfoque le permite usar una memoria limitada mientras maneja enormes conjuntos de datos.\n",
    "\n",
    "\n",
    "- **Cambio medio**\n",
    "\n",
    "    Este algoritmo comienza colocando un círculo centrado en cada instancia; luego, para cada círculo, calcula la media de todas las instancias ubicadas dentro de él, y desplaza el círculo para que esté centrado en la media. A continuación, itera este paso de cambio de media hasta que todos los círculos dejen de moverse (es decir, hasta que cada uno de ellos se centre en la media de las instancias que contiene). El cambio medio desplaza los círculos en la dirección de mayor densidad, hasta que cada uno de ellos ha encontrado un máximo de densidad local. Finalmente, todas las instancias cuyos círculos se han asentado en el mismo lugar (o lo suficientemente cerca) se asignan al mismo grupo. El desplazamiento medio tiene algunas de las mismas características que DBSCAN, como cómo puede encontrar cualquier número de grupos de cualquier forma, tiene muy pocos hiperparámetros (solo uno: el radio de los círculos, llamado ancho de banda), y se basa en la estimación de la densidad local. Pero a diferencia de DBSCAN, el cambio medio tiende a cortar los grupos en pedazos cuando tienen variaciones de densidad interna. Desafortunadamente, su complejidad computacional es O(m2n), por lo que no es adecuado para grandes conjuntos de datos.\n",
    "\n",
    "\n",
    "- **Propagación de afinidad**\n",
    "\n",
    "    En este algoritmo, las instancias intercambian mensajes entre sí repetidamente hasta que cada instancia ha elegido otra instancia (o a sí misma) para representarla. Estas instancias elegidas se llaman ejemplares. Cada ejemplo y todas las instancias que lo eligieron forman un grupo. En la política de la vida real, normalmente quieres votar por un candidato cuyas opiniones sean similares a las tuyas, pero también quieres que ganen las elecciones, por lo que puedes elegir un candidato con el que no estés totalmente de acuerdo, pero que sea más popular. Normalmente evalúas la popularidad a través de encuestas. La propagación de la afinidad funciona de manera similar, y tiende a elegir ejemplares ubicados cerca del centro de los grupos, similares a los k-medios. Pero a diferencia de los k-medios, no tienes que elegir una serie de grupos con anticipación: se determina durante el entrenamiento. Además, la propagación de la afinidad puede manejar bien con grupos de diferentes tamaños. Lamentablemente, este algoritmo tiene una complejidad computacional de O(m2), por lo que no es adecuado para grandes conjuntos de datos.\n",
    "\n",
    "\n",
    "- **Agrupación espectral**\n",
    "\n",
    "    Este algoritmo toma una matriz de similitud entre las instancias y crea una incrustación de baja dimensión a partir de ella (es decir, reduce la dimensionalidad de la matriz), luego utiliza otro algoritmo de agrupación en este espacio de baja dimensión (la implementación de Scikit-Learn utiliza k-means). La agrupación espectral puede capturar estructuras de clústeres complejas, y también se puede utilizar para cortar gráficos (por ejemplo, para identificar grupos de amigos en una red social). No se escala bien a un gran número de instancias, y no se comporta bien cuando los grupos tienen tamaños muy diferentes.\n",
    "\n",
    "\n",
    "\n",
    "Ahora vamos a sumergirnos en los modelos de mezcla gaussiana, que se pueden utilizar para la estimación de la densidad, la agrupación y la detección de anomalías."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d51e3",
   "metadata": {},
   "source": [
    "## Mezclas gaussianas\n",
    "\n",
    "Un modelo de mezcla gaussiana (GMM) es un modelo probabilístico que asume que las instancias se generaron a partir de una mezcla de varias distribuciones gaussiana cuyos parámetros son desconocidos. Todas las instancias generadas a partir de una sola distribución gaussiana forman un clúster que normalmente se parece a un elipsoide. Cada grupo puede tener una forma, tamaño, densidad y orientación elipsoidales diferentes, al igual que en la Figura 9-11. Cuando observas una instancia, sabes que se generó a partir de una de las distribuciones gaussiana, pero no se te dice cuál, y no sabes cuáles son los parámetros de estas distribuciones.\n",
    "\n",
    "Hay varias variantes de GMM. En la variante más simple, implementada en la clase `GaussianMixture`, debes saber de antemano el número k de las distribuciones gaussianas. Se supone que el conjunto de datos **X** se ha generado a través del siguiente proceso probabilístico:\n",
    "\n",
    "\n",
    "* Para cada instancia, se elige un grupo al azar entre k grupos. La probabilidad de elegir el jth cluster es el peso del cluster φ(j).⁠6 El índice del cluster elegido para la iésima instancia se señala z(i).\n",
    "\n",
    "* Si la iésima instancia se asignó al jth cluster (es decir, z(i) = j), entonces la locationx(i) de esta instancia se muestrea al azar de la distribución gaussiana con la media μ(j) y la matriz de covarianza Σ(j). Esto se nota x(i) ~ N(μ(j), Σ(j)).\n",
    "\n",
    "Entonces, ¿qué se puede hacer con un modelo así? Bueno, dado el conjunto de datos X, normalmente querrás comenzar estimando los pesos ϕ y todos los parámetros de distribución μ(1) a μ(k) y Σ(1) a Σ(k). La clase `GaussianMixture` de Scikit-Learn hace que esto sea súper fácil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "295780c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(n_components=3, n_init=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm = GaussianMixture(n_components=3, n_init=10)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df4887",
   "metadata": {},
   "source": [
    "Echemos un vistazo a los parámetros que el algoritmo estimó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be3514c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59205211, 0.20454428, 0.20340361])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88cfc2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50289177,  0.24871033],\n",
       "       [-0.74396241,  0.56177925],\n",
       "       [ 1.74886999, -0.04709797]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c55c16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.16627406, -0.10132087],\n",
       "        [-0.10132087,  0.28836932]],\n",
       "\n",
       "       [[ 0.05510354,  0.06242454],\n",
       "        [ 0.06242454,  0.08638241]],\n",
       "\n",
       "       [[ 0.05295016,  0.06174631],\n",
       "        [ 0.06174631,  0.08741031]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de878cf9",
   "metadata": {},
   "source": [
    "¡Genial, funcionó bien! De hecho, dos de los tres clústeres se generaron con 500 instancias cada uno, mientras que el tercer clúster solo contiene 250 instancias. Así que los verdaderos pesos del clúster son 0,4, 0,4 y 0,2, respectivamente, y eso es más o menos lo que encontró el algoritmo. \n",
    "\n",
    "Del mismo modo, las verdaderas medias y las matrices de covarianza son bastante cercanas a las encontradas por el algoritmo. ¿Pero cómo? Esta clase se basa en el algoritmo de maximización de expectativas (EM), que tiene muchas similitudes con el algoritmo k-means: también inicializa los parámetros del clúster al azar, luego repite dos pasos hasta la convergencia, primero asignando instancias a los clústeres (esto se llama el paso de expectativa) y luego actualizando los clústeres (esto se llama el paso de maximización). \n",
    "\n",
    "Suena familiar, ¿verdad? En el contexto de la agrupación, se puede pensar en EM como una generalización de k-medios que no solo encuentra los centros de la agrupación (μ(1) a μ(k)), sino también su tamaño, forma y orientación (Σ(1) a Σ(k)), así como sus pesos relativos (φ(1) a φ(k)). Sin embargo, a diferencia de k-means, EM utiliza asignaciones de clústeres suaves, no asignaciones difíciles. \n",
    "\n",
    "Para cada instancia, durante el paso de expectativa, el algoritmo estima la probabilidad de que pertenezca a cada clúster (basado en los parámetros actuales del clúster). \n",
    "\n",
    "Luego, durante el paso de maximización, cada clúster se actualiza utilizando todas las instancias del conjunto de datos, con cada instancia ponderada por la probabilidad estimada de que pertenezca a ese clúster. \n",
    "\n",
    "Estas probabilidades se denominan responsabilidades de los grupos para las instancias. \n",
    "\n",
    "Durante el paso de maximización, la actualización de cada clúster se verá afectada principalmente por las instancias de las que es más responsable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71276ac3",
   "metadata": {},
   "source": [
    "##### ADVERTENCIA\n",
    "\n",
    "Desafortunadamente, al igual que k-means, EM puede terminar convergiendo en soluciones deficientes, por lo que es necesario ejecutarlo varias veces y conservar solo la mejor solución. Es por eso que configuramos n_init en 10. Tenga cuidado: de forma predeterminada, `n_init` está configurado en `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e98aaa",
   "metadata": {},
   "source": [
    "Puedes comprobar si el algoritmo convergió o no y cuántas iteraciones tomó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5af01138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24bcaf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcb8c8",
   "metadata": {},
   "source": [
    "Ahora que tiene una estimación de la ubicación, el tamaño, la forma, la orientación y el peso relativo de cada grupo, el modelo puede asignar fácilmente cada instancia al grupo más probable (agrupación dura) o estimar la probabilidad de que pertenezca a un grupo en particular. (agrupación suave). Simplemente use el método `predict()` para agrupación dura, o el método `predict_proba()` para agrupación suave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "297c0fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 2,\n",
       "       2, 0, 0, 0, 2, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 0, 1, 1,\n",
       "       0, 0, 2, 0, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1,\n",
       "       2, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 0, 0, 2, 0, 0, 2,\n",
       "       0, 2, 1, 0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0,\n",
       "       2, 2, 0, 2, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0,\n",
       "       2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0,\n",
       "       2, 0, 2, 1, 0, 2, 0, 0, 0, 2, 2, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0,\n",
       "       0, 0, 0, 0, 2, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2,\n",
       "       2, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1,\n",
       "       0, 0, 2, 2, 2, 1, 0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 1, 0, 2, 0,\n",
       "       0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 1, 0,\n",
       "       0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0,\n",
       "       2, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 2, 1, 2, 2, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 1, 0, 2, 2, 0, 0, 0,\n",
       "       0, 0, 2, 0, 1, 2, 1, 1, 1, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 2, 1, 2, 0, 1, 1,\n",
       "       0, 2, 2, 1, 0, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       2, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 2, 2, 0, 2, 0, 2, 0,\n",
       "       0, 2, 0, 0, 1, 1, 2, 0, 1, 1, 0, 0, 2, 0, 0, 0, 2, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 1, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1,\n",
       "       0, 2, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2,\n",
       "       0, 1, 0, 2, 0, 0, 1, 2, 0, 1, 1, 0, 0, 2, 1, 0, 0, 2, 2, 0, 2, 0,\n",
       "       0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 0, 0, 1, 1,\n",
       "       0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 2, 0, 0, 2, 0, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 1, 1,\n",
       "       0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0,\n",
       "       1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 1, 2, 1, 0, 1, 2, 1, 0, 0, 2,\n",
       "       0, 1, 0, 2, 1, 0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 2, 0, 1, 2, 1, 0,\n",
       "       2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0,\n",
       "       1, 1, 0, 2, 0, 0, 1, 0, 1, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 1, 2, 0,\n",
       "       2, 1, 0, 0, 0, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 2, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1,\n",
       "       0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 0, 1, 1, 0, 0, 0,\n",
       "       2, 0, 0, 2, 2, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5ff7b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       ...,\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.788, 0.212, 0.   ],\n",
       "       [1.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.predict_proba(X).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca37402",
   "metadata": {},
   "source": [
    "Un modelo de mezcla gaussiana es un modelo generativo, lo que significa que puede muestrear nuevas instancias de él (tenga en cuenta que están ordenadas por índice de clúster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72abce16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51305816,  0.7969581 ],\n",
       "       [ 0.4976855 ,  0.03095133],\n",
       "       [-1.33146798,  0.08344344],\n",
       "       [-0.78931721,  0.47366145],\n",
       "       [-0.56167947,  0.82900384],\n",
       "       [ 1.64973695, -0.1597269 ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new, y_new = gm.sample(6)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a36b38d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee365a",
   "metadata": {},
   "source": [
    "También es posible estimar la densidad del modelo en cualquier ubicación. Esto se logra utilizando el método `score_samples()`: para cada instancia que se da, este método estima el registro de la función de densidad de probabilidad (PDF) en esa ubicación. Cuanto mayor sea la puntuación, mayor será la densidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a30ac3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.67, -1.61, -1.68, -1.55, -0.16, -1.61, -0.92, -1.79, -0.72,\n",
       "       -2.21, -0.7 , -1.54, -0.5 , -1.87, -1.89, -2.16, -2.03, -0.22,\n",
       "       -1.45, -0.37, -0.69, -0.45, -0.36, -1.36, -1.54, -1.46,  0.08,\n",
       "       -0.07, -0.14, -1.99, -1.64, -0.36, -0.33, -0.03, -1.59, -0.72,\n",
       "       -0.29, -1.47, -0.29, -1.62, -1.37, -1.65, -2.87, -0.76, -1.82,\n",
       "       -1.49, -0.17, -1.62, -0.34, -1.28,  0.04, -1.73, -1.66, -1.59,\n",
       "       -0.08, -1.71, -1.79, -2.23, -1.94, -1.68, -1.53, -1.88, -0.26,\n",
       "       -0.34, -1.31, -0.28, -0.97, -1.44, -1.96, -1.67,  0.11, -1.87,\n",
       "       -2.65, -0.41, -1.92, -1.61, -1.33, -0.13, -1.71, -1.87, -1.61,\n",
       "       -1.77, -1.73, -0.34, -2.22, -1.58, -1.25, -2.42, -2.04, -1.02,\n",
       "       -1.62, -1.78, -0.12, -0.2 , -1.21, -1.43, -1.31, -1.39, -0.51,\n",
       "       -1.69, -0.04, -1.44, -0.29, -2.02, -1.82, -0.95, -1.36, -0.77,\n",
       "       -1.65, -0.15, -1.97, -1.66, -1.56, -2.29,  0.07, -1.48, -2.2 ,\n",
       "        0.1 , -0.86, -1.42, -0.87, -1.04, -0.08, -2.12, -0.54, -1.01,\n",
       "       -1.79, -1.57, -0.08, -1.55, -2.12, -0.66, -1.81, -0.26, -2.01,\n",
       "       -1.66, -1.58, -0.44, -0.35, -1.33, -0.14, -0.88, -1.67, -1.37,\n",
       "       -1.32, -1.89, -1.63, -1.73, -2.19, -1.53, -1.72, -1.29, -1.81,\n",
       "       -1.65, -3.44, -3.57, -1.99, -1.87, -1.37,  0.01, -1.54, -1.79,\n",
       "       -1.69, -0.66, -1.53, -1.81, -1.9 ,  0.11, -1.59, -0.93, -1.45,\n",
       "       -1.65, -1.57, -0.34, -2.14, -1.58, -0.6 , -0.24, -1.59, -0.28,\n",
       "       -1.72, -1.56, -1.1 , -0.6 , -0.99, -2.16, -1.4 , -2.13, -1.69,\n",
       "       -1.83, -1.82, -0.27, -1.64, -0.28, -0.51, -1.77, -1.48, -1.65,\n",
       "       -0.57, -1.28, -1.47, -0.69, -0.41, -1.29, -1.64, -1.2 , -1.78,\n",
       "       -2.18, -1.86, -1.67, -1.55, -0.57, -0.66, -1.44, -0.12, -1.58,\n",
       "       -1.79, -1.52, -1.92, -1.54, -1.82, -1.45, -0.37, -2.  , -2.6 ,\n",
       "       -1.62, -1.62, -1.74, -1.82,  0.01, -1.47, -1.48, -1.17, -1.49,\n",
       "       -0.27, -0.48, -1.97, -2.06, -0.08, -1.1 , -1.67, -1.75, -0.13,\n",
       "       -1.49, -0.75, -0.29, -1.48, -0.76, -1.46, -1.77, -2.25, -0.43,\n",
       "       -0.17, -1.66, -0.55, -1.5 , -0.52, -1.21, -1.81, -0.36, -2.17,\n",
       "       -1.28, -0.28, -0.24, -2.05, -1.  , -1.69, -1.71,  0.08, -1.66,\n",
       "       -1.67, -2.05, -0.72, -1.5 , -1.77, -1.49, -1.95, -1.64, -1.65,\n",
       "       -1.4 , -1.58, -1.82, -1.74, -0.85, -0.87, -2.09, -1.56, -1.69,\n",
       "       -2.09, -1.57, -0.9 , -0.26, -1.37, -1.53, -1.64, -0.12, -2.2 ,\n",
       "       -2.02, -1.58, -1.55, -1.51, -1.33, -1.15, -1.45, -1.87, -0.66,\n",
       "       -1.68, -0.3 , -2.1 , -1.54, -1.05, -1.74, -1.83, -1.44, -0.59,\n",
       "       -1.95, -1.94, -0.1 , -2.14, -1.59, -1.83, -0.29, -2.23, -1.52,\n",
       "       -2.22, -0.69, -0.05, -1.64, -1.24, -1.14, -1.56, -1.61, -0.53,\n",
       "       -0.15, -0.7 , -0.38, -1.86, -1.64, -1.35, -0.81, -0.13, -1.51,\n",
       "       -1.12, -0.95, -1.61, -1.57, -1.44, -1.73, -0.35, -1.71,  0.07,\n",
       "       -1.66, -1.43, -1.47, -0.24, -1.62, -1.49, -1.72, -1.9 , -1.69,\n",
       "       -1.47, -1.47,  0.03, -1.62, -0.33, -1.59, -0.73, -1.92, -1.79,\n",
       "       -0.78, -1.4 , -1.47, -0.13, -1.7 , -1.5 , -1.6 , -1.95, -1.73,\n",
       "       -0.24, -1.6 , -1.81, -1.97, -2.21, -1.01, -2.06, -0.98, -1.53,\n",
       "       -0.74, -1.46, -1.6 , -1.02, -1.59, -1.79, -1.75, -0.39, -1.42,\n",
       "       -1.53, -1.83, -2.03, -1.51, -1.78, -1.54, -0.37, -0.84, -0.87,\n",
       "       -1.95, -1.24, -2.17, -1.27, -1.37, -0.08, -1.6 , -1.65, -1.68,\n",
       "       -1.47, -0.02, -0.03, -1.81, -1.3 , -1.9 , -2.09, -0.35, -1.59,\n",
       "       -0.09, -1.19, -0.4 , -2.18, -1.5 , -2.55, -1.6 , -1.12, -0.12,\n",
       "       -1.89, -1.6 , -0.23, -0.76, -1.33, -1.64, -1.55, -1.5 , -0.21,\n",
       "       -0.05, -2.2 , -1.77, -1.99, -1.62, -1.59,  0.11, -1.51, -1.47,\n",
       "       -1.56, -2.33, -1.48, -0.49, -1.42, -1.6 , -1.21, -0.48, -0.46,\n",
       "       -1.87, -1.31, -1.82, -1.6 , -1.76, -0.81,  0.08, -1.94, -0.29,\n",
       "       -1.87, -1.54, -1.4 , -2.11, -0.25, -0.62, -2.22, -1.61, -2.09,\n",
       "       -0.5 , -1.95, -0.9 , -0.3 , -1.95, -1.46, -2.01, -1.24, -1.37,\n",
       "       -4.03, -1.61, -0.4 , -1.84, -0.02, -0.26, -1.17, -2.18,  0.01,\n",
       "       -1.44, -2.2 , -0.69, -1.68, -1.71, -1.66, -0.65, -1.89, -2.15,\n",
       "       -1.49, -1.11, -1.33, -1.77, -1.63, -1.59, -2.21, -1.95, -1.77,\n",
       "        0.03, -0.67, -1.36, -1.54, -1.52, -1.88, -0.58, -1.51, -0.44,\n",
       "       -0.37, -0.15, -0.57, -1.75, -0.31, -0.57, -1.83, -1.56, -0.43,\n",
       "        0.06, -1.58, -1.68, -2.21, -1.43, -0.39, -0.47, -1.48, -0.27,\n",
       "       -1.53, -2.05, -1.57, -1.61, -1.65, -1.33, -0.25, -1.72, -0.46,\n",
       "       -1.44,  0.02, -1.69, -1.66, -1.58, -1.86, -0.09, -1.5 , -0.9 ,\n",
       "       -1.74, -1.6 , -1.97, -2.24, -1.41, -2.  , -0.53, -0.3 , -0.18,\n",
       "       -1.78, -0.88, -1.99, -4.46, -2.15, -1.6 , -0.08, -1.28, -1.49,\n",
       "       -0.44, -0.36, -1.17, -1.99, -0.38, -1.45, -1.98, -1.37, -0.02,\n",
       "       -1.84, -1.54, -1.68, -1.62,  0.09, -0.83, -1.43, -1.36, -1.65,\n",
       "       -1.57, -1.8 , -1.66, -0.48, -1.48, -1.8 , -1.66, -2.06, -1.7 ,\n",
       "       -1.94, -2.01, -1.73, -0.29, -1.74, -0.72, -0.38, -1.75, -1.59,\n",
       "       -2.1 , -1.53, -1.61, -1.29, -1.28, -1.64, -1.53, -1.66, -1.22,\n",
       "       -1.6 , -1.47, -2.18, -1.91, -1.34, -1.79, -1.25, -1.84, -2.99,\n",
       "       -1.77, -0.89, -1.94, -1.82, -1.4 , -0.44, -1.98, -0.43, -1.69,\n",
       "       -1.2 , -1.41,  0.01, -2.22, -0.87, -2.14, -1.22, -1.74, -1.71,\n",
       "       -1.45, -1.3 , -1.64, -1.49,  0.05, -1.67, -0.1 , -1.41, -1.63,\n",
       "       -2.13, -1.68, -2.07, -1.58, -2.16, -1.68, -0.53, -2.2 , -2.11,\n",
       "       -0.55, -0.85, -1.47, -0.81, -0.47, -1.46, -1.44, -1.99, -1.86,\n",
       "       -1.56, -1.59, -0.06, -0.8 , -1.87, -0.62, -1.66, -1.67, -1.03,\n",
       "       -1.57, -0.1 , -2.1 , -0.36, -1.76, -1.5 , -2.22, -2.18, -1.73,\n",
       "       -1.07, -1.84, -0.27, -0.34, -0.49, -1.57, -0.79, -1.5 , -1.62,\n",
       "       -0.02, -1.73, -1.42, -1.51, -0.03, -0.04, -1.66, -1.52, -1.81,\n",
       "       -1.53, -1.4 ,  0.05, -0.62, -1.31, -0.79, -2.17, -1.99, -1.39,\n",
       "       -1.7 , -1.44, -0.26, -0.24, -1.25, -1.5 , -1.14, -1.51, -0.79,\n",
       "       -1.68, -1.67, -1.02, -1.9 , -1.77, -1.75, -1.44, -1.36, -0.53,\n",
       "       -1.92, -2.85, -0.83, -1.89, -0.82, -0.23, -1.7 , -0.43, -0.75,\n",
       "       -0.72, -2.14, -0.88, -0.45, -1.64, -1.25, -1.64, -0.46, -1.96,\n",
       "       -1.76, -1.27, -1.72, -1.59, -0.21, -1.67, -1.71, -2.19, -1.67,\n",
       "       -1.93, -0.08,  0.03, -0.97, -1.56, -1.22, -1.52, -1.58, -1.8 ,\n",
       "       -1.65, -1.58, -0.9 , -0.86, -1.56, -1.63, -1.59, -1.54, -1.56,\n",
       "       -2.08, -0.96, -0.82, -1.64, -1.56, -1.76, -0.32, -1.34, -1.68,\n",
       "       -1.56, -1.93, -1.7 , -1.5 , -1.47, -1.1 , -1.73, -1.59, -3.65,\n",
       "       -1.57, -1.94, -1.65, -4.87,  0.09, -0.39, -1.99, -0.97, -0.34,\n",
       "        0.03, -1.99, -1.89, -2.36, -1.76, -1.18, -1.8 , -0.29, -0.65,\n",
       "       -1.29, -1.66, -2.08, -0.4 , -2.16, -1.09, -1.61, -1.81, -0.56,\n",
       "       -1.41, -1.31, -0.18, -1.57, -1.8 ,  0.04, -0.4 , -1.59, -0.48,\n",
       "       -1.42, -1.51, -1.3 , -1.42, -1.34, -0.71, -1.87, -0.13, -2.05,\n",
       "       -2.21, -1.78, -1.87, -1.96, -1.09, -1.53, -1.63, -1.52, -1.56,\n",
       "       -1.47, -0.77, -2.22,  0.02, -0.47, -1.41, -4.17, -2.12, -1.63,\n",
       "       -0.56, -1.73, -0.64, -1.82, -0.7 , -1.47, -1.61, -0.62, -1.44,\n",
       "       -1.77, -0.25, -2.09, -1.28, -0.12, -0.47, -2.2 , -0.07, -0.95,\n",
       "       -1.3 , -1.57, -1.22, -2.1 , -0.35, -0.52, -0.01, -0.34, -2.62,\n",
       "       -1.38, -1.39, -1.52, -1.76, -1.64, -2.04, -1.51, -1.46, -1.5 ,\n",
       "       -1.14, -0.2 , -1.58, -0.23, -1.83, -2.11, -0.27, -1.62, -1.51,\n",
       "       -1.41, -1.33, -0.5 , -0.88, -0.02, -0.54, -1.96, -2.05, -1.54,\n",
       "       -1.49, -0.32, -1.34, -2.15, -1.59, -1.45, -1.68, -0.45, -1.78,\n",
       "       -0.88, -1.39, -1.08, -1.84, -0.22, -0.39, -1.65, -0.9 , -1.53,\n",
       "       -1.33, -1.43, -0.78, -1.54, -1.47, -1.82, -1.54, -1.26, -1.94,\n",
       "       -1.44, -0.23, -1.68, -1.72, -1.7 , -1.87, -1.3 , -0.26, -1.45,\n",
       "       -1.19, -1.65, -0.49, -1.85, -1.71, -1.67, -1.38, -1.5 , -0.36,\n",
       "       -1.64, -1.96, -2.05, -0.05, -0.37, -1.53, -1.7 , -0.53, -1.85,\n",
       "       -1.3 , -1.17, -2.11, -1.77, -1.75, -1.47, -1.86, -1.13, -1.63,\n",
       "       -1.52, -0.71, -2.12, -1.47,  0.01, -1.16, -1.88, -1.65, -1.42,\n",
       "       -0.35, -2.18, -1.74, -0.85, -1.1 , -1.28, -1.71, -1.43, -2.18,\n",
       "       -1.89])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.score_samples(X).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff8a42",
   "metadata": {},
   "source": [
    "Si calcula la exponencial de estas puntuaciones, obtiene el valor del PDF en la ubicación de las instancias dadas. Estas no son probabilidades, sino densidades de probabilidad: pueden asumir cualquier valor positivo, no solo un valor entre 0 y 1. \n",
    "\n",
    "Para estimar la probabilidad de que una instancia caiga dentro de una región en particular, tendría que integrar el PDF en esa región (si lo hace en todo el espacio de posibles ubicaciones de instancias, el resultado será 1).\n",
    "\n",
    "La figura 9-16 muestra los medios del grupo, los límites de decisión (líneas discontinuas) y los contornos de densidad de este modelo.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0916.png)\n",
    "\n",
    "_Figura 9-16. Medios de grupo, límites de decisión y contornos de densidad de un modelo de mezcla gaussiana entrenado_\n",
    "\n",
    "\n",
    "¡Lindo! El algoritmo claramente encontró una solución excelente. Por supuesto, facilitamos su tarea generando los datos utilizando un conjunto de distribuciones gaussianas 2D (desafortunadamente, los datos de la vida real no siempre son tan gaussianos y de baja dimensión). También le dimos al algoritmo el número correcto de grupos. Cuando hay muchas dimensiones, muchos grupos o pocas instancias, los ME pueden tener dificultades para converger hacia la solución óptima. Es posible que deba reducir la dificultad de la tarea limitando la cantidad de parámetros que el algoritmo debe aprender. Una forma de hacerlo es limitar la gama de formas y orientaciones que pueden tener los grupos. Esto se puede lograr imponiendo restricciones a las matrices de covarianza. Para hacer esto, establezca el hiperparámetro `covariance_type` en uno de los siguientes valores:\n",
    "\n",
    "\n",
    "- `\"spherical\"`\n",
    "\n",
    "    Todos los grupos deben ser esféricos, pero pueden tener diferentes diámetros (es decir, diferentes variaciones).\n",
    "    \n",
    "    \n",
    "- `\"diag\"`\n",
    "\n",
    "    Los cúmulos pueden tomar cualquier forma elipsoidal de cualquier tamaño, pero los ejes del elipsoide deben ser paralelos a los ejes de coordenadas (es decir, las matrices de covarianza deben ser diagonales).\n",
    "\n",
    "\n",
    "- `\"tied\"`\n",
    "\n",
    "    Todos los grupos deben tener la misma forma, tamaño y orientación elipsoidal (es decir, todos los grupos comparten la misma matriz de covarianza).    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0187e43",
   "metadata": {},
   "source": [
    "De forma predeterminada, `covariance_type` es igual a `\"full\"`, lo que significa que cada grupo puede adoptar cualquier forma, tamaño y orientación (tiene su propia matriz de covarianza sin restricciones). La Figura 9-17 muestra las soluciones encontradas por el algoritmo EM cuando `covariance_type` se establece en `\"tied\"` o `\"spherical\"`.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0917.png)\n",
    "\n",
    "_Figura 9-17. Mezclas gaussiana para grupos atados (izquierda) y grupos esféricos (derecha)_\n",
    "\n",
    "\n",
    "##### NOTA\n",
    "\n",
    "La complejidad computacional de entrenar un modelo `GaussianMixture` depende del número de instancias m, el número de dimensiones n, el número de grupos k y las restricciones de las matrices de covarianza. Si `covariance_type` es `\"spherical\"` o `\"diag\"`, es O(kmn), asumiendo que los datos tienen una estructura de agrupamiento. Si `covariance_type` está `\"tied\"` o `\"full\"`, es O (kmn2 + kn3), por lo que no se escalará a una gran cantidad de funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3dc4d",
   "metadata": {},
   "source": [
    "Los modelos de mezcla gaussiana también se pueden utilizar para la detección de anomalías. Veremos cómo en la siguiente sección.\n",
    "\n",
    "### Uso de mezclas gaussiana para la detección de anomalías\n",
    "\n",
    "El uso de un modelo de mezcla gaussiana para la detección de anomalías es bastante simple: cualquier instancia ubicada en una región de baja densidad puede considerarse una anomalía. Debe definir qué umbral de densidad desea utilizar. Por ejemplo, en una empresa de fabricación que intenta detectar productos defectuosos, la proporción de productos defectuosos suele ser bien conocida. Digamos que es igual al 2 %. A continuación, establece que el umbral de densidad sea el valor que resulta en tener el 2 % de las instancias ubicadas en áreas por debajo de esa densidad de umbral. Si notas que obtienes demasiados falsos positivos (es decir, productos perfectamente buenos que están marcados como defectuosos), puedes bajar el umbral. Por el contrario, si tiene demasiados falsos negativos (es decir, productos defectuosos que el sistema no marca como defectuosos), puede aumentar el umbral. Esta es la compensación habitual de precisión/retiro (ver Capítulo 3). Así es como identificaría los valores atípicos utilizando la densidad más baja del cuarto percentil como umbral (es decir, aproximadamente el 4 % de las instancias se marcarán como anomalías):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69328bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = gm.score_samples(X)\n",
    "density_threshold = np.percentile(densities, 2)\n",
    "anomalies = X[densities < density_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911d182",
   "metadata": {},
   "source": [
    "La figura 9-18 representa estas anomalías como estrellas.\n",
    "\n",
    "Una tarea estrechamente relacionada es la detección de novedades: difiere de la detección de anomalías en que se supone que el algoritmo está entrenado en un conjunto de datos \"limpio\", no contaminado por valores atípicos, mientras que la detección de anomalías no hace esta suposición. De hecho, la detección de valores atípicos se utiliza a menudo para limpiar un conjunto de datos.\n",
    "\n",
    "##### PROPINA\n",
    "\n",
    "Los modelos de mezcla gaussiana intentan ajustarse a todos los datos, incluidos los valores atípicos; si tiene demasiados de ellos, esto sesgará la visión del modelo de la \"normalidad\", y algunos valores atípicos pueden considerarse erróneamente como normales. Si esto sucede, puede intentar ajustar el modelo una vez, usarlo para detectar y eliminar los valores atípicos más extremos, y luego ajustar el modelo de nuevo en el conjunto de datos limpiado. Otro enfoque es utilizar métodos robustos de estimación de covarianza (ver la clase `EllipticEnvelope`).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0918.png)\n",
    "\n",
    "_Figura 9-18. Detección de anomalías utilizando un modelo de mezcla gaussiana_\n",
    "\n",
    "\n",
    "Al igual que k-means, el algoritmo `GaussianMixture` requiere que especifiques el número de grupos. Entonces, ¿cómo puedes encontrar ese número?\n",
    "\n",
    "\n",
    "### Selección del número de clústeres\n",
    "\n",
    "\n",
    "Con k-means, puedes usar la inercia o la puntuación de la silueta para seleccionar el número apropiado de grupos. Pero con las mezclas gaussiana, no es posible utilizar estas métricas porque no son fiables cuando los grupos no son esféricos o tienen diferentes tamaños. En su lugar, puede tratar de encontrar el modelo que minimiza un criterio de información teórico, como el criterio de información bayesiana (BIC) o el criterio de información de Akaike (AIC), definido en la ecuación 9-1.\n",
    "\n",
    "\n",
    "#### Ecuación 9-1. Criterio de información bayesiano (BIC) y criterio de información de Akaike (AIC)\n",
    "\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/VDrCD7r/Captura-de-pantalla-2023-11-20-a-las-4-26-23.png\" alt=\"Captura-de-pantalla-2023-11-20-a-las-4-26-23\" border=\"0\"></a>\n",
    "\n",
    "En estas ecuaciones:\n",
    "\n",
    "* m es el número de instancias, como siempre.\n",
    "* p es el número de parámetros aprendidos por el modelo.\n",
    "* L^ es el valor máximo de la función de probabilidad del modelo.\n",
    "\n",
    "\n",
    "Tanto el BIC como el AIC penalizan los modelos que tienen más parámetros que aprender (por ejemplo, más grupos) y los modelos de recompensa que se ajustan bien a los datos. A menudo terminan seleccionando el mismo modelo. Cuando difieren, el modelo seleccionado por el BIC tiende a ser más simple (menos parámetros) que el seleccionado por el AIC, pero tiende a no ajustarse tan bien a los datos (esto es especialmente cierto para conjuntos de datos más grandes).\n",
    "\n",
    "\n",
    "##### FUNCIÓN DE PROBABILIDAD\n",
    "\n",
    "Los términos \"probabilidad\" y \"probabilidad\" a menudo se usan indistintamente en el lenguaje cotidiano, pero tienen significados muy diferentes en las estadísticas. Dado un modelo estadístico con algunos parámetros θ, la palabra \"probabilidad\" se utiliza para describir lo plausible que es un resultado futuro x (conociendo los valores de los parámetros θ), mientras que la palabra \"probabilidad\" se utiliza para describir lo plausible que es un conjunto particular de valores de parámetrosθ, después de que se conozca el resultado **x**.\n",
    "\n",
    "Considere un modelo de mezcla 1D de dos distribuciones gaussiana centradas en -4 y +1. Para simplificar, este modelo de juguete tiene un único parámetro θ que controla las desviaciones estándar de ambas distribuciones. El gráfico de contorno de la parte superior izquierda de la Figura 9-19 muestra todo el modelo f(x; θ) en función de x y θ. Para estimar la distribución de probabilidad de un resultado futuro x, debe establecer el parámetro del modelo θ. Por ejemplo, si estableces θ en 1,3 (la línea horizontal), obtienes la función de densidad de probabilidad f(x; θ=1.3) que se muestra en el gráfico inferior izquierdo. Digamos que quieres estimar la probabilidad de que x caiga entre -2 y +2. Debe calcular la integral del PDF en este rango (es decir, la superficie de la región sombreada). Pero, ¿qué pasa si no sabes θ, y en su lugar si has observado una sola instancia x=2.5 (la línea vertical en el gráfico de la parte superior izquierda)? En este caso, se obtiene la función de probabilidad ℒ(θ|x=2.5)=f(x=2.5; θ), representada en el gráfico de la parte superior derecha.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0919.png)\n",
    "\n",
    "_Figura 9-19. La función paramétrica de un modelo (arriba a la izquierda) y algunas funciones derivadas: un PDF (abajo a la izquierda), una función de probabilidad (arriba a la derecha) y una función de verosimilitud de registro (abajo a la derecha)_\n",
    "\n",
    "\n",
    "En resumen, el PDF es una función de x (con θ fijo), mientras que la función de probabilidad es una función de θ (con x fijo). Es importante entender que la función de probabilidad no es una distribución de probabilidad: si integras una distribución de probabilidad sobre todos los valores posibles de x, siempre obtienes 1, pero si integras la función de probabilidad sobre todos los valores posibles de θ, el resultado puede ser cualquier valor positivo.\n",
    "\n",
    "Dado un conjunto de datos **X**, una tarea común es tratar de estimar los valores más probables para los parámetros del modelo. Para hacer esto, debe encontrar los valores que maximizan la función de probabilidad, dada **X**. En este ejemplo, si ha observado una sola instanciax=2.5, la estimación de máxima probabilidad (MLE) de θ es θ^≈1.66. Si existe una distribución de probabilidad previa g sobre θ, es posible tenerla en cuenta maximizando ℒ(θ|x)g(θ) en lugar de solo maximizar ℒ(θ|x). Esto se llama estimación máxima de a-posteriori (MAP). Dado que MAP restringe los valores de los parámetros, puedes pensar en él como una versión regularizada de MLE.\n",
    "\n",
    "Tenga en cuenta que maximizar la función de verosimilitud es equivalente a maximizar su logaritmo (representado en el gráfico de la parte inferior derecha de la Figura 9-19). De hecho, el logaritmo es una función estrictamente creciente, por lo que si θ maximiza la probabilidad de logarítmica, también maximiza la probabilidad. Resulta que generalmente es más fácil maximizar la probabilidad de registro. Por ejemplo, si observó varias instancias independientes de x(1) a x(m), tendría que encontrar el valor de θ que maximiza el producto de las funciones de probabilidad individuales. Pero es equivalente, y mucho más simple, maximizar la suma (no el producto) de las funciones de probabilidad de registro, gracias a la magia del logaritmo que convierte los productos en sumas: log(ab) = log(a) + log(b).\n",
    "\n",
    "Una vez que hayas estimado 0^ el valor de θ que maximiza la función de probabilidad, entonces estás listo para calcular ℒ^ = ℒ(0^,X), que es el valor utilizado para calcular el AIC y el BIC; se puede pensar en él como una medida de lo bien que el modelo se ajusta a los datos.\n",
    "\n",
    "\n",
    "Para calcular el BIC y el AIC, llamamos a los métodos `big` y `aic()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ab1ece5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749.09899798868"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.bic(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4c42508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2665.6671582459835"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.aic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ae2e4",
   "metadata": {},
   "source": [
    "La figura 9-20 muestra el BIC para diferentes números de grupos k. Como puede ver, tanto el BIC como el AIC son los más bajos cuando k=3, por lo que lo más probable es que sea la mejor opción.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0920.png)\n",
    "\n",
    "_Figura 9-20. AIC y BIC para diferentes números de grupos k_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c40332",
   "metadata": {},
   "source": [
    "## Modelos de mezcla gaussiana bayesiana\n",
    "\n",
    "\n",
    "En lugar de buscar manualmente el número óptimo de clústeres, puede utilizar la clase `BayesianGaussianMixture`, que es capaz de asignar pesos iguales (o cercanos) a cero a los clústeres innecesarios. Establezca el número de clústeres `n_components` en un valor que tenga buenas razones para creer que es mayor que el número óptimo de clústeres (esto supone un conocimiento mínimo sobre el problema en cuestión) y el algoritmo eliminará los clústeres innecesarios automáticamente. Por ejemplo, establezcamos el número de clústeres en 10 y veamos qué sucede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eed317c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14, 0.13, 0.12, 0.13, 0.13, 0.11, 0.14, 0.  , 0.11, 0.  ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\n",
    "bgm.fit(X)\n",
    "bgm.weights_.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c5824b",
   "metadata": {},
   "source": [
    "Perfecto: el algoritmo detectó automáticamente que solo se necesitan tres grupos, y los grupos resultantes son casi idénticos a los de la Figura 9-16.\n",
    "\n",
    "Una nota final sobre los modelos de mezcla gaussiana: aunque funcionan muy bien en grupos con formas elipsoidales, no funcionan tan bien con grupos de formas muy diferentes. Por ejemplo, veamos qué sucede si usamos un modelo de mezcla gaussiana bayesiana para agrupar el conjunto de datos de lunas (ver Figura 9-21).\n",
    "\n",
    "¡Vaya! El algoritmo buscó desesperadamente elipsoides, por lo que encontró ocho grupos diferentes en lugar de dos. La estimación de la densidad no es tan mala, por lo que este modelo tal vez podría usarse para la detección de anomalías, pero no pudo identificar las dos lunas. Para concluir este capítulo, echemos un vistazo rápido a algunos algoritmos capaces de lidiar con grupos de forma arbitraria.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_0921.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981c09c",
   "metadata": {},
   "source": [
    "## Otros algoritmos para la detección de anomalías y novedades\n",
    "\n",
    "Scikit-Learn implementa otros algoritmos dedicados a la detección de anomalías o novedades:\n",
    "\n",
    "- **Fast-MCD** (determinado de covarianza mínima)\n",
    "\n",
    "    Implementado por la clase `EllipticEnvelope`, este algoritmo es útil para la detección de valores atípicos, en particular para limpiar un conjunto de datos. Supone que las instancias normales (inliers) se generan a partir de una sola distribución gaussiana (no de una mezcla). También asume que el conjunto de datos está contaminado con valores atípicos que no se generaron a partir de esta distribución gaussiana. Cuando el algoritmo estima los parámetros de la distribución gaussiana (es decir, la forma de la envoltura elíptica alrededor de los inliers), se tiene cuidado de ignorar las instancias que son más probables atípicas. Esta técnica proporciona una mejor estimación de la envoltura elíptica y, por lo tanto, hace que el algoritmo sea mejor para identificar los valores atípicos.\n",
    "\n",
    "\n",
    "- **Bosque de aislamiento**\n",
    "\n",
    "    Este es un algoritmo eficiente para la detección de valores atípicos, especialmente en conjuntos de datos de alta dimensión. El algoritmo construye un bosque aleatorio en el que cada árbol de decisión se cultiva al azar: en cada nodo, elige una característica al azar, luego elige un valor de umbral aleatorio (entre los valores mínimo y máximo) para dividir el conjunto de datos en dos. El conjunto de datos se corta gradualmente en pedazos de esta manera, hasta que todas las instancias terminan aisladas de las otras instancias. Las anomalías suelen estar lejos de otros casos, por lo que en promedio (a través de todos los árboles de decisión) tienden a aislarse en menos pasos que en los casos normales.\n",
    "\n",
    "\n",
    "- **Factor atípico local (LOF)**\n",
    "\n",
    "    Este algoritmo también es bueno para la detección de valores atípicos. Compara la densidad de instancias alrededor de una instancia dada con la densidad alrededor de sus vecinos. Una anomalía a menudo está más aislada que sus vecinos más cercanos.\n",
    "\n",
    "\n",
    "- **SVM de una clase**\n",
    "\n",
    "    Este algoritmo es más adecuado para la detección de novedades. Recuerde que un clasificador SVM con núcleo separa dos clases primero (implícitamente) mapeando todas las instancias a un espacio de alta dimensión, y luego separa las dos clases utilizando un clasificador SVM lineal dentro de este espacio de alta dimensión (ver el capítulo 5). Dado que solo tenemos una clase de instancias, el algoritmo SVM de una clase intenta separar las instancias en el espacio de alta dimensión del origen. En el espacio original, esto corresponderá a encontrar una pequeña región que abarque todas las instancias. Si una nueva instancia no cae dentro de esta región, es una anomalía. Hay algunos hiperparámetros que ajustar: los habituales para una SVM con kernel, más un hiperparámetro de margen que corresponde a la probabilidad de que una nueva instancia se considere erróneamente como nueva cuando de hecho es normal. Funciona muy bien, especialmente con conjuntos de datos de alta dimensión, pero como todos los SVM, no se escala a grandes conjuntos de datos.\n",
    "\n",
    "PCA y otras técnicas de reducción de dimensionalidad con el método aninverse `inverse_transform()`\n",
    "\n",
    "    Si comparas el error de reconstrucción de una instancia normal con el error de reconstrucción de una anomalía, esta última suele ser mucho mayor. Este es un enfoque de detección de anomalías simple y a menudo bastante eficiente (consulte los ejercicios de este capítulo para ver un ejemplo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6b889",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "1. ¿Cómo definirías la agrupación? ¿Puedes nombrar algunos algoritmos de agrupación?\n",
    "\n",
    "\n",
    "2. ¿Cuáles son algunas de las principales aplicaciones de los algoritmos de agrupación?\n",
    "\n",
    "\n",
    "3. Describa dos técnicas para seleccionar el número correcto de grupos cuando se usan k-means.\n",
    "\n",
    "\n",
    "4. ¿Qué es la propagación de etiquetas? ¿Por qué lo implementarías y cómo?\n",
    "\n",
    "5. ¿Puedes nombrar dos algoritmos de agrupación que pueden escalar a grandes conjuntos de datos? ¿Y dos que buscan regiones de alta densidad?\n",
    "\n",
    "\n",
    "6. ¿Se te ocurre un caso de uso en el que el aprendizaje activo sea útil? ¿Cómo lo implementarías?\n",
    "\n",
    "\n",
    "7. ¿Cuál es la diferencia entre la detección de anomalías y la detección de novedades?\n",
    "\n",
    "\n",
    "8. ¿Qué es una mezcla gaussiana? ¿Para qué tareas puedes usarlo?\n",
    "\n",
    "\n",
    "9. ¿Puedes nombrar dos técnicas para encontrar el número correcto de grupos cuando se utiliza un modelo de mezcla gaussiana?\n",
    "\n",
    "\n",
    "10. El clásico conjunto de datos de caras Olivetti contiene 400 imágenes de caras en escala de grises de 64 × 64 píxeles. Cada imagen se aplana a un vector 1D de tamaño 4.096. Se fotografió a cuarenta personas diferentes (10 veces cada una), y la tarea habitual es entrenar a un modelo que pueda predecir qué persona está representada en cada imagen. Cargue el conjunto de datos usando la función `sklearn.datasets.fetch_olivetti_faces()`, luego divídalo en un conjunto de entrenamiento, un conjunto de validación y un conjunto de pruebas (tenga en cuenta que el conjunto de datos ya está escalado entre 0 y 1). Dado que el conjunto de datos es bastante pequeño, es probable que desee utilizar un muestreo estratificado para asegurarse de que haya el mismo número de imágenes por persona en cada conjunto. A continuación, agrupa las imágenes usando k-means y asegúrate de tener un buen número de grupos (utilizando una de las técnicas discutidas en este capítulo). Visualiza los grupos: ¿ves caras similares en cada grupo?\n",
    "\n",
    "\n",
    "11. Continuando con el conjunto de datos de caras de Olivetti, entrene a un clasificador para predecir qué persona está representada en cada imagen y evalúelo en el conjunto de validación. A continuación, utilice k-means como herramienta de reducción de dimensionalidad y entrene a un clasificador en el conjunto reducido. Busca el número de clústeres que permiten al clasificador obtener el mejor rendimiento: ¿qué rendimiento puedes alcanzar? ¿Qué pasa si añades las características del conjunto reducido a las características originales (de nuevo, buscando el mejor número de grupos)?\n",
    "\n",
    "\n",
    "12. Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset’s dimensionality (e.g., use PCA, preserving 99% of the variance). Use the model to generate some new faces (using the sample() method), and visualize them (if you used PCA, you will need to use its inverse_transform() method). Try to modify some images (e.g., rotate, flip, darken) and see if the model can detect the anomalies (i.e., compare the output of the score_samples() method for normal images and for anomalies).\n",
    "\n",
    "\n",
    "13. Algunas técnicas de reducción de la dimensionalidad también se pueden utilizar para la detección de anomalías. Por ejemplo, tome el conjunto de datos de caras de Olivetti y reduciéndolo con PCA, preservando el 99 % de la varianza. Luego calcule el error de reconstrucción para cada imagen. A continuación, toma algunas de las imágenes modificadas que creaste en el ejercicio anterior y mira su error de reconstrucción: ten en cuenta lo grande que es. Si trazas una imagen reconstruido, verás por qué: intenta reconstruir una cara normal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Las soluciones a estos ejercicios están disponibles al final del cuaderno de este capítulo, en https://homl.info/colab3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7b929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
