{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9652ce3d",
   "metadata": {},
   "source": [
    "Aunque el superordenador Deep Blue de IBM venció al campeón mundial de ajedrez Garry Kasparov en 1996, no fue hasta hace poco que los ordenadores fueron capaces de realizar de manera confiable tareas aparentemente triviales, como detectar un cachorro en una imagen o reconocer las palabras habladas. ¿Por qué estas tareas son tan fáciles para nosotros los humanos? La respuesta radica en el hecho de que la percepción tiene lugar en gran medida fuera del ámbito de nuestra conciencia, dentro de módulos visuales, auditivos y de otro tipo especializados en nuestro cerebro. En el momento en que la información sensorial llega a nuestra conciencia, ya está adornada con características de alto nivel; por ejemplo, cuando miras una foto de un lindo cachorro, no puedes elegir no ver al cachorro, no notar su ternura. Tampoco puedes explicar cómo reconoces a un lindo cachorro; es obvio para ti. Por lo tanto, no podemos confiar en nuestra experiencia subjetiva: la percepción no es trivial en absoluto, y para entenderla debemos ver cómo funcionan nuestros módulos sensoriales.\n",
    "\n",
    "Las redes neuronales convolucionales (CNN) surgieron del estudio de la corteza visual del cerebro, y se han utilizado en el reconocimiento de imágenes por computadora desde la década de 1980. En los últimos 10 años, gracias al aumento de la potencia computacional, la cantidad de datos de entrenamiento disponibles y los trucos presentados en el Capítulo 11 para entrenar redes profundas, las CNN han logrado lograr un rendimiento sobrehumano en algunas tareas visuales complejas. Potencian los servicios de búsqueda de imágenes, los coches autónomos, los sistemas automáticos de clasificación de vídeo y más. Además, las CNN no se limitan a la percepción visual: también tienen éxito en muchas otras tareas, como el reconocimiento de voz y el procesamiento del lenguaje natural. Sin embargo, por ahora nos centraremos en las aplicaciones visuales.\n",
    "\n",
    "En este capítulo exploraremos de dónde vinieron las CNN, cómo se ven sus bloques de construcción y cómo implementarlos usando Keras. Luego discutiremos algunas de las mejores arquitecturas de CNN, así como otras tareas visuales, incluida la detección de objetos (clasificar varios objetos en una imagen y colocar cuadros delimitadores a su alrededor) y la segmentación semántica (clasificar cada píxel de acuerdo con la clase del objeto al que pertenece).\n",
    "\n",
    "\n",
    "# La arquitectura del cortex visual\n",
    "\n",
    "David H. Hubel y Torsten Wiesel realizaron una serie de experimentos en gatos en 1958⁠1 y 1959⁠2 (y unos años más tarde en los monos⁠3), dando una visión crucial sobre la estructura de la corteza visual (los autores recibieron el Premio Nobel de Fisiología o Medicina en 1981 por su trabajo). En particular, mostraron que muchas neuronas en la corteza visual tienen un pequeño campo receptivo local, lo que significa que reaccionan solo a estímulos visuales ubicados en una región limitada del campo visual (ver Figura 14-1, en la que los campos receptivos locales de cinco neuronas están representados por círculos con tallas). Los campos receptivos de las diferentes neuronas pueden superponerse, y juntos azulean todo el campo visual.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1401.png)\n",
    "\n",
    "(_Figura 14-1. Las neuronas biológicas en la corteza visual responden a patrones específicos en pequeñas regiones del campo visual llamados campos receptivos; a medida que la señal visual se abre camino a través de módulos cerebrales consecutivos, las neuronas responden a patrones más complejos en campos receptivos más grandes_)\n",
    "\n",
    "Además, los autores mostraron que algunas neuronas reaccionan solo a imágenes de líneas horizontales, mientras que otras reaccionan solo a líneas con diferentes orientaciones (dos neuronas pueden tener el mismo campo receptivo pero reaccionan a diferentes orientaciones de línea). También se dieron cuenta de que algunas neuronas tienen campos receptivos más grandes, y reaccionan a patrones más complejos que son combinaciones de los patrones de nivel inferior. Estas observaciones llevaron a la idea de que las neuronas de nivel superior se basan en las salidas de las neuronas vecinas de nivel inferior (en la Figura 14-1, observe que cada neurona está conectada solo a las neuronas cercanas de la capa anterior). Esta potente arquitectura es capaz de detectar todo tipo de patrones complejos en cualquier área del campo visual.\n",
    "\n",
    "Estos estudios de la corteza visual inspiraron el neocognitron,⁠ introducido en 1980, que gradualmente evolucionó hacia lo que ahora llamamos redes neuronales convolucionales. Un hito importante fue un documento de 1998⁠5 de Yann LeCun et al. que introdujo la famosa arquitectura LeNet-5, que fue ampliamente utilizada por los bancos para reconocer dígitos escritos a mano en los cheques. Esta arquitectura tiene algunos bloques de construcción que ya conoces, como capas totalmente conectadas y funciones de activación de sigmoid, pero también introduce dos nuevos bloques de construcción: capas convolucionales y capas de pooling. Vamos a verlos ahora.\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "¿Por qué no usar simplemente una red neuronal profunda con capas totalmente conectadas para tareas de reconocimiento de imágenes? Desafortunadamente, aunque esto funciona bien para imágenes pequeñas (por ejemplo, MNIST), se descompone para imágenes más grandes debido a la gran cantidad de parámetros que requiere. Por ejemplo, una imagen de 100 × 100 píxeles tiene 10.000 píxeles, y si la primera capa tiene solo 1000 neuronas (lo que ya restringe severamente la cantidad de información transmitida a la siguiente capa), esto significa un total de 10 millones de conexiones. Y esa es solo la primera capa. Las CNN resuelven este problema utilizando capas parcialmente conectadas y el uso compartido de peso.\n",
    "\n",
    "#### --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Capas convolucionales\n",
    "\n",
    "El componente más importante de una CNN es la capa convolucional: neuronas en la primera capa convolucional no están conectadas a cada píxel de la imagen de entrada (como estaban en las capas discutidas en capítulos anteriores), sino solo a píxeles en sus campos receptivos (ver Figura 14-2). A su vez, cada neurona de la segunda capa convolucional está conectada solo a las neuronas ubicadas dentro de un pequeño rectángulo en la primera capa. Esta arquitectura permite que la red se concentre en pequeñas características de bajo nivel en la primera capa oculta, luego las ensambla en características más grandes de nivel superior en la siguiente capa oculta, y así sucesi. Esta estructura jerárquica es común en las imágenes del mundo real, que es una de las razones por las que las CNN funcionan tan bien para el reconocimiento de imágenes.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1402.png)\n",
    "\n",
    "(_Figura 14-2. Capas de CNN con campos receptivos locales rectangulares_)\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "Todas las redes neuronales multicapa que hemos visto hasta ahora tenían capas compuestas por una larga línea de neuronas, y tuvimos que aplanar las imágenes de entrada a 1D antes de alimentarlas a la red neuronal. En una CNN, cada capa está representada en 2D, lo que facilita la coincidencia de las neuronas con sus entradas correspondientes.\n",
    "\n",
    "#### --------------------------------------------------------------------\n",
    "\n",
    "Una neurona ubicada en la fila i, columna j de una capa dada está conectada a las salidas de las neuronas en la capa anterior ubicada en las filas i a i + fh - 1, columnas j a j + fw - 1, donde fh y fw son la altura y el ancho del campo receptivo (ver Figura 14-3). Para que una capa tenga la misma altura y anchura que la capa anterior, es común añadir ceros alrededor de las entradas, como se muestra en el diagrama. Esto se llama relleno cero.\n",
    "\n",
    "También es posible conectar una capa de entrada grande a una capa mucho más pequeña espaciando los campos receptivos, como se muestra en la Figura 14-4. Esto reduce drásticamente la complejidad computacional del modelo. El tamaño del paso horizontal o vertical de un campo receptivo al siguiente se llama paso. En el diagrama, una capa de entrada de 5 × 7 (más cero relleno) está conectada a una capa de 3 × 4, utilizando 3 × 3 campos receptivos y una zancada de 2 (en este ejemplo, la zancada es la misma en ambas direcciones, pero no tiene por que ser así). Una neurona ubicada en la fila i, columna j en la capa superior, está conectada a las salidas de las neuronas en la capa anterior ubicada en filas i × sh a i × sh +fh - 1, columnas j × sw a j × sw + fw - 1, donde sh y sw son los pasos verticales y horizontales.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1403.png)\n",
    "\n",
    "(_Figura 14-3. Conexiones entre capas y relleno cero_)\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1404.png)\n",
    "\n",
    "(_Figura 14-4. Reducir la dimensionalidad usando un paso de 2_)\n",
    "\n",
    "\n",
    "# Filtros\n",
    "\n",
    "Los pesos de una neurona se pueden representar como una pequeña imagen del tamaño del campo receptivo. Por ejemplo, la Figura 14-5 muestra dos posibles conjuntos de pesos, llamados filtros (o núcleos de convolución, o simplemente núcleos). El primero se representa como un cuadrado negro con una línea blanca vertical en el medio (es una matriz de 7 × 7 llena de 0s, excepto por la columna central, que está llena de 1s); las neuronas que usan estos pesos ignorarán todo en su campo receptivo, excepto la línea vertical central (ya que todas las entradas se multiplicarán por 0, excepto las de la línea vertical central). El segundo filtro es un cuadrado negro con una línea blanca horizontal en el medio. Las neuronas que utilicen estos pesos ignorarán todo en su campo receptivo, excepto la línea horizontal central.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1405.png)\n",
    "\n",
    "(_Figura 14-5. Aplicar dos filtros diferentes para obtener dos mapas de características_)\n",
    "\n",
    "\n",
    "Ahora, si todas las neuronas de una capa utilizan el mismo filtro de línea vertical (y el mismo término de sesgo), y alimentas a la red la imagen de entrada que se muestra en la Figura 14-5 (la imagen inferior), la capa mostrará la imagen de la parte superior izquierda. Tenga en cuenta que las líneas blancas verticales se mejoran mientras el resto se difumina. Del mismo modo, la imagen de la parte superior derecha es lo que se obtiene si todas las neuronas usan el mismo filtro de línea horizontal; tenga en cuenta que las líneas blancas horizontales se mejoran mientras el resto está borroso. Por lo tanto, una capa llena de neuronas que utilizan el mismo filtro genera un mapa de características, que resalta las áreas de una imagen que más activan el filtro. Pero no te preocupes, no tendrás que definir los filtros manualmente: en su lugar, durante el entrenamiento, la capa convolucional aprenderá automáticamente los filtros más útiles para su tarea, y las capas anteriores aprenderán a combinarlos en patrones más complejos.\n",
    "\n",
    "\n",
    "## Apilar múltiples mapas de características\n",
    "\n",
    "\n",
    "Hasta ahora, en aras de la simplicidad, he representado la salida de cada capa convolucional como una capa 2D, pero en realidad una capa convolucional tiene múltiples filtros (tú decides cuántos) y genera un mapa de características por filtro, por lo que se representa con mayor precisión en 3D (ver Figura 14-6). Tiene una neurona por píxel en cada mapa de características, y todas las neuronas dentro de un mapa de características determinado comparten los mismos parámetros (es decir, el mismo núcleo y el mismo término de sesgo). Las neuronas en diferentes mapas de características utilizan diferentes parámetros. El campo receptivo de una neurona es el mismo que se describió anteriormente, pero se extiende a través de todos los mapas de características de la capa anterior. En resumen, una capa convolucional aplica simultáneamente múltiples filtros entrenables a sus entradas, lo que la hace capaz de detectar múltiples características en cualquier lugar de sus entradas.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1406.png)\n",
    "\n",
    "(_Figura 14-6. Dos capas convolucionales con múltiples filtros cada una (nústreos), procesando una imagen de color con tres canales de color; cada capa convolucional genera un mapa de características por filtro_)\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "El hecho de que todas las neuronas de un mapa de características compartan los mismos parámetros reduce drásticamente el número de parámetros en el modelo. Una vez que la CNN ha aprendido a reconocer un patrón en un lugar, puede reconocerlo en cualquier otro lugar. Por el contrario, una vez que una red neuronal totalmente conectada ha aprendido a reconocer un patrón en un lugar, solo puede reconocerlo en ese lugar en particular.\n",
    "\n",
    "#### --------------------------------------------------------------------------\n",
    "\n",
    "Las imágenes de entrada también están compuestas por múltiples subcapas: una por canal de color. Como se menciona en el capítulo 9, normalmente hay tres: rojo, verde y azul (RGB). Las imágenes en escala de grises tienen un solo canal, pero algunas imágenes pueden tener muchas más, por ejemplo, imágenes de satélite que capturan frecuencias de luz adicionales (como el infrarrojo).\n",
    "\n",
    "Específicamente, una neurona ubicada en la fila i, columna j del mapa de características k en una capa convolucional dada l está conectada a las salidas de las neuronas en la capa anterior l - 1, ubicada en filas i × sh a i × sh + fh - 1 y columnas j × sw a j × sw + fw - 1, a través de todos los mapas de características (en la capa l - 1). Tenga en cuenta que, dentro de una capa, todas las neuronas ubicadas en la misma fila i y columna j, pero en diferentes mapas de características, están conectadas a las salidas de exactamente las mismas neuronas en la capa anterior.\n",
    "\n",
    "La ecuación 14-1 resume las explicaciones anteriores en una gran ecuación matemática: muestra cómo calcular la salida de una neurona dada en una capa convolucional. Es un poco feo debido a todos los diferentes índices, pero todo lo que hace es calcular la suma ponderada de todas las entradas, más el término de sesgo.\n",
    "\n",
    "### Ecuación 14-1. Calcular la salida de una neurona en una capa convolucional\n",
    "\n",
    "<a href=\"https://ibb.co/mvfXf12\"><img src=\"https://i.ibb.co/f8jnjBy/Captura-de-pantalla-2024-03-21-a-las-20-51-17.png\" alt=\"Captura-de-pantalla-2024-03-21-a-las-20-51-17\" border=\"0\"></a>\n",
    "\n",
    "\n",
    "En esta ecuación:\n",
    "\n",
    "* **zi, j, k** es la salida de la neurona ubicada en la fila i, columna j en el mapa de características k de la capa convolucional (capa l).\n",
    "\n",
    "- Como se explicó anteriormente, sh y sw son los pasos verticales y horizontales, fh y fw son la altura y el ancho del campo receptivo, y fn′ es el número de mapas de características en la capa anterior (capa l - 1).\n",
    "\n",
    "* **xi′, j′, k′** es la salida de la neurona ubicada en la capa l - 1, fila i′, columna j′, mapa de características k′ (o canal k′ si la capa anterior es la capa de entrada).\n",
    "\n",
    "- **bk** es el término sesgado para el mapa de características k (en la capa l). Puedes pensar en él como una perilla que ajusta el brillo general del mapa de características k.\n",
    "\n",
    "* **wu, v, k′, k** es el peso de conexión entre cualquier neurona en el mapa de características k de la capa l y su entrada ubicada en la fila u, columna v (en relación con el campo receptivo de la neurona) y el mapa de características k′.\n",
    "\n",
    "Veamos cómo crear y usar una capa convolucional usando Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fee80e",
   "metadata": {},
   "source": [
    "## Implementación de capas convolucionales con Keras\n",
    "\n",
    "Primero, carguemos y preprocesemos un par de imágenes de muestra, usando la función `load_sample_image()` de Scikit-Learn y las capas `CenterCrop` y `Rescaling` de Keras (todas las cuales se introdujeron en el Capítulo 13):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399fc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 19:21:32.001518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-23 19:21:39.405708: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "\n",
    "images = load_sample_images()[\"images\"]\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale=1 / 255)(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23f6b7",
   "metadata": {},
   "source": [
    "Echemos un vistazo a la forma del tensor de las `images`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859dab3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b070a",
   "metadata": {},
   "source": [
    "Vaya, es un tensor 4D; ¡No hemos visto esto antes! ¿Qué significan todas estas dimensiones? Bueno, hay dos imágenes de muestra, lo que explica la primera dimensión. Entonces cada imagen es de 70 × 120, ya que ese es el tamaño que especificamos al crear la capa `CenterCrop` (las imágenes originales eran de 427 × 640). Esto explica la segunda y tercera dimensión. Y, por último, cada píxel tiene un valor por canal de color, y hay tres (rojo, verde y azul), lo que explica la última dimensión.\n",
    "\n",
    "Ahora creemos una capa convolucional 2D y alimentémosla con estas imágenes para ver qué sale. Para ello, Keras proporciona una capa `Convolution2D`, alias `Conv2D`. En esencia, esta capa se basa en la operación `tf.nn.conv2d()` de TensorFlow. Creemos una capa convolucional con 32 filtros, cada uno de tamaño 7 × 7 (usando `kernel_size=7`, que es equivalente a usar `kernel_size=(7, 7)`), y apliquemos esta capa a nuestro pequeño lote de dos imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b1308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d4852",
   "metadata": {},
   "source": [
    "#### NOTA\n",
    "\n",
    "Cuando hablamos de una capa convolucional 2D, \"2D\" se refiere al número de dimensiones espaciales (altura y ancho), pero como puede ver, la capa toma entradas 4D: como vimos, las dos dimensiones adicionales son el tamaño del lote (primera dimensión) y los canales (última dimensión).\n",
    "\n",
    "#### -------------------------------------------------------------------------\n",
    "\n",
    "Ahora echemos un vistazo a la forma de la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c278871d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64, 114, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15710b77",
   "metadata": {},
   "source": [
    "La forma de salida es similar a la forma de entrada, con dos diferencias principales. Primero, hay 32 canales en lugar de 3. Esto se debe a que configuramos `filters=32`, por lo que obtenemos 32 mapas de características de salida: en lugar de la intensidad de rojo, verde y azul en cada ubicación, ahora tenemos la intensidad de cada característica. en cada ubicación. En segundo lugar, la altura y el ancho se han reducido en 6 píxeles. Esto se debe al hecho de que la capa `Conv2D` no utiliza ningún relleno con ceros de forma predeterminada, lo que significa que perdemos algunos píxeles en los lados de los mapas de características de salida, dependiendo del tamaño de los filtros. En este caso, dado que el tamaño del kernel es 7, perdemos 6 píxeles horizontalmente y 6 píxeles verticalmente (es decir, 3 píxeles en cada lado).\n",
    "\n",
    "#### ADVERTENCIA\n",
    "\n",
    "Sorprendentemente, la opción predeterminada se llama `padding=\"valid\"`, lo que en realidad significa que no hay ningún relleno con ceros. Este nombre proviene del hecho de que en este caso el campo receptivo de cada neurona se encuentra estrictamente dentro de posiciones válidas dentro de la entrada (no sale de los límites). No es una peculiaridad de los nombres de Keras: todo el mundo usa esta nomenclatura extraña.\n",
    "\n",
    "#### --------------------------------------------------------------------------\n",
    "\n",
    "Si, en cambio, configuramos `padding=\"same\"`, entonces las entradas se rellenan con suficientes ceros en todos los lados para garantizar que los mapas de características de salida terminen con el mismo tamaño que las entradas (de ahí el nombre de esta opción):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad194e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\")\n",
    "# ...\n",
    "# ...\n",
    "fmaps = conv_layer(images)\n",
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c67ae",
   "metadata": {},
   "source": [
    "Estas dos opciones de relleno se ilustran en la Figura 14-7. Para simplificar, aquí solo se muestra la dimensión horizontal, pero, por supuesto, la misma lógica también se aplica a la dimensión vertical.\n",
    "\n",
    "Si la zancada es mayor que 1 (en cualquier dirección), entonces el tamaño de salida no será igual al tamaño de entrada, incluso si `padding=\"same\"`. Por ejemplo, si establece `strides=2` (o equivalentemente `strides=(2, 2)`), entonces los mapas de características de salida serán 35 × 60: reducido a la mitad tanto vertical como horizontalmente. La Figura 14-8 muestra lo que sucede cuando `strides=2`, con ambas opciones de relleno.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1407.png)\n",
    "\n",
    "(_Figura 14-7. Las dos opciones de relleno, cuando `strides=1`_)\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1408.png)\n",
    "\n",
    "(_Figure 14-8. With strides greater than 1, the output is much smaller even when using `\"same\"` padding (and `\"valid\"` padding may ignore some inputs_)\n",
    "\n",
    "Si tienes curiosidad, así es como se calcula el tamaño de salida:\n",
    "\n",
    "* Con `padding=\"valid\"`, si el ancho de la entrada es ih, entonces el ancho de salida es igual a **(ih – fh + sh) / sh**, redondeado hacia abajo. Recuerde que fh es el ancho del núcleo y sh es la zancada (stride) horizontal. Cualquier resto en la división corresponde a columnas ignoradas en el lado derecho de la imagen de entrada. Se puede utilizar la misma lógica para calcular la altura de salida y las filas ignoradas en la parte inferior de la imagen.\n",
    "\n",
    "- Con `padding=\"same\"`, el ancho de salida es igual a ih/sh, redondeado hacia arriba. Para que esto sea posible, se rellena el número apropiado de columnas cero a la izquierda y a la derecha de la imagen de entrada (un número igual si es posible, o solo uno más en el lado derecho). Suponiendo que el ancho de salida es ow, entonces el número de columnas rellenas con ceros es **(ow – 1) × sh + fh – ih**. Nuevamente, se puede usar la misma lógica para calcular la altura de salida y el número de filas rellenas.\n",
    "\n",
    "Ahora veamos los pesos de la capa (que se anotaron **wu, v, k′, k** y **bk** en la Ecuación 14-1). Al igual que una capa `Dense`, una capa `Conv2D` contiene todos los pesos de la capa, incluidos los núcleos y los sesgos. Los núcleos se inicializan aleatoriamente, mientras que los sesgos se inicializan a cero. Se puede acceder a estos pesos como variables TF a través del atributo de pesos `waights`, o como matrices NumPy a través del método `get_weights()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a81d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels, biases = conv_layer.get_weights()\n",
    "kernels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f5b30be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890ee3b",
   "metadata": {},
   "source": [
    "La matriz de `kernel` es 4D y su forma es [kernel_height, kernel_width, input_channels, output_channels]. La matriz de sesgos `biases` es 1D, con forma [output_channels]. La cantidad de canales de salida es igual a la cantidad de mapas de características de salida, que también es igual a la cantidad de filtros.\n",
    "\n",
    "Lo más importante es que tenga en cuenta que la altura y el ancho de las imágenes de entrada no aparecen en la forma del núcleo: esto se debe a que todas las neuronas en los mapas de características de salida comparten los mismos pesos, como se explicó anteriormente. Esto significa que puede alimentar imágenes de cualquier tamaño a esta capa, siempre y cuando sean al menos tan grandes como los núcleos, y si tienen el número correcto de canales (tres en este caso).\n",
    "\n",
    "Por último, generalmente querrá especificar una función de activación (como ReLU) al crear una capa `Conv2D`, y también especificar el inicializador del núcleo correspondiente (como la inicialización He). Esto es por la misma razón que para las capas `Dense`: una capa convolucional realiza una operación lineal, por lo que si apilas varias capas convolucionales sin ninguna función de activación, todas serían equivalentes a una sola capa convolucional, y no serían capaces de aprender nada realmente complejo.\n",
    "\n",
    "Como puede ver, las capas convolucionales tienen bastantes hiperparámetros: `filters`, `kernel_size`, `padding`, `strides`, `activation`, `kernel_initializer`, etc. Como siempre, puedes usar la validación cruzada para encontrar los valores de hiperparámetros correctos, pero esto lleva mucho tiempo. Discutiremos las arquitecturas comunes de CNN más adelante en este capítulo, para darle una idea de qué valores de hiperparámetros funcionan mejor en la práctica.\n",
    "\n",
    "\n",
    "## Requisitos de memoria\n",
    "\n",
    "Otro desafío con las CNN es que las capas convolucionales requieren una gran cantidad de RAM. Esto es especialmente cierto durante el entrenamiento, porque el pase inverso de la repropagación requiere todos los valores intermedios calculados durante el paso hacia adelante.\n",
    "\n",
    "Por ejemplo, considere una capa convolucional con 200 filtros de 5 × 5, con paso 1 y mismo `\"same\"` relleno. Si la entrada es una imagen RGB de 150 × 100 (tres canales), entonces el número de parámetros es (5 × 5 × 3 + 1) × 200 = 15 200 (el + 1 corresponde a los términos de sesgo), lo cual es bastante pequeño en comparación. a una capa completamente conectada.⁠ Sin embargo, cada uno de los 200 mapas de características contiene 150 × 100 neuronas, y cada una de estas neuronas necesita calcular una suma ponderada de sus 5 × 5 × 3 = 75 entradas: eso es un total de 225 millones multiplicaciones flotantes. No es tan malo como una capa totalmente conectada, pero sigue siendo bastante intensivo desde el punto de vista computacional. Además, si los mapas de características se representan usando flotantes de 32 bits, entonces la salida de la capa convolucional ocupará 200 × 150 × 100 × 32 = 96 millones de bits (12 MB) de RAM.⁠ Y eso es sólo por un ejemplo: si un El lote de entrenamiento contiene 100 instancias, entonces esta capa utilizará 1,2 GB de RAM.\n",
    "\n",
    "Durante la inferencia (es decir, al hacer una predicción para una nueva instancia), la RAM ocupada por una capa se puede liberar tan pronto como se haya calculado la siguiente capa, por lo que solo se necesita tanta RAM como se requiera por dos capas consecutivas. Pero durante el entrenamiento, todo lo calculado durante el pase hacia adelante debe preservarse para el paso inverso, por lo que la cantidad de RAM necesaria es (al menos) la cantidad total de RAM requerida por todas las capas.\n",
    "\n",
    "\n",
    "#### PROPINA\n",
    "\n",
    "Si el entrenamiento se bloquea debido a un error fuera de memoria, puedes intentar reducir el tamaño del mini lote. Alternativamente, puede intentar reducir la dimensionalidad usando una zancada, eliminando unas pocas capas, usando flotadores de 16 bits en lugar de flotadores de 32 bits, o distribuyendo la CNN en varios dispositivos (verá cómo hacerlo en el capítulo 19).\n",
    "\n",
    "#### ------------------------------------------------------------------------------\n",
    "\n",
    "Ahora echemos un vistazo al segundo bloque de construcción común de las CNN: la capa de agrupación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e91f18",
   "metadata": {},
   "source": [
    "# Capas de agrupación\n",
    "\n",
    "Una vez que entiendes cómo funcionan las capas convolucionales, las capas de agrupación son bastante fáciles de entender. Su objetivo es submuestrear (es decir, reducir) la imagen de entrada para reducir la carga computacional, el uso de la memoria y el número de parámetros (limitando así el riesgo de sobreajuste).\n",
    "\n",
    "Al igual que en las capas convolucionales, cada neurona en una capa de agrupación está conectada a las salidas de un número limitado de neuronas en la capa anterior, ubicada dentro de un pequeño campo receptivo rectangular. Debes definir su tamaño, la zancada y el tipo de relleno, como antes. Sin embargo, una neurona de agrupación no tiene pesos; todo lo que hace es agregar las entradas utilizando una función de agregación como el máximo o la media. La figura 14-9 muestra una capa de agrupación máxima, que es el tipo más común de capa de agrupación. En este ejemplo, utilizamos un núcleo de agrupación de 2 × 2, con un paso de 2 y sin relleno. Solo el valor máximo de entrada en cada campo receptivo llega a la siguiente capa, mientras que las otras entradas se eliminan. Por ejemplo, en el campo receptivo inferior izquierdo de la Figura 14-9, los valores de entrada son 1, 5, 3, 2, por lo que solo el valor máximo, 5, se propaga a la siguiente capa. Debido a la zancada de 2, la imagen de salida tiene la mitad de la altura y la mitad del ancho de la imagen de entrada (redondeada hacia abajo ya que no usamos relleno).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1409.png)\n",
    "\n",
    "(_Figura 14-9. Capa máxima de agrupación (2 × 2 núcleo de agrupación, paso 2, sin relleno)_)\n",
    "\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "Una capa de agrupación normalmente funciona en cada canal de entrada de forma independiente, por lo que la profundidad de salida (es decir, el número de canales) es la misma que la profundidad de entrada.\n",
    "\n",
    "#### -----------------------------------------------------------------------------\n",
    "\n",
    "Además de reducir los cálculos, el uso de la memoria y el número de parámetros, una capa de agrupación máxima también introduce algún nivel de invariancia en las traducciones pequeñas, como se muestra en la Figura 14-10. Aquí asumimos que los píxeles brillantes tienen un valor más bajo que los píxeles oscuros, y consideramos que tres imágenes (A, B, C) pasan por una capa de agrupación máxima con un núcleo de 2 × 2 y paso 2. Las imágenes B y C son las mismas que las imágenes A, pero se desplazan uno y dos píxeles hacia la derecha. Como puede ver, las salidas de la capa de agrupación máxima para las imágenes A y B son idénticas. Esto es lo que significa la invariancia de la traducción. Para la imagen C, la salida es diferente: se desplaza un píxel hacia la derecha (pero todavía hay una invariancia del 50%). Al insertar una capa de agrupación máxima cada pocas capas en una CNN, es posible obtener algún nivel de invariancia de traducción a mayor escala. Además, la agrupación máxima ofrece una pequeña cantidad de invariancia rotacional y una ligera invariancia de escala. Dicha invariancia (incluso si es limitada) puede ser útil en los casos en que la predicción no debería depender de estos detalles, como en las tareas de clasificación.\n",
    "\n",
    "Sin embargo, la agrupación máxima también tiene algunas desventajas. Obviamente es muy destructivo: incluso con un pequeño núcleo de 2 × 2 y un paso de 2, la salida será dos veces más pequeña en ambas direcciones (por lo que su área será cuatro veces más pequeña), simplemente perdiendo el 75 % de los valores de entrada. Y en algunas aplicaciones, la invariancia no es deseable. Tome la segmentación semántica (la tarea de clasificar cada píxel en una imagen de acuerdo con el objeto al que pertenece ese píxel, que exploraremos más adelante en este capítulo): obviamente, si la imagen de entrada se traduce por un píxel a la derecha, la salida también debe traducirse por un píxel a la derecha. El objetivo en este caso es la equivarianza, no la invariancia: un pequeño cambio en las entradas debería conducir a un pequeño cambio correspondiente en la salida.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1410.png)\n",
    "\n",
    "(_Figura 14-10. Invariancia a las traducciones pequeñas_)\n",
    "\n",
    "\n",
    "# Implementación de capas agrupación con Keras\n",
    "\n",
    "\n",
    "El siguiente código crea una capa `MaxPooling2D`, alias `MaxPool2D`, utilizando un kernel 2 × 2. Las zancadas tienen por defecto el tamaño del núcleo, por lo que esta capa usa una zancada de 2 (horizontal y verticalmente). De forma predeterminada, utiliza un relleno `\"valid\"` (es decir, sin ningún relleno):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea43b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8204951",
   "metadata": {},
   "source": [
    "Para crear una capa de agrupación promedio, simplemente use `AveragePooling2D`, aliasAvgPool2D, en lugar de `MaxPool2D`. Como era de esperar, funciona exactamente como una capa de agrupación máxima, excepto que calcula la media en lugar de la máxima. Las capas de agrupación promedio solían ser muy populares, pero la gente ahora utiliza principalmente capas de agrupación máximas, ya que generalmente funcionan mejor. Esto puede parecer sorprendente, ya que calcular la media generalmente pierde menos información que calcular el máximo. Pero, por otro lado, la agrupación máxima conserva solo las características más fuertes, deshaciéndose de todas las que no tienen sentido, por lo que las siguientes capas obtienen una señal más limpia con la que trabajar. Además, la agrupación máxima ofrece una invariancia de traducción más fuerte que la agrupación promedio, y requiere un poco menos de computación.\n",
    "\n",
    "Tenga en cuenta que la agrupación máxima y la agrupación promedio se pueden realizar a lo largo de la dimensión de profundidad en lugar de las dimensiones espaciales, aunque no es tan común. Esto puede permitir que la CNN aprenda a ser invariante a varias características. Por ejemplo, podría aprender múltiples filtros, cada uno detectando una rotación diferente del mismo patrón (como dígitos escritos a mano; ver Figura 14-11), y la capa de agrupación máxima en profundidad garantizaría que la salida sea la misma, independientemente de la rotación. De manera similar, la CNN podría aprender a ser invariante para cualquier cosa: grosor, brillo, sesgo, color, etc.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1411.png)\n",
    "\n",
    "(_Figura 14-11. La agrupación máxima en profundidad puede ayudar a la CNN a aprender a ser invariante (a la rotación en este caso)_)\n",
    "\n",
    "Keras no incluye una capa de agrupación máxima en profundidad, pero no es demasiado difícil implementar una capa personalizada para eso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f095f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)  # shape[-1] is the number of channels\n",
    "        groups = shape[-1] // self.pool_size  # number of channel groups\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c1cc2",
   "metadata": {},
   "source": [
    "Esta capa reforma sus entradas para dividir los canales en grupos del tamaño deseado (`pool_size`), luego usa `tf.reduce_max()` para calcular el máximo de cada grupo. Esta implementación supone que la zancada es igual al tamaño de la piscina, que generalmente es lo que desea. Alternativamente, puede usar la operación `tf.nn.max_pool()` de TensorFlow y envolverla en una capa `Lambda` para usarla dentro de un modelo Keras, pero lamentablemente esta operación no implementa la agrupación profunda para la GPU, solo para la CPU.\n",
    "\n",
    "Un último tipo de capa de agrupación que a menudo verá en las arquitecturas modernas es la capa de agrupación promedio global. Funciona de manera muy diferente: todo lo que hace es calcular la media de cada mapa de características completo (es como una capa de agrupación promedio que utiliza un núcleo de agrupación con las mismas dimensiones espaciales que las entradas). Esto significa que solo emite un solo número por mapa de características y por instancia. Aunque esto es, por supuesto, extremadamente destructivo (la mayor parte de la información en el mapa de características se pierde), puede ser útil justo antes de la capa de salida, como verá más adelante en este capítulo. Para crear una capa de este tipo, simplemente use la clase `GlobalAveragePooling2D`, aliasGlobalAvgPool2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca30fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e38104",
   "metadata": {},
   "source": [
    "Es equivalente a la siguiente capa `Lambda`, que calcula la media sobre las dimensiones espaciales (altura y anchura):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce1cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = tf.keras.layers.Lambda(\n",
    "    lambda X: tf.reduce_mean(X, axis=[1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18188ecb",
   "metadata": {},
   "source": [
    "Por ejemplo, si aplicamos esta capa a las imágenes de entrada, obtenemos la intensidad media de rojo, verde y azul para cada imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59aac7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.64338624, 0.5971759 , 0.5824972 ],\n",
       "       [0.76306933, 0.26011038, 0.10849128]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_pool(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b93a1",
   "metadata": {},
   "source": [
    "Ahora conoces todos los bloques de construcción para crear redes neuronales convolucionales. Veamos cómo montarlos.\n",
    "\n",
    "\n",
    "# Arquitecturas de CNN\n",
    "\n",
    "\n",
    "Las arquitecturas típicas de CNN apilan unas pocas capas convolucionales (cada una generalmente seguida de una capa ReLU), luego una capa de agrupación, luego otras pocas capas convolucionales (+ReLU), luego otra capa de agrupación, y así sucesiva. La imagen se hace cada vez más pequeña a medida que avanza a través de la red, pero también suele ser cada vez más profunda (es decir, con más mapas de características), gracias a las capas convolucionales (ver Figura 14-12). En la parte superior de la pila, se agrega una red neuronal de alimentación regular, compuesta por unas pocas capas completamente conectadas (+ReLU), y la capa final genera la predicción (por ejemplo, una capa softmax que genera probabilidades de clase estimadas).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1412.png)\n",
    "\n",
    "(_Figura 14-12. Arquitectura típica de CNN_)\n",
    "\n",
    "#### PROPINA\n",
    "\n",
    "Un error común es usar núcleos de convolución que son demasiado grandes. Por ejemplo, en lugar de usar una capa convolucional con un núcleo de 5 × 5, apila dos capas con núcleos de 3 × 3: utilizará menos parámetros y requerirá menos cálculos, y por lo general funcionará mejor. Una excepción es para la primera capa convolucional: normalmente puede tener un núcleo grande (por ejemplo, 5 × 5), generalmente con un paso de 2 o más. Esto reducirá la dimensión espacial de la imagen sin perder demasiada información, y dado que la imagen de entrada solo tiene tres canales en general, no será demasiado costosa.\n",
    "\n",
    "#### ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Así es como puede implementar un CNN básico para abordar el conjunto de datos de Fashion MNIST (introducido en el capítulo 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627022b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
    "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ca5ac",
   "metadata": {},
   "source": [
    "Vamos a revisar este código:\n",
    "\n",
    "* Usamos la función `functools.partial()` (introducida en el Capítulo 11) para definir `DefaultConv2D`, que actúa igual que `Conv2D` pero con diferentes argumentos predeterminados: un tamaño de kernel pequeño de 3, `\"same\"` relleno, la función de activación ReLU y su correspondiente Él inicializador.\n",
    "\n",
    "- A continuación, creamos el modelo `Sequential`. Su primera capa es un `DefaultConv2D` con 64 filtros bastante grandes (7 × 7). Utiliza el paso predeterminado de 1 porque las imágenes de entrada no son muy grandes. También establece `input_shape=[28, 28, 1]`, porque las imágenes son de 28 × 28 píxeles, con un solo canal de color (es decir, escala de grises). Cuando cargue el conjunto de datos Fashion MNIST, asegúrese de que cada imagen tenga esta forma: es posible que necesite usar `np.reshape()` o `np.expanddims()` para agregar la dimensión de los canales. Alternativamente, puede usar una capa `Reshape` como primera capa del modelo.\n",
    "\n",
    "* Luego añadimos una capa de agrupación máxima que utiliza el tamaño de grupo predeterminado de 2, por lo que divide cada dimensión espacial por un factor de 2.\n",
    "\n",
    "- Luego repetimos la misma estructura dos veces: dos capas convolucionales seguidas de una capa de agrupación máxima. Para imágenes más grandes, podríamos repetir esta estructura varias veces más. El número de repeticiones es un hiperparámetro que puedes ajustar.\n",
    "\n",
    "* Tenga en cuenta que el número de filtros se duplica a medida que subimos por la CNN hacia la capa de salida (inicialmente es 64, luego 128, luego 256): tiene sentido que crezca, ya que el número de características de bajo nivel a menudo es bastante bajo (por ejemplo, círculos pequeños, líneas horizontales), pero hay muchas formas diferentes de combinarlas en características de nivel superior. Es una práctica común duplicar el número de filtros después de cada capa de agrupación: dado que una capa de agrupación divide cada dimensión espacial por un factor de 2, podemos permitirnos duplicar el número de mapas de características en la siguiente capa sin temor a explotar el número de parámetros, el uso de la memoria o la carga computacional.\n",
    "\n",
    "- La siguiente es la red totalmente conectada, compuesta por dos capas densas ocultas y una capa de salida densa. Dado que es una tarea de clasificación con 10 clases, la capa de salida tiene 10 unidades y utiliza la función de activación softmax. Tenga en cuenta que debemos aplanar las entradas justo antes de la primera capa densa, ya que espera una matriz 1D de características para cada instancia. También añadimos dos capas de abandono, con una tasa de abandono del 50 % cada una, para reducir el sobreajuste.\n",
    "\n",
    "Si compila este modelo utilizando la pérdida `\"sparse_categorical_crossentropy\"` y ajusta el modelo al conjunto de entrenamiento Fashion MNIST, debería alcanzar más del 92 % de precisión en el conjunto de prueba. No es lo último en tecnología, pero es bastante bueno y claramente mucho mejor que lo que logramos con redes densas en el Capítulo 10.\n",
    "\n",
    "A lo largo de los años, se han desarrollado variantes de esta arquitectura fundamental, lo que ha llevado a avances sorprendentes en el campo. Una buena medida de este progreso es la tasa de error en competiciones como el desafío ILSVRC ImageNet. En esta competencia, la tasa de error de los cinco principales para la clasificación de imágenes, es decir, el número de imágenes de prueba para las que las cinco predicciones principales del sistema no incluían la respuesta correcta, cayó de más del 26 % a menos del 2,3 % en solo seis años. Las imágenes son bastante grandes (por ejemplo, 256 píxeles de alto) y hay 1000 clases, algunas de las cuales son realmente sutiles (intenta distinguir 120 razas de perros). Mirar la evolución de las entradas ganadoras es una buena manera de entender cómo funcionan las CNN y cómo progresa la investigación en el aprendizaje profundo.\n",
    "\n",
    "Primero miraremos la arquitectura clásica de LeNet-5 (1998), luego a varios ganadores del desafío ILSVRC: AlexNet (2012), GoogLeNet (2014), ResNet (2015) y SENet (2017). En el camino, también veremos algunas arquitecturas más, incluyendo Xception, ResNeXt, DenseNet, MobileNet, CSPNet y EfficientNet.\n",
    "\n",
    "\n",
    "## LeNet-5\n",
    "\n",
    "La arquitectura LeNet-5⁠ es quizás la arquitectura de CNN más conocida. Como se mencionó anteriormente, fue creado por Yann LeCun en 1998 y se ha utilizado ampliamente para el reconocimiento de dígitos escritos a mano (MNIST). Se compone de las capas que se muestran en la Tabla 14-1.\n",
    "\n",
    "_Tabla 14-1. Arquitectura LeNet-5_\n",
    "\n",
    "| **Capa** | **Tipo**             | **Mapas** | **Tamaño** | **Tamaño.Kernel** | **Paso** | **Activacion** |\n",
    "|----------|----------------------|-----------|------------|-------------------|----------|----------------|\n",
    "| Fuera    | Totalmente conectado | -         | 10         | -                 | -        | RBF            |\n",
    "| F6       | Totalmente conectado | -         | 84         | -                 | -        | Bronceado      |\n",
    "| C5       | Circonvolucion       | 120       | 1x1        | 5x5               | 1        | Bronceado      |\n",
    "| S4       | Agrupacion media     | 16        | 5x5        | 2x2               | 2        | Bronceado      |\n",
    "| C3       | Circonvolucion       | 16        | 10x10      | 5x5               | 1        | Bronceado      |\n",
    "| S2       | Agrupacion media     | 6         | 14x14      | 2x2               | 2        | Bronceado      |\n",
    "| C1       | Circonvolucion       | 6         | 28x28      | 5x5               | 1        | Bronceado      |\n",
    "| Indio    | Entrada              | 1         | 32x32      | -                 | -        | -              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e46a1",
   "metadata": {},
   "source": [
    "Como puede ver, esto se ve bastante similar a nuestro modelo Fashion MNIST: una pila de capas convolucionales y capas de agrupación, seguida de una red densa. Tal vez la principal diferencia con las CNN de clasificación más modernas son las funciones de activación: hoy en día, usaríamos ReLU en lugar de tanh y softmax en lugar de RBF. Había varias otras diferencias menores que realmente no importan mucho, pero en caso de que esté interesado, se enumeran en el cuaderno de este capítulo en https://homl.info/colab3. El sitio web de Yann LeCun también cuenta con excelentes demostraciones de los dígitos de clasificación de LeNet-5.\n",
    "\n",
    "## AlexNet\n",
    "\n",
    "La arquitectura AlexNet CNN⁠ ganó el desafío ILSVRC de 2012 por un gran margen: logró una tasa de error de los cinco primeros del 17 %, mientras que el segundo mejor competidor logró solo el 26 %. AlexaNet fue desarrollado por Alex Krizhevsky (de ahí el nombre), Ilya Sutskever y Geoffrey Hinton. Es similar a LeNet-5, solo que mucho más grande y más profundo, y fue el primero en apilar capas convolucionales directamente una encima de la otra, en lugar de apilar una capa de agrupación encima de cada capa convolucional. La tabla 14-2 presenta esta arquitectura.\n",
    "\n",
    "_Tabla 14-2. Arquitectura AlexNet_\n",
    "\n",
    "\n",
    "| **Capa** | **Tipo**             | **Mapas** | **Tamaño** | **Tamaño.Kernel** | **Paso** | **Rellno** | **Activacion** |\n",
    "|----------|----------------------|-----------|------------|-------------------|----------|------------|----------------|\n",
    "| Fuera    | Totalmente conectado | -         | 1000       | -                 | -        | -          | Softmax        |\n",
    "| F10      | Totalmente conectado | -         | 4096       | -                 | -        | -          | ReLU           |\n",
    "| F9       | Totalmente conectado | -         | 4096       | -                 | -        | -          | ReLU           |\n",
    "| S8       | Agrupacion maxima    | 256       | 6x6        | 3x3               | 2        | `valid`    | -              |\n",
    "| C7       | Circonvolucion       | 256       | 13x13      | 3x3               | 1        | `same`     | ReLU           |\n",
    "| C6       | Circonvolucion       | 384       | 13x13      | 3x3               | 1        | `same`     | ReLu           |\n",
    "| C5       | Circonvolucion       | 384       | 13x13      | 3x3               | 1        | `same`     | ReLU           |\n",
    "| S4       | Agrupacion maxima    | 256       | 13x13      | 3x3               | 2        | `valid`    | -              |\n",
    "| C3       | Circonvolucion       | 256       | 27x27      | 5x5               | 1        | `same`     | ReLU           |\n",
    "| S2       | Agrupacion maxima    | 96        | 27x27      | 3x3               | 2        | `valid`    | -              |\n",
    "| C1       | Circonvolucion       | 96        | 55x55      | 11x11             | 4        | `valid`    | ReLU           |\n",
    "| Indio    | Entrada              | 3 (RGB)   | 227x227    | -                 | -        | -          | -              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e264f67",
   "metadata": {},
   "source": [
    "Para reducir el sobreajuste, los autores utilizaron dos técnicas de regularización. En primer lugar, aplicaron el abandono (introducido en el capítulo 11) con una tasa de abandono del 50 % durante el entrenamiento a los resultados de las capas F9 y F10. En segundo lugar, realizaron un aumento de datos cambiando al azar las imágenes de entrenamiento por varios desplazamientos, volteándolos horizontalmente y cambiando las condiciones de iluminación.\n",
    "\n",
    "\n",
    "#### AUMENTO DE DATOS\n",
    "\n",
    "El aumento de datos aumenta artificialmente el tamaño del conjunto de entrenamiento al generar muchas variantes realistas de cada instancia de entrenamiento. Esto reduce el sobreajuste, lo que lo convierte en una técnica de regularización. Las instancias generadas deben ser lo más realistas posible: idealmente, dada una imagen del conjunto de entrenamiento aumentado, un humano no debería ser capaz de saber si se aumentó o no. El simple hecho de agregar ruido blanco no ayudará; las modificaciones deberían poder aprenderse (el ruido blanco no lo es).\n",
    "\n",
    "Por ejemplo, puede cambiar ligeramente, rotar y cambiar el tamaño de cada imagen del conjunto de entrenamiento en varias cantidades y agregar las imágenes resultantes al conjunto de entrenamiento (ver Figura 14-13). Para ello, puede utilizar las capas de aumento de datos de Keras, introducidas en el capítulo 13 (por ejemplo, `RandomCrop`, `RandomRotation`, etc.). Esto obliga al modelo a ser más tolerante a las variaciones en la posición, la orientación y el tamaño de los objetos de las imágenes. Para producir un modelo que sea más tolerante con las diferentes condiciones de iluminación, de manera similar puede generar muchas imágenes con varios contrastes. En general, también puedes voltear las imágenes horizontalmente (excepto texto y otros objetos asimétricos). Al combinar estas transformaciones, puedes aumentar en gran medida el tamaño de tu conjunto de entrenamiento.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1413.png)\n",
    "\n",
    "(_Figura 14-13. Generar nuevas instancias de entrenamiento a partir de las existentes_)\n",
    "\n",
    "El aumento de datos también es útil cuando tienes un conjunto de datos desequilibrado: puedes usarlo para generar más muestras de las clases menos frecuentes. Esto se llama la técnica de sobremuestreo de minorías sintéticas, o SMOTE para abreviar.\n",
    "\n",
    "#### ---------------------------------------------------------------------------\n",
    "\n",
    "AlexNet también utiliza un paso de normalización competitiva inmediatamente después del paso ReLU de las capas C1 y C3, llamado normalización de respuesta local (LRN): las neuronas más fuertemente activadas inhiben otras neuronas ubicadas en la misma posición en los mapas de características vecinos. Dicha activación competitiva se ha observado en las neuronas biológicas. Esto alienta a los diferentes mapas de características a especializarse, separándolos y obligándolos a explorar una gama más amplia de características, mejorando en última instancia la generalización. La ecuación 14-2 muestra cómo aplicar LRN.\n",
    "\n",
    "### Ecuación 14-2. Normalización de la respuesta local (LRN)\n",
    "\n",
    "<a href=\"https://ibb.co/BNn818R\"><img src=\"https://i.ibb.co/bJbpfpn/Captura-de-pantalla-2024-03-23-a-las-1-37-31.png\" alt=\"Captura-de-pantalla-2024-03-23-a-las-1-37-31\" border=\"0\"></a>\n",
    "\n",
    "En esta ecuación:\n",
    "\n",
    "* **bi** es la salida normalizada de la neurona ubicada en el mapa de características i, en alguna fila u y columna v (tenga en cuenta que en esta ecuación consideramos solo las neuronas ubicadas en esta fila y columna, por lo que u y v no se muestran).\n",
    "\n",
    "- **ai** es la activación de esa neurona después del paso ReLU, pero antes de la normalización.\n",
    "\n",
    "* **k, α, β y r** son hiperparámetros. k se llama sesgo y r se llama radio de profundidad.\n",
    "\n",
    "- fn es el número de mapas de características.\n",
    "\n",
    "Por ejemplo, si r = 2 y una neurona tiene una fuerte activación, inhibirá la activación de las neuronas ubicadas en los mapas de características inmediatamente por encima y por debajo de la suya.\n",
    "\n",
    "En AlexNet, los hiperparámetros se establecen como: r = 5, α = 0,0001, β = 0,75 y k = 2. Puede implementar este paso utilizando la función `tf.nn.local_response_normalization()` (que puede incluir en un `Lambda` capa si desea utilizarla en un modelo Keras).\n",
    "\n",
    "Una variante de AlexNet llamada ZF Net⁠ fue desarrollada por Matthew Zeiler y Rob Fergus y ganó el desafío ILSVRC de 2013. Es esencialmente AlexNet con algunos hiperparámetros ajustados (número de mapas de características, tamaño del núcleo, zancada, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c3998",
   "metadata": {},
   "source": [
    "## GoogLeNet\n",
    "\n",
    "La arquitectura GoogLeNet fue desarrollada por Christian Szegedy et al. de Google Research,⁠ y ganó el desafío ILSVRC 2014 al empujar la tasa de error de los cinco primeros por debajo del 7 %. Esta gran actuación vino en gran parte del hecho de que la red era mucho más profunda que las CNN anteriores (como verás en la Figura 14-15). Esto fue posible gracias a las subredes llamadas módulos de inicio,⁠ que permiten a GoogLeNet usar parámetros de manera mucho más eficiente que las arquitecturas anteriores: GoogLeNet en realidad tiene 10 veces menos parámetros que AlexNet (aproximadamente 6 millones en lugar de 60 millones).\n",
    "\n",
    "La Figura 14-14 muestra la arquitectura de un módulo inicial. La notación “3 × 3 + 1(S)” significa que la capa usa un núcleo de 3 × 3, zancada 1 y el `\"same\"` relleno. La señal de entrada se envía primero a cuatro capas diferentes en paralelo. Todas las capas convolucionales utilizan la función de activación ReLU. Tenga en cuenta que las capas convolucionales superiores utilizan diferentes tamaños de núcleo (1 × 1, 3 × 3 y 5 × 5), lo que les permite capturar patrones a diferentes escalas. También tenga en cuenta que cada capa utiliza un paso de 1 y el `\"same\"` relleno (incluso la capa de agrupación máxima), por lo que todas sus salidas tienen la misma altura y ancho que sus entradas. Esto hace posible concatenar todas las salidas a lo largo de la dimensión de profundidad en la capa de concatenación de profundidad final (es decir, apilar los mapas de características de las cuatro capas convolucionales superiores). Se puede implementar usando la capa `Concatenate` de Keras, usando el eje predeterminado `axis=-1`.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1414.png)\n",
    "\n",
    "(_Figura 14-14. Módulo de inicio_)\n",
    "\n",
    "Puede que te preguntes por qué los módulos de inicio tienen capas convolucionales con núcleos de 1 × 1. Seguramente estas capas no pueden capturar ninguna característica porque solo miran un píxel a la vez, ¿verdad? De hecho, estas capas tienen tres propósitos:\n",
    "\n",
    "- Aunque no pueden capturar patrones espaciales, pueden capturar patrones a lo largo de la dimensión de profundidad (es decir, a través de los canales).\n",
    "\n",
    "+ Están configurados para generar menos mapas de características que sus entradas, por lo que sirven como capas de cuello de botella, lo que significa que reducen la dimensionalidad. Esto reduce el costo computacional y el número de parámetros, acelerando la capacitación y mejorando la generalización.\n",
    "\n",
    "- Cada par de capas convolucionales ([1 × 1, 3 × 3] y [1 × 1, 5 × 5]) actúa como una sola capa convolucional poderosa, capaz de capturar patrones más complejos. Una capa convolucional es equivalente a barrer una capa densa a través de la imagen (en cada ubicación, solo mira un pequeño campo receptivo), y estos pares de capas convolucionales son equivalentes a barrer redes neuronales de dos capas a través de la imagen.\n",
    "\n",
    "En resumen, puedes pensar en todo el módulo de inicio como una capa convolucional con esteroides, capaz de generar mapas de características que capturan patrones complejos a varias escalas.\n",
    "\n",
    "Ahora echemos un vistazo a la arquitectura de GoogLeNet CNN (ver Figura 14-15). El número de mapas de características de salida por cada capa convolucional y cada capa de agrupación se muestra antes del tamaño del núcleo. La arquitectura es tan profunda que tiene que estar representada en tres columnas, pero GoogLeNet es en realidad una pila alta, que incluye nueve módulos de inicio (las cajas con las puntas giratorias). Los seis números en los módulos de inicio representan el número de mapas de características outputdos por cada capa convolucional en el módulo (en el mismo orden que en la Figura 14-14). Tenga en cuenta que todas las capas convolucionales utilizan la función de activación ReLU.\n",
    "\n",
    "Pasemos por esta red:\n",
    "\n",
    "- Las dos primeras capas dividen la altura y el ancho de la imagen por 4 (por lo que su área se divide por 16), para reducir la carga computacional. La primera capa utiliza un gran tamaño de núcleo, 7 × 7, por lo que se conserva gran parte de la información.\n",
    "\n",
    "* Luego, la capa de normalización de la respuesta local garantiza que las capas anteriores aprendan una amplia variedad de características (como se discutió anteriormente).\n",
    "\n",
    "- Siguen dos capas convolucionales, donde la primera actúa como una capa de cuello de botella. Como se mencionó, puedes pensar en este par como una sola capa convolucional más inteligente.\n",
    "\n",
    "* Una vez más, una capa de normalización de la respuesta local garantiza que las capas anteriores capturen una amplia variedad de patrones.\n",
    "\n",
    "- A continuación, una capa de agrupación máxima reduce la altura y el ancho de la imagen en 2, de nuevo para acelerar los cálculos.\n",
    "\n",
    "* Luego viene la columna vertebral de CNN: una pila alta de nueve módulos de inicio, entrelazados con un par de capas de agrupación máxima para reducir la dimensionalidad y acelerar la red.\n",
    "\n",
    "- A continuación, la capa de agrupación promedio global genera la media de cada mapa de características: esto elimina cualquier información espacial restante, lo cual está bien porque no queda mucha información espacial en ese momento. De hecho, normalmente se espera que las imágenes de entrada de GoogLeNet sean de 224 × 224 píxeles, por lo que después de 5 capas de agrupación máximas, cada una dividiendo la altura y el ancho por 2, los mapas de características se reducen a 7 × 7. Además, esta es una tarea de clasificación, no de localización, por lo que no importa dónde esté el objeto. Gracias a la reducción de la dimensionalidad que trae esta capa, no hay necesidad de tener varias capas totalmente conectadas en la parte superior de la CNN (como en AlexNet), y esto reduce considerablemente el número de parámetros en la red y limita el riesgo de sobreajuste.\n",
    "\n",
    "* Las últimas capas se explican por sí mismas: abandono para la regularización, luego una capa totalmente conectada con 1.000 unidades (ya que hay 1.000 clases) y una función de activación softmax para generar probabilidades de clase estimadas.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1415.png)\n",
    "\n",
    "(_Figura 14-15. Arquitectura GoogLeNet_)\n",
    "\n",
    "La arquitectura original de GoogLeNet incluía dos clasificadores auxiliares conectados en la parte superior de los módulos de inicio tercero y sexto. Ambos estaban compuestos por una capa de agrupación promedio, una capa convolucional, dos capas totalmente conectadas y una capa de activación softmax. Durante el entrenamiento, su pérdida (reducida en un 70 %) se añadió a la pérdida general. El objetivo era luchar contra el problema de los gradientes de desaparición y regularizar la red, pero más tarde se demostró que su efecto era relativamente menor.\n",
    "\n",
    "Más tarde, los investigadores de Google propusieron varias variantes de la arquitectura de GoogLeNet, incluyendo Inception-v3 e Inception-v4, utilizando módulos de inicio ligeramente diferentes para alcanzar un rendimiento aún mejor.\n",
    "\n",
    "\n",
    "## VGGNet\n",
    "\n",
    "El subcampeón en el desafío ILSVRC 2014 fue VGGNet,⁠ Karen Simonyan y Andrew Zisserman, del laboratorio de investigación del Grupo de Geometría Visual (VGG) de la Universidad de Oxford, desarrollaron una arquitectura muy simple y clásica; tenía 2 o 3 capas convolucionales y una capa de agrupación, luego de nuevo 2 o 3 capas convolucionales y una capa de agrupación, y así sucesí (alcanzando un total de 16 o 19 capas convolucionales, dependiendo de la variante VGG), además de una red final densa con 2 capas ocultas y la capa de salida. Usó filtros pequeños de 3 × 3, pero tenía muchos de ellos.\n",
    "\n",
    "\n",
    "## ResNet\n",
    "\n",
    "Kaiming He et al. ganaron el desafío ILSVRC 2015 utilizando una Red Residual (ResNet)⁠ que ofreció una asombrosa tasa de error de los cinco primeros por debajo del 3,6 %. La variante ganadora utilizó una CNN extremadamente profunda compuesta por 152 capas (otras variantes tenían 34, 50 y 101 capas). Confirmó la tendencia general: los modelos de visión por ordenador se estaban haciendo cada vez más profundos, con cada vez menos parámetros. La clave para poder entrenar una red tan profunda es utilizar conexiones de salto (también llamadas conexiones de acceso directo): la señal que se alimenta en una capa también se añade a la salida de una capa ubicada más arriba de la pila. Veamos por qué esto es útil.\n",
    "\n",
    "Al entrenar una red neuronal, el objetivo es hacer que modele una función objetivo h(x). Si agrega la entrada x a la salida de la red (es decir, agrega una conexión de omitir), entonces la red se verá obligada a modelar f(x) = h(x) - x en lugar de h(x). Esto se llama aprendizaje residual (ver Figura 14-16).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1416.png)\n",
    "\n",
    "(_Figura 14-16. Aprendizaje residual_)\n",
    "\n",
    "Cuando inicializas una red neuronal regular, sus pesos están cerca de cero, por lo que la red solo genera valores cercanos a cero. Si agrega una conexión de salto, la red resultante solo genera una copia de sus entradas; en otras palabras, inicialmente modela la función de identidad. Si la función de destino está bastante cerca de la función de identidad (que a menudo es el caso), esto acelerará considerablemente el entrenamiento.\n",
    "\n",
    "Además, si agrega muchas conexiones de salto, la red puede comenzar a progresar incluso si varias capas aún no han comenzado a aprender (ver Figura 14-17). Gracias a la omitir las conexiones, la señal puede llegar fácilmente a través de toda la red. La red residual profunda se puede ver como una pila de unidades residuales (RU), donde cada unidad residual es una pequeña red neuronal con una conexión de salto.\n",
    "\n",
    "Ahora veamos la arquitectura de ResNet (consulte la Figura 14-18). Es sorprendentemente simple. Comienza y termina exactamente como GoogLeNet (excepto sin una capa de abandono), y en el medio hay solo una pila muy profunda de unidades residuales. Cada unidad residual se compone de dos capas convolucionales (¡y ninguna capa de agrupación!), con normalización por lotes (BN) y activación ReLU, utilizando núcleos de 3 × 3 y preservando las dimensiones espaciales (zancada 1, `\"same\"` relleno).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1417.png)\n",
    "\n",
    "(_Figura 14-17. Red neuronal profunda regular (izquierda) y red residual profunda (derecha)_)\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1418.png)\n",
    "\n",
    "(_Figura 14-18. Arquitectura ResNet_)\n",
    "\n",
    "Tenga en cuenta que el número de mapas de características se duplica cada pocas unidades residuales, al mismo tiempo que su altura y ancho se reducen a la mitad (usando una capa convolucional con paso 2). Cuando esto sucede, las entradas no se pueden agregar directamente a las salidas de la unidad residual porque no tienen la misma forma (por ejemplo, este problema afecta a la conexión de salto representada por la flecha disionada en la Figura 14-18). Para resolver este problema, las entradas se pasan a través de una capa convolucional 1 × 1 con paso 2 y el número correcto de mapas de características de salida (ver Figura 14-19).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1419.png)\n",
    "\n",
    "(_Figura 14-19. Omitir la conexión al cambiar el tamaño y la profundidad del mapa de características_)\n",
    "\n",
    "Existen diferentes variaciones de la arquitectura, con diferentes números de capas. ResNet-34 es un ResNet con 34 capas (solo contando las capas convolucionales y la capa completamente conectada)⁠ que contiene 3 RU que producen 64 mapas de características, 4 RU con 128 mapas, 6 RU con 256 mapas y 3 RU con 512 mapas. Implementaremos esta arquitectura más adelante en este capítulo.\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "La arquitectura Inception-v4⁠ de Google fusionó las ideas de GoogLeNet y ResNet y logró una tasa de error de los cinco primeros de cerca del 3 % en la clasificación de ImageNet.\n",
    "\n",
    "#### -------------------------------------------------------------------------\n",
    "\n",
    "Las ResNets más profundas que eso, como ResNet-152, utilizan unidades residuales ligeramente diferentes. En lugar de dos capas convolucionales de 3 × 3 con, digamos, 256 mapas de características, utilizan tres capas convolucionales: primero una capa convolucional de 1 × 1 con solo 64 mapas de características (4 × menos), que actúa como una capa de cuello de botella (como ya se ha discutido), luego una capa de 3 × 3 con 64 mapas de características y, finalmente, otra capa convolucional de 1 × 1 con 256 mapas de características (4 veces 64) que restaura la profundidad original. ResNet-152 contiene 3 de estas RU que producen 256 mapas, luego 8 RU con 512 mapas, la friolera de 36 RU con 1.024 mapas y, finalmente, 3 RU con 2.048 mapas.\n",
    "\n",
    "\n",
    "## Xcepción\n",
    "\n",
    "Vale la pena señalar otra variante de la arquitectura GoogLeNet: Xception⁠ (que significa Extreme Inception) fue propuesta en 2016 por François Chollet (el autor de Keras), y superó significativamente a Inception-v3 en una enorme tarea de visión (350 millones de imágenes y 17.000 clases). Al igual que Inception-v4, fusiona las ideas de GoogLeNet y ResNet, pero reemplaza los módulos de inicio con un tipo especial de capa llamada capa de convolución separable por profundidad (o capa de convolución separable por corto⁠20). Estas capas se habían utilizado antes en algunas arquitecturas de CNN, pero no eran tan centrales como en la arquitectura de Xception. Mientras que una capa convolucional regular utiliza filtros que intentan capturar simultáneamente patrones espaciales (por ejemplo, un óvalo) y patrones de canal cruzado (por ejemplo, boca + nariz + ojos = cara), una capa convolucional separable hace la fuerte suposición de que los patrones espaciales y los patrones de canal cruzado se pueden modelar por separado (ver Figura 14-20). Por lo tanto, se compone de dos partes: la primera parte aplica un solo filtro espacial a cada mapa de características de entrada, luego la segunda parte busca exclusivamente patrones de canales cruzados: es solo una capa convolucional regular con filtros de 1 × 1.\n",
    "\n",
    "Dado que las capas convolucionales separables solo tienen un filtro espacial por canal de entrada, debe evitar usarlas después de capas que tienen muy pocos canales, como la capa de entrada (concedido, eso es lo que representa la Figura 14-20, pero es solo para fines ilustrativos). Por esta razón, la arquitectura Xception comienza con 2 capas convolucionales regulares, pero luego el resto de la arquitectura utiliza solo convoluciones separables (34 en total), además de algunas capas de agrupación máxima y las capas finales habituales (una capa de agrupación promedio global y una capa de salida densa).\n",
    "\n",
    "Puede que te preguntes por qué Xception se considera una variante de GoogLeNet, ya que no contiene módulos de inicio en absoluto. Bueno, como se discutió anteriormente, un módulo de inicio contiene capas convolucionales con filtros 1 × 1: estos se ven exclusivamente para patrones entre canales. Sin embargo, las capas convolucionales que se encuentran encima de ellas son capas convolucionales regulares que buscan patrones espaciales y de canal cruzado. Por lo tanto, puede pensar en un módulo de inicio como un intermediario entre una capa convolucional regular (que considera los patrones espaciales y los patrones entre canales conjuntamente) y una capa convolucional separable (que los considera por separado). En la práctica, parece que las capas convolucionales separables a menudo funcionan mejor.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1420.png)\n",
    "\n",
    "(_Figura 14-20. Capa convolucional separable en profundidad_)\n",
    "\n",
    "#### CONSEJO\n",
    "\n",
    "Las capas convolucionales separables utilizan menos parámetros, menos memoria y menos cálculos que las capas convolucionales normales y, a menudo, funcionan mejor. Considere usarlos de forma predeterminada, excepto después de capas con pocos canales (como el canal de entrada). En Keras, simplemente use `SeparableConv2D` en lugar de `Conv2D`: es un reemplazo directo. Keras también ofrece una capa `DepthwiseConv2D` que implementa la primera parte de una capa convolucional separable en profundidad (es decir, aplicando un filtro espacial por mapa de características de entrada).\n",
    "\n",
    "#### -----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## SENet\n",
    "\n",
    "La arquitectura ganadora en el desafío ILSVRC 2017 fue la Red de Extracción y Extracción (SENet). Esta arquitectura extiende las arquitecturas existentes, como las redes de inicio y las ResNets, y aumenta su rendimiento. ¡Esto permitió a SENet ganar la competencia con una asombrosa tasa de error del 2,25% entre los cinco primeros! Las versiones extendidas de las redes de inicio y ResNets se llaman SE-Inception y SE-ResNet, respectivamente. El impulso proviene del hecho de que un SENet agrega una pequeña red neuronal, llamada bloque SE, a cada módulo de inicio o unidad residual en la arquitectura original, como se muestra en la Figura 14-21.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1421.png)\n",
    "\n",
    "(_Figura 14-21. Módulo SE-Inception (izquierda) y unidad SE-ResNet (derecha)_)\n",
    "\n",
    "Un bloque SE analiza la salida de la unidad a la que está conectado, centrándose exclusivamente en la dimensión de profundidad (no busca ningún patrón espacial), y aprende qué características suelen ser más activas juntas. Luego utiliza esta información para recalibrar los mapas de características, como se muestra en la Figura 14-22. Por ejemplo, un bloqueo SE puede aprender que la boca, la nariz y los ojos suelen aparecer juntos en imágenes: si ves una boca y una nariz, debes esperar ver los ojos también. Por lo tanto, si el bloque ve una fuerte activación en los mapas de características de la boca y la nariz, pero solo una activación leve en el mapa de características del ojo, aumentará el mapa de características del ojo (más exactamente, reducirá los mapas de características irrelevantes). Si los ojos se confundieran un poco con otra cosa, esta recalibración del mapa de características ayudará a resolver la ambigüedad.\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1422.png)\n",
    "\n",
    "(_Figura 14-22. Un bloque SE realiza la recalibración del mapa de características_)\n",
    "\n",
    "Un bloque SE se compone de solo tres capas: una capa de agrupación promedio global, una capa densa oculta que utiliza la función de activación ReLU y una capa de salida densa que utiliza la función de activación sigmoide (ver Figura 14-23).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1423.png)\n",
    "\n",
    "(_Figura 14-23. Arquitectura de bloques SE_)\n",
    "\n",
    "Como anteriormente, la capa de agrupación promedio global calcula la activación media para cada mapa de características: por ejemplo, si su entrada contiene 256 mapas de características, producirá 256 números que representan el nivel general de respuesta para cada filtro. La siguiente capa es donde ocurre el \"apretón\": esta capa tiene significativamente menos de 256 neuronas, generalmente 16 veces menos que el número de mapas de características (por ejemplo, 16 neuronas), por lo que los 256 números se comprimen en un pequeño vector (por ejemplo, 16 dimensiones). Esta es una representación vectorial de baja dimensión (es decir, una incrustación) de la distribución de las respuestas de las características. Este paso de cuello de botella obliga al bloque SE a aprender una representación general de las combinaciones de características (veremos este principio en acción de nuevo cuando discutamos los autocodificadores en el capítulo 17). Finalmente, la capa de salida toma la incrustación y genera un vector de recalibración que contiene un número por mapa de características (por ejemplo, 256), cada uno entre 0 y 1. Los mapas de características se multiplican por este vector de recalibración, por lo que las características irrelevantes (con una puntuación de recalibración baja) se reducen, mientras que las características relevantes (con una puntuación de recalibración cercana a 1) se dejan solas.\n",
    "\n",
    "\n",
    "## Otras arquitecturas notables\n",
    "\n",
    "Hay muchas otras arquitecturas de CNN para explorar. Aquí hay una breve descripción de algunos de los más notables:\n",
    "\n",
    "- **ResNeXt⁠**:\n",
    "\n",
    "    ResNeXt mejora las unidades residuales en ResNet. Mientras que las unidades residuales en los mejores modelos de ResNet solo contienen 3 capas convolucionales cada una, las unidades residuales de ResNeXt se componen de muchas pilas paralelas (por ejemplo, 32 pilas), con 3 capas convolucionales cada una. Sin embargo, las dos primeras capas de cada pila solo utilizan unos pocos filtros (por ejemplo, solo cuatro), por lo que el número total de parámetros sigue siendo el mismo que en ResNet. Luego, las salidas de todas las pilas se suman y el resultado se pasa a la siguiente unidad residual (junto con la conexión de omitir).\n",
    "\n",
    "* **DenseNet⁠**:\n",
    "\n",
    "    Una DenseNet se compone de varios bloques densos, cada uno compuesto por unas pocas capas convolucionales densamente conectadas. Esta arquitectura logró una excelente precisión al tiempo que utilizaba comparativamente pocos parámetros. ¿Qué significa \"densamente conectado\"? La salida de cada capa se alimenta como entrada a cada capa después de ella dentro del mismo bloque. Por ejemplo, la capa 4 de un bloque toma como entrada la concatenación en profundidad de las salidas de las capas 1, 2 y 3 en ese bloque. Los bloques densos están separados por unas pocas capas de transición.\n",
    "\n",
    "- **MobileNet⁠**:\n",
    "\n",
    "    Las MobileNets son modelos optimizados diseñados para ser ligeros y rápidos, lo que los hace populares en aplicaciones móviles y web. Se basan en capas convolucionales que se pueden separar en profundidad, como Xception. Los autores propusieron varias variantes, intercambiando un poco de precisión por modelos más rápidos y más pequeños.\n",
    "\n",
    "* **CSPNet:**⁠\n",
    "\n",
    "    Una red parcial de escenario cruzado (CSPNet) es similar a una DenseNet, pero parte de la entrada de cada bloque denso se concatena directamente a la salida de ese bloque, sin pasar por el bloque.\n",
    "\n",
    "- **EfficientNet**⁠:\n",
    "\n",
    "    Podría decirse que EfficientNet es el modelo más importante de esta lista. Los autores propusieron un método para escalar cualquier CNN de manera eficiente, aumentando conjuntamente la profundidad (número de capas), el ancho (número de filtros por capa) y la resolución (tamaño de la imagen de entrada) de una manera basada en principios. Esto se llama escalado compuesto. Usaron la búsqueda de arquitectura neuronal para encontrar una buena arquitectura para una versión reducida de ImageNet (con imágenes más pequeñas y menos), y luego usaron la escala compuesta para crear versiones cada vez más grandes de esta arquitectura. Cuando salieron los modelos de EfficientNet, superaron en gran medida a todos los modelos existentes, en todos los presupuestos de computación, y siguen estando entre los mejores modelos que existen hoy en día.\n",
    "\n",
    "Comprender el método de escalado compuesto de EfficientNet es útil para obtener una comprensión más profunda de las CNN, especialmente si alguna vez necesita escalar una arquitectura de CNN. Se basa en una medida logarítmica del presupuesto de cómputo, señalada φ: si su presupuesto de cómputo se duplica, entonces φ aumenta en 1. En otras palabras, el número de operaciones de coma flotante disponibles para el entrenamiento es proporcional a 2φ. La profundidad, el ancho y la resolución de su arquitectura CNN deben escalar como αφ, βφ y γφ, respectivamente. Los factores α, β y γ deben ser mayores que 1, y α + β2 + γ2 deben estar cerca de 2. Los valores óptimos para estos factores dependen de la arquitectura de la CNN. Para encontrar los valores óptimos para la arquitectura EfficientNet, los autores comenzaron con un pequeño modelo de línea de base (EfficientNetB0), fijo φ = 1 y simplemente realizaron una búsqueda en cuadrícula: encontraron α = 1,2, β = 1.1 y γ = 1.1. Luego utilizaron estos factores para crear varias arquitecturas más grandes, llamadas EfficientNetB1 a EfficientNetB7, para aumentar los valores de φ.\n",
    "\n",
    "\n",
    "## Elegir la arquitectura de CNN adecuada\n",
    "\n",
    "Con tantas arquitecturas de CNN, ¿cómo eliges cuál es la mejor para tu proyecto? Bueno, depende de lo que más te importe: ¿la precisión? ¿El tamaño del modelo (por ejemplo, para la implementación en un dispositivo móvil)? ¿La velocidad de inferencia en la CPU? ¿En GPU? La tabla 14-3 enumera los mejores modelos preentrenados actualmente disponibles en Keras (verás cómo usarlos más adelante en este capítulo), ordenados por tamaño de modelo. Puedes encontrar la lista completa en https://keras.io/api/applications. Para cada modelo, la tabla muestra el nombre de la clase Keras a utilizar (en el paquete tf.keras.applications), el tamaño del modelo en MB, la precisión de validación de la parte superior 1 y la parte superior 5 en el conjunto de datos de ImageNet, el número de parámetros (millones) y el tiempo de inferencia en la CPU y la GPU en ms, utilizando lotes de 32 imágenes en hardware razonablemente potente. Para cada columna, se resalta el mejor valor. Como puede ver, los modelos más grandes son generalmente más precisos, pero no siempre; por ejemplo, EfficientNetB2 supera a InceptionV3 tanto en tamaño como en precisión. Solo mantuve InceptionV3 en la lista porque es casi el doble de rápido que EfficientNetB2 en una CPU. Del mismo modo, InceptionResNetV2 es rápido en una CPU, y ResNet50V2 y ResNet101V2 son increíblemente rápidos en una GPU.\n",
    "\n",
    "(_Tabla 14-3. Modelos preentrenados disponibles en Keras_)\n",
    "\n",
    "| **Nombre de la clase** | **Tamaño (MB)** | **Top-1 acc** | **Top-5 acc** | **Parametros** | **CPU (ms)** | **GPU (ms)** |\n",
    "|------------------------|-----------------|---------------|---------------|----------------|--------------|--------------|\n",
    "| MobileNetV2            | **14**          | 71,3%         | 90,1%         | **3,5M**       | 25,9         | 3,8          |\n",
    "| MobileNet              | 16              | 70,4%         | 89,5%         | 4,3M           | **22,6**     | **3,4**      |\n",
    "| NASNetMobile           | 23              | 74,4%         | 91,9%         | 5,3M           | 27.0         | 6,7          |\n",
    "| EfficientNetB0         | 29              | 77,1%         | 93,3%         | 5,3M           | 46,0         | 4,9          |\n",
    "| EfficientNetB1         | 31              | 79,1%         | 94,4%         | 7,9M           | 60,2         | 5,6          |\n",
    "| EfficientNetB2         | 36              | 80,1%         | 94,9%         | 9,2M           | 80,8         | 6,5          |\n",
    "| EfficientNetB3         | 48              | 81,6%         | 95,7%         | 12,3M          | 140,0        | 8,8          |\n",
    "| EfficientNetB4         | 75              | 82.9%         | 96,4 %        | 19,5M          | 308,3        | 15,1         |\n",
    "| InceptionV3            | 92              | 77,9%         | 93,7%         | 23,9M          | 42,2         | 6,9          |\n",
    "| ResNet50V2             | 98              | 76,0%         | 93,0%         | 25,6M          | 45,6         | 4,4          |\n",
    "| EfficientNetB5         | 118             | 83,6%         | 96,7%         | 30,6M          | 579,2        | 25,3         |\n",
    "| EfficientNetB6         | 166             | 84,0%         | 96,8%         | 43,3M          | 958,1        | 40,4         |\n",
    "| ResNet101V2            | 171             | 77,2%         | 93,8%         | 44,7M          | 72,7         | 5,4          |\n",
    "| InicioResNetV2         | 215             | 80,3%         | 95,3%         | 55,9M          | 130,2        | 10,0         |\n",
    "| EfficientNetB7         | 256             | **84,3%**     | **97,0%**     | 66,7M          | 1578,9       | 61,6         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8da3ba",
   "metadata": {},
   "source": [
    "¡Espero que hayas disfrutado de esta inmersión profunda en las principales arquitecturas de CNN! Ahora veamos cómo implementar uno de ellos usando Keras.\n",
    "\n",
    "\n",
    "# Implementación de un ResNet-34 CNN usando Keras\n",
    "\n",
    "La mayoría de las arquitecturas de CNN descritas hasta ahora se pueden implementar de forma bastante natural usando Keras (aunque generalmente se cargaría una red preentrenada en su lugar, como verá). Para ilustrar el proceso, implementemos un ResNet-34 desde cero con Keras. En primer lugar, crearemos una capa `ResidualUnit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e2b4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                        use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a556a",
   "metadata": {},
   "source": [
    "Como puede ver, este código coincide bastante con la Figura 14-19. En el constructor, creamos todas las capas que necesitaremos: las capas principales son las que están en el lado derecho del diagrama, y las capas de salto son las de la izquierda (solo son necesarias si la zancada es mayor que 1). Luego, en el método `call()`, hacemos que las entradas pasen por las capas principales y las capas de salto (si las hay), y agregamos ambas salidas y aplicamos la función de activación.\n",
    "\n",
    "Ahora podemos construir un ResNet-34 usando un modelo `Sequential`, ya que en realidad es solo una larga secuencia de capas: podemos tratar cada unidad residual como una sola capa ahora que tenemos la clase `ResidualUnit`. El código coincide estrechamente con la Figura 14-18:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe01a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418bd5ed",
   "metadata": {},
   "source": [
    "La única parte complicada de este código es el bucle que agrega las capas `ResidualUnit` al modelo: como se explicó anteriormente, las primeras 3 RU tienen 64 filtros, luego las siguientes 4 RU tienen 128 filtros, y así sucesivamente. En cada iteración, debemos establecer la zancada en 1 cuando el número de filtros es el mismo que en el RU anterior, o en caso contrario lo establecemos en 2; luego agregamos `ResidualUnit` y finalmente actualizamos `prev_filters`.\n",
    "\n",
    "¡Es increíble que en unas 40 líneas de código podamos construir el modelo que ganó el desafío ILSVRC 2015! Esto demuestra tanto la elegancia del modelo ResNet como la expresividad de la API de Keras. La implementación de las otras arquitecturas de CNN es un poco más larga, pero no mucho más difícil. Sin embargo, Keras viene con varias de estas arquitecturas integradas, así que ¿por qué no usarlas en su lugar?\n",
    "\n",
    "# Usando modelos preentrenados de Keras\n",
    "\n",
    "En general, no tendrá que implementar modelos estándar como GoogLeNet o ResNet manualmente, ya que las redes preentrenadas están fácilmente disponibles con una sola línea de código en el paquete thetf `tf.keras.applications`.\n",
    "\n",
    "Por ejemplo, puede cargar el modelo ResNet-50, preentrenado en ImageNet, con la siguiente línea de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3422664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae11b1",
   "metadata": {},
   "source": [
    "¡Eso es todo! Esto creará un modelo ResNet-50 y descargará pesos previamente entrenados en el conjunto de datos de ImageNet. Para usarlo, primero debes asegurarte de que las imágenes tengan el tamaño correcto. Un modelo ResNet-50 espera imágenes de 224 × 224 píxeles (otros modelos pueden esperar otros tamaños, como 299 × 299), así que usemos la capa `Resizing` de Keras (presentada en el Capítulo 13) para cambiar el tamaño de dos imágenes de muestra (después de recortarlas al relación de aspecto objetivo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dfb278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_sample_images()[\"images\"]\n",
    "images_resized = tf.keras.layers.Resizing(height=224, width=224,\n",
    "                                          crop_to_aspect_ratio=True)(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751cae5",
   "metadata": {},
   "source": [
    "Los modelos previamente entrenados suponen que las imágenes se preprocesan de una manera específica. En algunos casos, pueden esperar que las entradas se escale de 0 a 1, o de –1 a 1, y así sucesivamente. Cada modelo proporciona una función `preprocess_input()` que puede utilizar para preprocesar sus imágenes. Estas funciones suponen que los valores de píxeles originales oscilan entre 0 y 255, como es el caso aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a49fae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619750ef",
   "metadata": {},
   "source": [
    "Ahora podemos usar el modelo preentrenado para hacer predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33d81be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = model.predict(inputs)\n",
    "Y_proba.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19c7fe",
   "metadata": {},
   "source": [
    "Como es habitual, la salida Y_proba es una matriz con una fila por imagen y una columna por clase (en este caso, hay 1000 clases). Si desea mostrar las K predicciones principales, incluido el nombre de la clase y la probabilidad estimada de cada clase predicha, utilice la función `decode_predictions()`. Para cada imagen, devuelve una matriz que contiene las K predicciones principales, donde cada predicción se representa como una matriz que contiene el identificador de clase,⁠ su nombre y la puntuación de confianza correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "761d1cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #0\n",
      "  n03877845 - palace       54.69%\n",
      "  n03781244 - monastery    24.72%\n",
      "  n02825657 - bell_cote    18.55%\n",
      "Image #1\n",
      "  n04522168 - vase         32.66%\n",
      "  n11939491 - daisy        17.81%\n",
      "  n03530642 - honeycomb    12.06%\n"
     ]
    }
   ],
   "source": [
    "top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(f\"Image #{image_index}\")\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f\"  {class_id} - {name:12s} {y_proba:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e303d46",
   "metadata": {},
   "source": [
    "La salida se ve así:\n",
    "\n",
    "**Imagen #0 n03877845 - palacio 54,69 % n03781244 - monasterio 24,72 % n02825657 - bell_cote 18.55 % Imagen #1 n04522168 - jarrón 32,66 % n11939491 - margarita 17,81% n03530642 - panal 12,06 %**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7643d40",
   "metadata": {},
   "source": [
    "Las clases correctas son palacio y dalia, por lo que el modelo es correcto para la primera imagen, pero incorrecto para la segunda. Sin embargo, eso se debe a que la dalia no es una de las 1000 clases de ImageNet. Con eso en mente, el jarrón es una suposición razonable (¿tal vez la flor está en un jarrón?), y la margarita tampoco es una mala elección, ya que las dalias y las margaritas son de la misma familia Compositae.\n",
    "\n",
    "Como puedes ver, es muy fácil crear un clasificador de imágenes bastante bueno utilizando un modelo preentrenado. Como viste en la Tabla 14-3, muchos otros modelos de visión están disponibles en `tf.keras.applications`, desde modelos ligeros y rápidos hasta modelos grandes y precisos.\n",
    "\n",
    "Pero, ¿qué pasa si quieres usar un clasificador de imágenes para clases de imágenes que no forman parte de ImageNet? En ese caso, aún puede beneficiarse de los modelos preentrenados al usarlos para realizar el aprendizaje por transferencia.\n",
    "\n",
    "\n",
    "# Modelos preentrenados para el aprendizaje por transferencia\n",
    "\n",
    "\n",
    "Si quieres construir un clasificador de imágenes pero no tienes suficientes datos para entrenarlo desde cero, entonces a menudo es una buena idea reutilizar las capas inferiores de un modelo preentrenado, como discutimos en el capítulo 11. Por ejemplo, entrenemos a un modelo para clasificar imágenes de flores, reutilizando un modelo de Xception preentrenado. En primer lugar, cargaremos el conjunto de datos de flores utilizando los conjuntos de datos de TensorFlow (introducidos en el capítulo 13):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6729cf0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dataset, info \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_flowers\u001b[39m\u001b[38;5;124m\"\u001b[39m, as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m dataset_size \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39msplits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_examples  \u001b[38;5;66;03m# 3670\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples  # 3670\n",
    "class_names = info.features[\"label\"].names  # [\"dandelion\", \"daisy\", ...]\n",
    "n_classes = info.features[\"label\"].num_classes  # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1858b94",
   "metadata": {},
   "source": [
    "Tenga en cuenta que puede obtener información sobre el conjunto de datos configurando `with_info=True`. Aquí obtenemos el tamaño del conjunto de datos y los nombres de las clases. Desafortunadamente, solo hay un conjunto de datos de `\"train\"`, no un conjunto de prueba ni un conjunto de validación, por lo que debemos dividir el conjunto de entrenamiento. Llamemos a `tfds.load()` nuevamente, pero esta vez tomando el primer 10% del conjunto de datos para prueba, el siguiente 15% para validación y el 75% restante para entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9de9258d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_set_raw, valid_set_raw, train_set_raw \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_flowers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     split\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:10\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[10\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:25\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[25\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:]\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf38c01",
   "metadata": {},
   "source": [
    "Los tres conjuntos de datos contienen imágenes individuales. Necesitamos agruparlos, pero primero debemos asegurarnos de que todos tengan el mismo tamaño, o el procesamiento por lotes fallará. Podemos usar una capa de cambio de tamaño para esto. También debemos llamar a la función `tf.keras.applications.xcep⁠tion.preprocess_input()` para preprocesar las imágenes de forma adecuada para el modelo Xception. Por último, también mezclaremos el conjunto de entrenamiento y usaremos la captación previa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28fdeec9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      2\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      3\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mResizing(height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m, crop_to_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLambda(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mxception\u001b[38;5;241m.\u001b[39mpreprocess_input)\n\u001b[1;32m      5\u001b[0m ])\n\u001b[0;32m----> 6\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_set_raw\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m X, y: (preprocess(X), y))\n\u001b[1;32m      7\u001b[0m train_set \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1000\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(batch_size)\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m valid_set \u001b[38;5;241m=\u001b[39m valid_set_raw\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m X, y: (preprocess(X), y))\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_raw' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])\n",
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f9825",
   "metadata": {},
   "source": [
    "Ahora cada lote contiene 32 imágenes, todas ellas de 224 × 224 píxeles, con valores de píxeles que van de -1 a 1. ¡Perfecto!\n",
    "\n",
    "Dado que el conjunto de datos no es muy grande, un poco de aumento de datos sin duda ayudará. Vamos a crear un modelo de aumento de datos que incrustaremos en nuestro modelo final. Durante el entrenamiento, volteará aleatoriamente las imágenes horizontalmente, las rotará un poco y ajustará el contraste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8673805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f64360",
   "metadata": {},
   "source": [
    "#### TIP\n",
    "\n",
    "La clase `tf.keras.preprocessing.image.ImageDataGenerator` facilita cargar imágenes desde el disco y aumentarlas de varias maneras: puede desplazar cada imagen, rotarla, cambiar su escala, voltearla horizontal o verticalmente, cortarla o aplicar cualquier función de transformación que desee. Esto es muy conveniente para proyectos simples. Sin embargo, una canalización tf.data no es mucho más complicada y, en general, es más rápida. Además, si tiene una GPU e incluye las capas de preprocesamiento o aumento de datos dentro de su modelo, se beneficiarán de la aceleración de la GPU durante el entrenamiento.\n",
    "\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "A continuación, carguemos un modelo Xception, previamente entrenado en ImageNet. Excluimos la parte superior de la red configurando `include_top=False`. Esto excluye la capa de agrupación promedio global y la capa de salida densa. Luego agregamos nuestra propia capa de agrupación promedio global (alimentándola con la salida del modelo base), seguida de una capa de salida densa con una unidad por clase, usando la función de activación softmax. Finalmente, envolvemos todo esto en un `Model` Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5391c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                     include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2d60d",
   "metadata": {},
   "source": [
    "Como se explica en el capítulo 11, por lo general es una buena idea congelar los pesos de las capas preentrenadas, al menos al comienzo del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11eed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3b715",
   "metadata": {},
   "source": [
    "#### ADVERTENCIA\n",
    "\n",
    "Dado que nuestro modelo utiliza las capas del modelo base directamente, en lugar del objeto base_model en sí, establecer `base_model.trainable=False` no tendría ningún efecto.\n",
    "\n",
    "#### --------------------------------------------------------------------------------\n",
    "\n",
    "Finalmente, podemos compilar el modelo y comenzar la formación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31f07b",
   "metadata": {},
   "source": [
    "#### ADVERTENCIA\n",
    "\n",
    "Si se está ejecutando en Colab, asegúrese de que el tiempo de ejecución esté utilizando una GPU: seleccione Tiempo de ejecución → \"Cambiar tipo de tiempo de ejecución\", elija \"GPU\" en el menú desplegable \"Acelerador de hardware\" y luego haga clic en Guardar. Es posible entrenar el modelo sin una GPU, pero será terriblemente lento (minutos por época, en lugar de segundos).\n",
    "\n",
    "#### -------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Después de entrenar el modelo durante algunas épocas, su precisión de validación debería alcanzar un poco más del 80 % y luego dejar de mejorar. Esto significa que las capas superiores ahora están bastante bien entrenadas, y estamos listos para descongelar algunas de las capas superiores del modelo base y luego continuar entrenando. Por ejemplo, descondelemos las capas 56 y superiores (ese es el comienzo de la unidad residual 7 de 14, como puede ver si enumera los nombres de las capas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eda059",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2c292",
   "metadata": {},
   "source": [
    "No olvides compilar el modelo cada vez que congeles o descongeles las capas. También asegúrese de utilizar una tasa de aprendizaje mucho más baja para evitar dañar las pesas preentrenadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d9edf",
   "metadata": {},
   "source": [
    "Este modelo debería alcanzar alrededor del 92 % de precisión en el conjunto de pruebas, en solo unos minutos de entrenamiento (con una GPU). Si ajustas los hiperparámetros, bajas la tasa de aprendizaje y entrenas durante bastante más tiempo, deberías ser capaz de alcanzar del 95 % al 97 %. ¡Con eso, puedes empezar a entrenar clasificadores de imágenes increíbles en tus propias imágenes y clases! Pero hay más en la visión por ordenador que solo la clasificación. Por ejemplo, ¿y si también quieres saber dónde está la flor en una foto? Echemos un vistazo a esto ahora.\n",
    "\n",
    "\n",
    "# Clasificación y localización\n",
    "\n",
    "La localización de un objeto en una imagen se puede expresar como una tarea de regresión, como se discute en el Capítulo 10: para predecir un cuadro delimitador alrededor del objeto, un enfoque común es predecir las coordenadas horizontales y verticales del centro del objeto, así como su altura y anchura. Esto significa que tenemos cuatro números que predecir. No requiere mucho cambio en el modelo; solo necesitamos agregar una segunda capa de salida densa con cuatro unidades (normalmente encima de la capa de agrupación promedio global), y se puede entrenar usando la pérdida de MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                     include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = tf.keras.layers.Dense(4)(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input,\n",
    "                       outputs=[class_output, loc_output])\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "              loss_weights=[0.8, 0.2],  # depends on what you care most about\n",
    "              optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7b916",
   "metadata": {},
   "source": [
    "Pero ahora tenemos un problema: el conjunto de datos de flores no tiene cajas delimitadoras alrededor de las flores. Por lo tanto, tenemos que añadirlos nosotros mismos. Esta es a menudo una de las partes más difíciles y costosas de un proyecto de aprendizaje automático: conseguir las etiquetas. Es una buena idea pasar tiempo buscando las herramientas adecuadas. Para anotar imágenes con cuadros delimitadores, es posible que desee utilizar una herramienta de etiquetado de imágenes de código abierto como VGG Image Annotator, LabelImg, OpenLabeler o ImgLab, o tal vez una herramienta comercial como LabelBox o Supervisely. También puede considerar plataformas de crowdsourcing como Amazon Mechanical Turk si tiene un gran número de imágenes para anotar. Sin embargo, es mucho trabajo establecer una plataforma de crowdsourcing, preparar el formulario para que se envíe a los trabajadores, supervisarlos y asegurarse de que la calidad de las cajas delimitadoras que producen sea buena, así que asegúrese de que valga la pena el esfuerzo. Adriana Kovashka et al. escribieron un artículo muy práctico⁠ sobre el crowdsourcing en la visión por ordenador. Te recomiendo que lo revises, incluso si no planeas usar el crowdsourcing. Si solo hay unos pocos cientos o incluso un par de miles de imágenes para etiquetar, y no planea hacer esto con frecuencia, puede ser preferible hacerlo usted mismo: con las herramientas adecuadas, solo tomará unos días, y también obtendrá una mejor comprensión de su conjunto de datos y tarea.\n",
    "\n",
    "Ahora supongamos que has obtenido los cuadros delimitadores para cada imagen en el conjunto de datos de flores (por ahora asumiremos que hay un solo cuadro delimitador por imagen). A continuación, debe crear un conjunto de datos cuyos elementos serán lotes de imágenes preprocesadas junto con sus etiquetas de clase y sus cuadros delimitadores. Cada elemento debe ser una tupla del formulario `(images, (class_labels, bounding_boxes))` ¡Entonces estás listo para entrenar a tu modelo!\n",
    "\n",
    "#### PROPINA\n",
    "\n",
    "Los cuadros delimitadores deben normalizarse de modo que las coordenadas horizontales y verticales, así como la altura y el ancho, vayan de 0 a 1. Además, es común predecir la raíz cuadrada de la altura y el ancho en lugar de la altura y el ancho directamente: de esta manera, un error de 10 píxeles para un cuadro delimitador grande no será penalizado tanto como un error de 10 píxeles para un cuadro delimitador pequeño.\n",
    "\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "El MSE a menudo funciona bastante bien como una función de costo para entrenar el modelo, pero no es una gran métrica para evaluar qué tan bien el modelo puede predecir las cajas delimitadoras. La métrica más común para esto es la intersección sobre unión (IoU): el área de superposición entre la caja delimitadora predicha y la caja delimitadora objetivo, dividida por el área de su unión (ver Figura 14-24). En Keras, es implementado por la clase thetf `tf.keras.metrics.MeanIoU`.\n",
    "\n",
    "Clasificar y localizar un solo objeto es agradable, pero ¿qué pasa si las imágenes contienen varios objetos (como suele ser el caso en el conjunto de datos de flores)?\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1424.png)\n",
    "\n",
    "(_Figura 14-24. Métrica de IoU para cajas delimitadoras_)\n",
    "\n",
    "\n",
    "# Detección de objetos\n",
    "\n",
    "La tarea de clasificar y localizar múltiples objetos en una imagen se llama detección de objetos. Hasta hace unos años, un enfoque común era tomar una CNN que estuviera entrenada para clasificar y localizar un solo objeto aproximadamente centrado en la imagen, luego deslizar esta CNN a través de la imagen y hacer predicciones en cada paso. La CNN fue generalmente entrenada para predecir no solo las probabilidades de clase y un cuadro delimitador, sino también una puntuación de objetividad: esta es la probabilidad estimada de que la imagen contenga un objeto centrado cerca del centro. Esta es una salida de clasificación binaria; puede ser producida por una capa de salida densa con una sola unidad, utilizando la función de activación sigmoide y entrenada utilizando la pérdida de entropía cruzada binaria.\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "En lugar de una puntuación de objetividad, a veces se añadía una clase de \"sin objeto\", pero en general esto no funcionó tan bien: las preguntas \"¿Está presente un objeto?\" y \"¿Qué tipo de objeto es?\" se responde mejor por separado.\n",
    "\n",
    "#### ------------------------------------------------------------------------------\n",
    "\n",
    "Este enfoque de CNN deslizante se ilustra en la Figura 14-25. En este ejemplo, la imagen se cortó en una cuadrícula de 5 × 7, y vemos una CNN, el grueso rectángulo negro, que se desliza por las 3 regiones × 3 y hace predicciones en cada paso.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1425.png)\n",
    "\n",
    "(_Figura 14-25. Detectar múltiples objetos deslizando una CNN a través de la imagen_)\n",
    "\n",
    "En esta cifra, la CNN ya ha hecho predicciones para tres de estas regiones 3 × 3:\n",
    "\n",
    "* Al mirar la región 3 × 3 en la parte superior izquierda (centrada en la celda de cuadrícula de sombra roja ubicada en la segunda fila y la segunda columna), detectó la rosa más a la izquierda. Tenga en cuenta que el cuadro delimitador predicho excede el límite de esta región de 3 × 3. Eso está absolutamente bien: a pesar de que la CNN no pudo ver la parte inferior de la rosa, fue capaz de hacer una suposición razonable sobre dónde podría estar. También predijo las probabilidades de la clase, dando una alta probabilidad a la clase \"rosa\". Por último, predijo una puntuación de objetividad bastante alta, ya que el centro de la caja delimitadora se encuentra dentro de la celda de la cuadrícula central (en esta figura, la puntuación de objetividad está representada por el grosor de la caja delimitadora).\n",
    "\n",
    "- Al mirar la siguiente región de 3 × 3, una celda de cuadrícula a la derecha (centrada en el cuadrado azul sombreado), no detectó ninguna flor centrada en esa región, por lo que predijo una puntuación de objetividad muy baja; por lo tanto, el cuadro delimitador predicho y las probabilidades de clase se pueden ignorar con seguridad. Puedes ver que la caja delimitadora predicha no era buena de todos modos.\n",
    "\n",
    "* finalmente, al mirar la siguiente región de 3 × 3, de nuevo una celda de cuadrícula a la derecha (centrada en la celda verde sombreada), detectó la rosa en la parte superior, aunque no perfectamente: esta rosa no está bien centrada dentro de esta región, por lo que la puntuación de objetividad predicha no fue muy alta.\n",
    "\n",
    "Puedes imaginar cómo deslizar la CNN a través de toda la imagen te daría un total de 15 cajas delimitadoras predichas, organizadas en una cuadrícula de 3 × 5, con cada caja delimitadora acompañada de sus probabilidades de clase estimadas y su puntuación de objetividad. Dado que los objetos pueden tener diferentes tamaños, es posible que también desee deslizar la CNN de nuevo a través de regiones más grandes de 4 × 4, para obtener aún más cajas delimitadoras.\n",
    "\n",
    "Esta técnica es bastante sencilla, pero como puedes ver, a menudo detecta el mismo objeto varias veces, en posiciones ligeramente diferentes. Se necesita algo de procesamiento posterior para deshacerse de todas las cajas delimitadoras innecesarias. Un enfoque común para esto se llama supresión no máxima. Así es como funciona:\n",
    "\n",
    "1. En primer lugar, deshagase de todas las cajas delimitadoras para las que la puntuación de objetividad está por debajo de algún umbral: dado que la CNN cree que no hay objeto en ese lugar, la caja delimitadora es inútil.\n",
    "\n",
    "2. Encuentre el cuadro delimitador restante con la puntuación de objetividad más alta y deshágase de todos los demás cuadros delimitadores restantes que se superponen mucho con él (por ejemplo, con un IOU superior al 60 %). Por ejemplo, en la Figura 14-25, el cuadro delimitador con la puntuación máxima de objetividad es el cuadro delimitador grueso sobre la rosa más a la izquierda. La otra caja delimitadora que toca esta misma rosa se superpone mucho con la caja delimitadora máxima, por lo que nos desharemos de ella (aunque en este ejemplo ya se habría eliminado en el paso anterior).\n",
    "\n",
    "3. Repita el paso 2 hasta que no haya más cajas delimitadoras de las que deshacerse.\n",
    "\n",
    "Este enfoque simple para la detección de objetos funciona bastante bien, pero requiere ejecutar la CNN muchas veces (15 veces en este ejemplo), por lo que es bastante lento. Afortunadamente, hay una forma mucho más rápida de deslizar una CNN a través de una imagen: usar una red totalmente convolucional (FCN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfa77c",
   "metadata": {},
   "source": [
    "## Redes totalmente convolucionales\n",
    "\n",
    "La idea de las FCN se introdujo por primera vez en un documento de 2015⁠ por Jonathan Long et al., para la segmentación semántica (la tarea de clasificar cada píxel en una imagen de acuerdo con la clase del objeto al que pertenece). Los autores señalaron que se podrían reemplazar las capas densas en la parte superior de una CNN con capas convolucionales. Para entender esto, veamos un ejemplo: supongamos que una capa densa con 200 neuronas se encuentra encima de una capa convolucional que genera 100 mapas de características, cada uno de tamaño 7 × 7 (este es el tamaño del mapa de características, no el tamaño del núcleo). Cada neurona calculará una suma ponderada de todas las 100 × 7 × 7 activaciones de la capa convolucional (más un término de sesgo). Ahora veamos qué pasa si reemplazamos la capa densa con una capa convolucional usando 200 filtros, cada uno de tamaño 7 × 7, y con un relleno `\"valid\"` Esta capa mostrará 200 mapas de características, cada uno 1 × 1 (ya que el núcleo es exactamente del tamaño de los mapas de características de entrada y estamos utilizando un relleno `\"valid\"`\"). En otras palabras, producirá 200 números, al igual que lo hizo la capa densa; y si observas de cerca los cálculos realizados por una capa convolucional, notarás que estos números serán exactamente los mismos que los producidos por la capa densa. La única diferencia es que la salida de la capa densa fue un tensor de forma [tamaño del lote, 200], mientras que la capa convolucional producirá un tensor de forma [tamaño del lote, 1, 1, 200].\n",
    "\n",
    "#### TIP\n",
    "\n",
    "Para convertir una capa densa en una capa convolucional, el número de filtros en la capa convolucional debe ser igual al número de unidades en la capa densa, el tamaño del filtro debe ser igual al tamaño de los mapas de características de entrada y debe usar un relleno `\"valid\"` La zancada puede establecerse en 1 o más, como verás en breve.\n",
    "\n",
    "#### --------------------------------------------------------------------------------\n",
    "\n",
    "¿Por qué es importante esto? Bueno, mientras que una capa densa espera un tamaño de entrada específico (ya que tiene un peso por función de entrada), una capa convolucional procesará felizmente imágenes de cualquier tamaño (sin embargo, espera que sus entradas tengan un número específico de canales, ya que cada núcleo contiene un conjunto diferente de pesos para cada canal de entrada). Dado que un FCN contiene solo capas convolucionales (y capas de agrupación, que tienen la misma propiedad), ¡se puede entrenar y ejecutar en imágenes de cualquier tamaño!\n",
    "\n",
    "Por ejemplo, supongamos que ya habíamos entrenado a una CNN para la clasificación y localización de flores. Fue entrenado en 224 × 224 imágenes, y genera 10 números:\n",
    "\n",
    "* Las salidas de 0 a 4 se envían a través de la función de activación softmax, y esto da las probabilidades de la clase (una por clase).\n",
    "\n",
    "- La salida 5 se envía a través de la función de activación sigmoide, y esto da la puntuación de objetividad.\n",
    "\n",
    "* Las salidas 6 y 7 representan las coordenadas centrales de la caja delimitadora; también pasan por una función de activación sigmoide para garantizar que van de 0 a 1.\n",
    "\n",
    "- Por último, las salidas 8 y 9 representan la altura y el ancho de la caja delimitadora; no pasan por ninguna función de activación para permitir que las cajas delimitadoras se extiendan más allá de los bordes de la imagen.\n",
    "\n",
    "Ahora podemos convertir las capas densas de la CNN en capas convolucionales. De hecho, ni siquiera necesitamos volver a entrenarlo; ¡solo podemos copiar los pesos de las capas densas a las capas convolucionales! Alternativamente, podríamos haber convertido la CNN en una FCN antes de entrenar.\n",
    "\n",
    "Ahora supongamos que la última capa convolucional antes de la capa de salida (también llamada capa de cuello de botella) genera mapas de características de 7 × 7 cuando la red recibe una imagen de 224 × 224 (consulte el lado izquierdo de la Figura 14-26). Si alimentamos al FCN con una imagen de 448 × 448 (consulte el lado derecho de la Figura 14-26), la capa de cuello de botella ahora generará mapas de características de 14 × 14.⁠32 Dado que la capa de salida densa fue reemplazada por una capa convolucional que usa 10 filtros de tamaño 7 × 7, con relleno `\"valid\"` y zancada 1, la salida estará compuesta por 10 mapas de características, cada uno de tamaño 8 × 8 (ya que 14 – 7 + 1 = 8). En otras palabras, el FCN procesará la imagen completa solo una vez y generará una cuadrícula de 8 × 8 donde cada celda contiene 10 números (5 probabilidades de clase, 1 puntuación de objetividad y 4 coordenadas del cuadro delimitador). Es exactamente como tomar la CNN original y deslizarla por la imagen usando 8 pasos por fila y 8 pasos por columna. Para visualizar esto, imagine cortar la imagen original en una cuadrícula de 14 × 14 y luego deslizar una ventana de 7 × 7 a través de esta cuadrícula; Habrá 8 × 8 = 64 ubicaciones posibles para la ventana, por lo tanto, predicciones de 8 × 8. Sin embargo, el enfoque FCN es mucho más eficiente, ya que la red sólo mira la imagen una vez. De hecho, You Only Look Once (YOLO) es el nombre de una arquitectura de detección de objetos muy popular, que veremos a continuación.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1426.png)\n",
    "\n",
    "(_Figura 14-26. La misma red totalmente convolucional que procesa una imagen pequeña (izquierda) y una grande (derecha)_)\n",
    "\n",
    "## Solo Miras Una Vez (You Only Look Once)\n",
    "\n",
    "YOLO es una arquitectura de detección de objetos rápida y precisa propuesta por Joseph Redmon et al. en un artículo de 2015. Es tan rápida que puede ejecutarse en tiempo real en un vídeo, como se ve en la demostración de Redmon. La arquitectura de YOLO es bastante similar a la que acabamos de discutir, pero con algunas diferencias importantes:\n",
    "\n",
    "* Para cada celda de cuadrícula, YOLO solo considera los objetos cuyo centro de la caja delimitador se encuentra dentro de esa celda. Las coordenadas de la caja delimitadora son relativas a esa celda, donde (0, 0) significa la esquina superior izquierda de la celda y (1, 1) significa la parte inferior derecha. Sin embargo, la altura y el ancho de la caja delimitadora pueden extenderse mucho más allá de la celda.\n",
    "\n",
    "- Emite dos cuadros delimitadores para cada celda de cuadrícula (en lugar de solo una), lo que permite al modelo manejar casos en los que dos objetos están tan cerca el uno del otro que sus centros de cuadros delimitadores se encuentran dentro de la misma celda. Cada cuadro delimitador también viene con su propia puntuación de objetividad.\n",
    "\n",
    "* YOLO también produce una distribución de probabilidad de clase para cada celda de cuadrícula, prediciendo 20 probabilidades de clase por celda de cuadrícula desde que YOLO fue entrenado en el conjunto de datos PASCAL VOC, que contiene 20 clases. Esto produce un mapa de probabilidad de clase gruesa. Tenga en cuenta que el modelo predice una distribución de probabilidad de clase por celda de cuadrícula, no por cuadro delimitador. Sin embargo, es posible estimar las probabilidades de clase para cada cuadro delimitador durante el postprocesamiento, midiendo qué tan bien cada cuadro delimitador coincide con cada clase en el mapa de probabilidad de clase. Por ejemplo, imagina una foto de una persona de pie frente a un coche. Habrá dos cajas delimitadoras: una horizontal grande para el coche y una vertical más pequeña para la persona. Estos cuadros delimitadores pueden tener sus centros dentro de la misma celda de la cuadrícula. Entonces, ¿cómo podemos saber qué clase se debe asignar a cada cuadro delimitador? Bueno, el mapa de probabilidad de clase contendrá una gran región donde la clase de \"coche\" es dominante, y en su interior habrá una región más pequeña donde la clase de \"persona\" es dominante. Con suerte, el cuadro delimitador del coche coincidirá aproximadamente con la región del \"coche\", mientras que el cuadro delimitador de la persona coincidirá aproximadamente con la región de la \"persona\": esto permitirá asignar la clase correcta a cada cuadro delimitador.\n",
    "\n",
    "YOLO se desarrolló originalmente utilizando Darknet, un marco de aprendizaje profundo de código abierto desarrollado inicialmente en C por Joseph Redmon, pero pronto fue portado a TensorFlow, Keras, PyTorch y más. Se ha mejorado continuamente a lo largo de los años, con YOLOv2, YOLOv3 y YOLO9000 (de nuevo por Joseph Redmon et al.), YOLOv4 (por Alexey Bochkovskiy et al.), YOLOv5 (por Glenn Jocher) y PP-YOLO (por Xiang Long et al.).\n",
    "\n",
    "Cada versión trajo algunas mejoras impresionantes en velocidad y precisión, utilizando una variedad de técnicas; por ejemplo, YOLOv3 aumentó la precisión en parte gracias a los antecedentes de anclaje, explotando el hecho de que algunas formas de cajas delimitadoras son más probables que otras, dependiendo de la clase (por ejemplo, las personas tienden a tener cajas delimitadoras verticales, mientras que los coches generalmente no lo tienen). También aumentaron el número de cajas delimitadoras por celda de cuadrícula, se entrenaron en diferentes conjuntos de datos con muchas más clases (hasta 9.000 clases organizadas en una jerarquía en el caso de YOLO9000), agregaron conexiones de salto para recuperar parte de la resolución espacial que se pierde en la CNN (lo discutiremos en breve, cuando veamos la segmentación semántica), y mucho más. También hay muchas variantes de estos modelos, como YOLOv4-tiny, que está optimizado para ser entrenado en máquinas menos potentes y que puede funcionar extremadamente rápido (¡a más de 1000 fotogramas por segundo!), pero con una precisión media (mAP) ligeramente más baja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ea8dd",
   "metadata": {},
   "source": [
    "#### PRECISIÓN MEDIA\n",
    "\n",
    "Una métrica muy común utilizada en las tareas de detección de objetos es la precisión media. \"Promedio medio\" suena un poco redundante, ¿verdad? Para entender esta métrica, volvamos a las dos métricas de clasificación que discutimos en el Capítulo 3: precisión y recuperación. Recuerda la compensación: cuanto mayor sea la retirada, menor será la precisión. Puedes visualizar esto en una curva de precisión/recuerdo (ver Figura 3-6). Para resumir esta curva en un solo número, podríamos calcular su área bajo la curva (AUC). Pero tenga en cuenta que la curva de precisión/recuerdado puede contener algunas secciones en las que la precisión realmente aumenta cuando aumenta la recuperación, especialmente a valores de recuperación bajos (puede ver esto en la parte superior izquierda de la Figura 3-6). Esta es una de las motivaciones de la métrica mAP.\n",
    "\n",
    "Supongamos que el clasificador tiene un 90 % de precisión al 10 % de retirada, pero un 96 % de precisión al 20 % de retirada. Realmente no hay compensación aquí: simplemente tiene más sentido usar el clasificador a un 20 % de retirada en lugar de al 10 % de retirada, ya que obtendrá una mayor recuperación y una mayor precisión. Así que en lugar de mirar la precisión al 10% de retirada, realmente deberíamos estar mirando la máxima precisión que el clasificador puede ofrecer con al menos un 10% de recuperación. Sería el 96 %, no el 90 %. Por lo tanto, una forma de tener una idea justa del rendimiento del modelo es calcular la máxima precisión que puede obtener con al menos un 0 % de recuperación, luego un 10 % de recuperación, un 20 %, y así suceso, hasta el 100 %, y luego calcular la media de estas precisiones máximas. Esto se llama la métrica de precisión media (AP). Ahora, cuando hay más de dos clases, podemos calcular el AP para cada clase y luego calcular el AP medio (mAP). ¡Eso es todo!\n",
    "\n",
    "En un sistema de detección de objetos, hay un nivel adicional de complejidad: ¿qué pasaría si el sistema detectara la clase correcta, pero en la ubicación incorrecta (es decir, el cuadro delimitador está completamente desactivado)? Seguramente no deberíamos contar esto como una predicción positiva. Un enfoque es definir un umbral de IoU: por ejemplo, podemos considerar que una predicción es correcta solo si el IoU es mayor que, digamos, 0,5, y la clase predicha es correcta. El mAP correspondiente generalmente se indica mAP@0.5 (o mAP@50%, o a veces solo AP50). En algunas competiciones (como el desafío PASCAL VOC), esto es lo que se hace. En otros (como la competencia COCO), el mAP se calcula para diferentes umbrales de IoU (0,50, 0,55, 0,60, ... , 0,95), y la métrica final es la media de todos estos mAP (anotados mAP@[.50:.95] o mAP@[.50:0.05:.95]). Sí, ese es un promedio medio.\n",
    "\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "Muchos modelos de detección de objetos están disponibles en TensorFlow Hub, a menudo con pesos preentrenados, como YOLOv5, ⁠SSD, ⁠Faster R-CNN, y EfficentDet.\n",
    "\n",
    "SSD y EfficientDet son modelos de detección de \"mira una vez\", similares a YOLO. EfficientDet se basa en la arquitectura convolucional de EfficientNet. La R-CNN más rápida es más compleja: la imagen primero pasa por una CNN, luego la salida se pasa a una red de propuesta de región (RPN) que propone cuadros delimitadores que tienen más probabilidades de contener un objeto; luego se ejecuta un clasificador para cada cuadro delimitador, basado en la salida recortada de la CNN. El mejor lugar para empezar a usar estos modelos es el excelente tutorial de detección de objetos de TensorFlow Hub.\n",
    "\n",
    "Hasta ahora, solo hemos considerado la detección de objetos en imágenes individuales. Pero, ¿qué pasa con los vídeos? Los objetos no solo deben detectarse en cada fotograma, sino que también deben rastrearse a lo largo del tiempo. Echemos un vistazo rápido al seguimiento de objetos ahora.\n",
    "\n",
    "# Seguimiento de objetos\n",
    "\n",
    "El seguimiento de objetos es una tarea desafiante: los objetos se mueven, pueden crecer o encogerse a medida que se acercan o se alejan más de la cámara, su apariencia puede cambiar a medida que se dan la vuelta o se mueven a diferentes condiciones de iluminación o fondos, pueden estar ocluidos temporalmente por otros objetos, etc.\n",
    "\n",
    "Uno de los sistemas de seguimiento de objetos más populares es DeepSORT.⁠ Se basa en una combinación de algoritmos clásicos y aprendizaje profundo:\n",
    "\n",
    "* Utiliza filtros de Kalman para estimar la posición actual más probable de un objeto dadas las detecciones anteriores, y asumiendo que los objetos tienden a moverse a una velocidad constante.\n",
    "\n",
    "- Utiliza un modelo de aprendizaje profundo para medir el parecido entre las nuevas detecciones y los objetos rastreados existentes.\n",
    "\n",
    "* Por último, utiliza el algoritmo húngaro para asignar nuevas detecciones a objetos rastreados existentes (o a nuevos objetos rastreados): este algoritmo encuentra de manera eficiente la combinación de asignaciones que minimiza la distancia entre las detecciones y las posiciones predichas de los objetos rastreados, al tiempo que minimiza la discrepancia de apariencia.\n",
    "\n",
    "Por ejemplo, imagina una bola roja que acaba de rebotar en una bola azul que viaja en la dirección opuesta. Basado en las posiciones anteriores de las bolas, el filtro Kalman predecirá que las bolas se atravesarán entre sí: de hecho, asume que los objetos se mueven a una velocidad constante, por lo que no esperará el rebote. Si el algoritmo húngaro solo considerara las posiciones, entonces felizmente mapearía las nuevas detecciones a las bolas equivocadas, como si se hubieran atravesado entre sí e intercambiado colores. Pero gracias a la medida de semejanza, el algoritmo húngaro notará el problema. Suponiendo que las bolas no sean demasiado similares, el algoritmo asignará las nuevas detecciones a las bolas correctas.\n",
    "\n",
    "#### TIP \n",
    "\n",
    "Hay algunas implementaciones de DeepSORT disponibles en GitHub, incluida una implementación de TensorFlow de YOLOv4 + DeepSORT: https://github.com/theAIGuysCode/yolov4-deepsort.\n",
    "\n",
    "#### -----------------------------------------------------------------------------\n",
    "\n",
    "Hasta ahora hemos localizado objetos usando cajas delimitadoras. Esto a menudo es suficiente, pero a veces necesitas localizar objetos con mucha más precisión, por ejemplo, para eliminar el fondo detrás de una persona durante una videoconferencia. Veamos cómo bajar al nivel de píxeles.\n",
    "\n",
    "\n",
    "# Segmentación semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb32cd",
   "metadata": {},
   "source": [
    "En la segmentación semántica, cada píxel se clasifica de acuerdo con la clase del objeto al que pertenece (por ejemplo, carretera, coche, peatón, edificio, etc.), como se muestra en la Figura 14-27. Tenga en cuenta que no se distinguen diferentes objetos de la misma clase. Por ejemplo, todas las bicicletas en el lado derecho de la imagen segmentada terminan como un gran trozo de píxeles. La principal dificultad en esta tarea es que cuando las imágenes pasan por una CNN regular, pierden gradualmente su resolución espacial (debido a las capas con pasos mayores de 1); por lo tanto, una CNN regular puede terminar sabiendo que hay una persona en algún lugar de la parte inferior izquierda de la imagen, pero no será mucho más precisa que eso.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1427.png)\n",
    "\n",
    "(_Figura 14-27. Segmentación semántica_)\n",
    "\n",
    "Al igual que con la detección de objetos, hay muchos enfoques diferentes para abordar este problema, algunos bastante complejos. Sin embargo, se propuso una solución bastante simple en el documento de 2015 de Jonathan Long et al. Mencioné antes, en redes totalmente convolucionales. Los autores comienzan tomando una CNN preentrenada y convirtiéndolo en una FCN. La CNN aplica una zancada general de 32 a la imagen de entrada (es decir, si suma todos los pasos mayores que 1), lo que significa que los mapas de características de salida de la última capa son 32 veces más pequeños que la imagen de entrada. Esto es claramente demasiado grueso, por lo que añadieron una sola capa de muestreo que multiplica la resolución por 32.\n",
    "\n",
    "Hay varias soluciones disponibles para el muestreo adicional (aumentando el tamaño de una imagen), como la interpolación bilineal, pero eso solo funciona razonablemente bien hasta ×4 o ×8. En su lugar, utilizan una capa convolucional transpuesta: esto equivale a estirar primero la imagen insertando filas y columnas vacías (llenas de ceros), y luego realizando una convolución regular (ver Figura 14-28). Alternativamente, algunas personas prefieren pensar en ello como una capa convolucional regular que utiliza pasos fraccionarios (por ejemplo, el paso es 1/2 en la Figura 14-28). La capa convolucional transpuesta se puede inicializar para realizar algo parecido a la interpolación lineal, pero como es una capa que se puede entrenar, aprenderá a hacerlo mejor durante el entrenamiento. En Keras, puedes usar la capa `Conv2DTranspose`.\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "En una capa convolucional transpuesta, la zancada define cuánto se estirará la entrada, no el tamaño de los pasos del filtro, por lo que cuanto más grande sea la zancada, mayor será la salida (a diferencia de las capas convolucionales o las capas de agrupación).\n",
    "\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1428.png)\n",
    "\n",
    "(_Figura 14-28. Muestreo utilizando una capa convolucional transpuesta_)\n",
    "\n",
    "#### OTRAS CAPAS CONVOLUCIONALES DE KERAS\n",
    "\n",
    "Keras también ofrece algunos otros tipos de capas convolucionales:\n",
    "\n",
    "`tf.keras.layers.Conv1D`\n",
    "\n",
    "    Una capa convolucional para entradas 1D, como series temporales o texto (secuencias de letras o palabras), como verá en el capítulo 15.\n",
    "\n",
    "`tf.keras.layers.Conv3D`\n",
    "\n",
    "    Una capa convolucional para entradas 3D, como escaneos de PET 3D.\n",
    "\n",
    "`dilation_rate`\n",
    "\n",
    "    Establecer el hiperparámetro `dilation_rate` de cualquier capa convolucional en un valor de 2 o más crea una capa convolucional à-trous (“à trous” en francés significa “con agujeros”). Esto equivale a utilizar una capa convolucional regular con un filtro dilatado insertando filas y columnas de ceros (es decir, agujeros). Por ejemplo, un filtro de 1 × 3 igual a `[[1,2,3]]` puede dilatarse con una velocidad de dilatación de 4, lo que da como resultado un filtro dilatado de `[[1, 0, 0, 0, 2, 0, 0 , 0, 3]]`. Esto permite que la capa convolucional tenga un campo receptivo más grande sin costo computacional y sin utilizar parámetros adicionales.\n",
    "\n",
    "#### ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "El uso de capas convolucionales transpuestas para el muestreo está bien, pero sigue siendo demasiado impreciso. Para hacerlo mejor, Long et al. añadieron conexiones de omitir de capas inferiores: por ejemplo, muestrearon la imagen de salida por un factor de 2 (en lugar de 32), y añadieron la salida de una capa inferior que tenía esta doble resolución. Luego muestrearon el resultado en un factor de 16, lo que llevó a un factor de muestreo total de 32 (ver Figura 14-29). Esto recuperó parte de la resolución espacial que se perdió en capas de agrupación anteriores. En su mejor arquitectura, utilizaron una segunda conexión de salto similar para recuperar detalles aún más finos de una capa aún más baja. En resumen, la salida de la CNN original pasa por los siguientes pasos adicionales: aumentar la muestra × 2, agregar la salida de una capa inferior (de la escala apropiada), aumentar la muestra ×2, agregar la salida de una capa aún más baja y, finalmente, aumentar la muestra ×8. Incluso es posible escalar más allá del tamaño de la imagen original: esto se puede utilizar para aumentar la resolución de una imagen, que es una técnica llamada superresolución.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1429.png)\n",
    "\n",
    "(_Figura 14-29. Omitir capas, recuperar cierta resolución espacial de las capas inferiores_)\n",
    "\n",
    "La segmentación de instancias es similar a la segmentación semántica, pero en lugar de fusionar todos los objetos de la misma clase en un gran bulto, cada objeto se distingue de los demás (por ejemplo, identifica cada bicicleta individual). Por ejemplo, la arquitectura Mask R-CNN, propuesta en un documento de 2017⁠ de Kaiming He et al., amplía el modelo Faster R-CNN produciendo además una máscara de píxeles para cada cuadro delimitador. Por lo tanto, no solo obtienes un cuadro delimitador alrededor de cada objeto, con un conjunto de probabilidades de clase estimadas, sino que también obtienes una máscara de píxeles que localiza los píxeles en el cuadro delimitador que pertenecen al objeto. Este modelo está disponible en TensorFlow Hub, preentrenado en el conjunto de datos COCO 2017. Sin embargo, el campo se está moviendo rápido, así que si quieres probar los últimos y mejores modelos, echa un vistazo a la sección de última generación de https://paperswithcode.com.\n",
    "\n",
    "Como puede ver, el campo de la visión profunda por ordenador es vasto y de ritmo rápido, con todo tipo de arquitecturas que aparecen cada año. Casi todos ellos se basan en redes neuronales convolucionales, pero desde 2020 otra arquitectura de red neuronal ha entrado en el espacio de la visión por ordenador: los transformadores (que discutiremos en el capítulo 16). El progreso realizado en la última década ha sido asombroso, y los investigadores ahora se están centrando en problemas cada vez más difíciles, como el aprendizaje contradictorio (que intenta hacer que la red sea más resistente a las imágenes diseñadas para engañarla), la explicabilidad (comprender por qué la red hace una clasificación específica), la generación de imágenes realistas (a la que volveremos en el capítulo 17), el aprendizaje de un solo disparo (un sistema que puede reconocer un objeto después de haberlo visto solo una vez), la predicción de los próximos fotogramas en un video, la combinación de tareas de texto e imagen, y más.\n",
    "\n",
    "Ahora pasemos al siguiente capítulo, donde veremos cómo procesar datos secuenciales como series temporales utilizando redes neuronales recurrentes y redes neuronales convolucionales.\n",
    "\n",
    "\n",
    "# Ejercicios\n",
    "\n",
    "1. ¿Cuáles son las ventajas de una CNN sobre una DNN totalmente conectada para la clasificación de imágenes?\n",
    "\n",
    "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels:\n",
    "\n",
    "- ¿Cuál es el número total de parámetros en la CNN?\n",
    "\n",
    "- Si estamos usando flotadores de 32 bits, ¿al menos cuánta RAM requerirá esta red al hacer una predicción para una sola instancia?\n",
    "\n",
    "- ¿Qué pasa cuando se entrena en un mini-lote de 50 imágenes?\n",
    "\n",
    "3. Si tu GPU se queda sin memoria mientras entrenas a una CNN, ¿cuáles son las cinco cosas que podrías intentar resolver el problema?\n",
    "\n",
    "4. ¿Por qué querrías añadir una capa de agrupación máxima en lugar de una capa convolucional con el mismo paso?\n",
    "\n",
    "5. ¿Cuándo te gustaría añadir una capa de normalización de la respuesta local?\n",
    "\n",
    "6. ¿Puedes nombrar las principales innovaciones de AlexNet, en comparación con LeNet-5? ¿Qué pasa con las principales innovaciones en GoogLeNet, ResNet, SENet, Xception y EfficientNet?\n",
    "\n",
    "7. ¿Qué es una red totalmente convolucional? ¿Cómo se puede convertir una capa densa en una capa convolucional?\n",
    "\n",
    "8. ¿Cuál es la principal dificultad técnica de la segmentación semántica?\n",
    "\n",
    "9. Construye tu propia CNN desde cero y trata de lograr la mayor precisión posible en MNIST.\n",
    "\n",
    "10. Utilice el aprendizaje por transferencia para la clasificación de imágenes grandes, siguiendo estos pasos:\n",
    "\n",
    "- Crea un conjunto de entrenamiento que contenga al menos 100 imágenes por clase. Por ejemplo, podría clasificar sus propias imágenes en función de la ubicación (playa, montaña, ciudad, etc.), o alternativamente puede utilizar un conjunto de datos existente (por ejemplo, de los conjuntos de datos de TensorFlow).\n",
    "\n",
    "- Divídelo en un conjunto de entrenamiento, un conjunto de validación y un conjunto de pruebas.\n",
    "\n",
    "- Construya la canalización de entrada, aplique las operaciones de preprocesamiento apropiadas y, opcionalmente, agregue aumento de datos.\n",
    "\n",
    "- Ajuste un modelo preentrenado en este conjunto de datos.\n",
    "\n",
    "11. Revisa el tutorial de transferencia de estilo de TensorFlow. Esta es una forma divertida de generar arte usando el aprendizaje profundo.\n",
    "Las soluciones a estos ejercicios están disponibles al final del cuaderno de este capítulo, en https://homl.info/colab3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca32af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
