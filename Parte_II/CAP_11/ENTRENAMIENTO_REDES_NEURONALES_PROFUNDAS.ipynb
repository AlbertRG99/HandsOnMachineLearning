{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fc489b",
   "metadata": {},
   "source": [
    "# Entrenamiento de redes neuronales profundas\n",
    "\n",
    "\n",
    "En el capítulo 10, construyeste, entrenaste y afinaste tus primeras redes neuronales artificiales. Pero eran redes poco profundas, con solo unas pocas capas ocultas. ¿Qué pasa si necesitas abordar un problema complejo, como detectar cientos de tipos de objetos en imágenes de alta resolución? Es posible que necesites entrenar un ANN mucho más profundo, tal vez con 10 capas o muchas más, cada una de las cuales contiene cientos de neuronas, unidas por cientos de miles de conexiones. Entrenar una red neuronal profunda no es un paseo por el parque. Estos son algunos de los problemas con los que podrías encontrarte:\n",
    "\n",
    "- Puede enfrentarse al problema de que los gradientes se vuelvan cada vez más pequeños o más grandes, cuando fluyan hacia atrás a través del DNN durante el entrenamiento. Ambos problemas hacen que las capas inferiores sean muy difíciles de entrenar.\n",
    "\n",
    "* Es posible que no tengas suficientes datos de entrenamiento para una red tan grande, o que sea demasiado costoso etiquetarlo.\n",
    "\n",
    "- El entrenamiento puede ser extremadamente lento.\n",
    "\n",
    "* Un modelo con millones de parámetros correría el riesgo de sobreaprobar el conjunto de entrenamiento, especialmente si no hay suficientes instancias de entrenamiento o si son demasiado ruidosas.\n",
    "\n",
    "En este capítulo repasaremos cada uno de estos problemas y presentaremos técnicas para resolverlos. Comenzaremos explorando los problemas de gradientes que desaparecen y explotan y algunas de sus soluciones más populares. A continuación, analizaremos el aprendizaje por transferencia y el preentrenamiento no supervisado, que puede ayudarte a abordar tareas complejas incluso cuando tienes pocos datos etiquetados. Luego discutiremos varios optimizadores que pueden acelerar enormemente el entrenamiento de grandes modelos. Finalmente, cubriremos algunas técnicas de regularización populares para grandes redes neuronales.\n",
    "\n",
    "Con estas herramientas, podrás entrenar redes muy profundas. ¡Bienvenido al aprendizaje profundo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3155e8",
   "metadata": {},
   "source": [
    "## Los problemas de los gradientes de desaparición/explosión\n",
    "\n",
    "Como se discute en el capítulo 10, la segunda fase del algoritmo de repropagación funciona pasando de la capa de salida a la capa de entrada, propagando el gradiente de error a lo largo del camino. Una vez que el algoritmo ha calculado el gradiente de la función de costo con respecto a cada parámetro de la red, utiliza estos gradientes para actualizar cada parámetro con un paso de descenso de gradiente.\n",
    "\n",
    "Desafortunadamente, los gradientes a menudo se hacen cada vez más pequeños a medida que el algoritmo avanza hasta las capas inferiores. Como resultado, la actualización de descenso del gradiente deja los pesos de conexión de las capas inferiores prácticamente sin cambios, y el entrenamiento nunca converge en una buena solución. Esto se llama el problema de los gradientes de desaparición. En algunos casos, puede suceder lo contrario: los gradientes pueden crecer cada vez más hasta que las capas obtienen actualizaciones de peso increíblemente grandes y el algoritmo diverge. Este es el problema de los gradientes explosivos, que aparece con mayor frecuencia en redes neuronales recurrentes (véase el capítulo 15). De manera más general, las redes neuronales profundas sufren de gradientes inestables; diferentes capas pueden aprender a velocidades muy diferentes.\n",
    "\n",
    "Este desafortunado comportamiento se observó empíricamente hace mucho tiempo, y fue una de las razones por las que las redes neuronales profundas se abandonaron en su mayoría a principios de la década de 2000. No estaba claro qué causó que los gradientes fueran tan inestables al entrenar un DNN, pero se arrojó algo de luz en un artículo de 2010 de Xavier Glorot y Yoshua Bengio.⁠1 Los autores encontraron algunos sospechosos, incluida la combinación de la popular función de activación sigmoide (logística) y la técnica de inicialización de peso que era más popular en ese momento (es decir, una distribución normal con una media de 0 y una desviación estándar de 1). En resumen, mostraron que con esta función de activación y este esquema de inicialización, la varianza de las salidas de cada capa es mucho mayor que la varianza de sus entradas. En el futuro en la red, la variación sigue aumentando después de cada capa hasta que la función de activación se satura en las capas superiores. Esta saturación en realidad se ve empeorada por el hecho de que la función sigmoide tiene una media de 0,5, no 0 (la función tangente hiperbólica tiene una media de 0 y se comporta ligeramente mejor que la función sigmoide en redes profundas).\n",
    "\n",
    "Mirando la función de activación sigmoide (ver Figura 11-1), se puede ver que cuando las entradas se vuelven grandes (negativas o positivas), la función se satura en 0 o 1, con una derivada extremadamente cercana a 0 (es decir, la curva es plana en ambos extremos). Por lo tanto, cuando la propagación posterior se inicia, prácticamente no tiene ningún gradiente para propagarse a través de la red, y el pequeño gradiente que existe se sigue diluyendo a medida que la propagación posterior progresa hacia abajo a través de las capas superiores, por lo que realmente no queda nada para las capas inferiores.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1101.png)\n",
    "\n",
    "_(Figura 11-1. Saturación de la función de activación sigmoide)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42cdc2",
   "metadata": {},
   "source": [
    "### Inicialización de Glorot y He\n",
    "\n",
    "En su artículo, Glorot y Bengio proponen una forma de aliviar significativamente el problema de los gradientes inestables. Señalan que necesitamos que la señal fluya correctamente en ambas direcciones: en la dirección hacia adelante al hacer predicciones, y en la dirección inversa cuando se retroceden los gradientes. No queremos que la señal se apare, ni queremos que explote y se sature. Para que la señal fluya correctamente, los autores argumentan que necesitamos que la varianza de las salidas de cada capa sea igual a la varianza de sus entradas,⁠ y necesitamos que los gradientes tengan la misma varianza antes y después de fluir a través de una capa en la dirección inversa (consulte el documento si está interesado en los detalles matemáticos). En realidad, no es posible garantizar ambas cosas a menos que la capa tenga un número igual de entradas y salidas (estos números se llaman fan-in y fan-out de la capa), pero Glorot y Bengio propusieron un buen compromiso que ha demostrado funcionar muy bien en la práctica: los pesos de conexión de cada capa deben inicializarse al azar como se describe en la Ecuación 11-1, donde **fanavg = (fanin + fanout) / 2**. Esta estrategia de inicialización se llama inicialización de Xavier o inicialización de Glorot, por el primer autor del artículo.\n",
    "\n",
    "#### Ecuación 11-1. Inicialización de Glorot (cuando se utiliza la función de activación sigmoide)\n",
    "\n",
    "<a href=\"https://ibb.co/mSvWd5z\"><img src=\"https://i.ibb.co/jHVXqk4/Captura-de-pantalla-2024-02-27-a-las-22-17-45.png\" alt=\"Captura-de-pantalla-2024-02-27-a-las-22-17-45\" border=\"0\"></a>\n",
    "\n",
    "Si reemplazas fanavg por fanin en la ecuación 11-1, obtienes una estrategia de inicialización que Yann LeCun propuso en la década de 1990. Lo llamó inicialización de LeCun. Genevieve Orr y Klaus-Robert Müller incluso lo recomendaron en su libro de 1998 Neural Networks: Tricks of the Trade (Springer). La inicialización de LeCun es equivalente a la inicialización de Glorot cuando fanin = fanout. Los investigadores tardaron más de una década en darse cuenta de lo importante que es este truco. El uso de la inicialización de Glorot puede acelerar considerablemente el entrenamiento, y es una de las prácticas que llevaron al éxito del aprendizaje profundo.\n",
    "\n",
    "Algunos documentos⁠ han proporcionado estrategias similares para diferentes funciones de activación. Estas estrategias difieren solo por la escala de la varianza y si usanfanavg o fanin, como se muestra en la Tabla 11-1 (para la distribución uniforme, solo use\n",
    "R=sqrt(3σ^2) ). La estrategia de inicialización propuesta para la función de activación ReLU y sus variantes se llama inicialización He o inicialización Kaiming, por el primer autor del artículo. Para SELU, utilice el método de inicialización de Yann LeCun, preferiblemente con una distribución normal. Cubriremos todas estas funciones de activación en breve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad3a70",
   "metadata": {},
   "source": [
    "| **Inicialización** | **Funciones de activación**              | **σ² (Normal)** |\n",
    "| ------------------ | ---------------------------------------- | --------------- |\n",
    "| **Glorot**         | Ninguno, tanh, sigmoid, softmax          | 1 / _fanavg_    |\n",
    "| **He**             | ReLU, Leaky ReLU, ELU, GELU, Swish, Mish | 2 / _fanin_     |\n",
    "| **LeCun**          | SELU                                     | 1 / _fanin_     |\n",
    "\n",
    "(_Tabla 11-1. Parámetros de inicialización para cada tipo de función de activación_)\n",
    "\n",
    "De forma predeterminada, Keras utiliza la inicialización de Glorot con una distribución uniforme. Cuando creas una capa, puedes cambiar a la inicialización He configurando `kernel_initializer=\"he_uniform\"` o `kernel_initializer=\"he_normal\"` de esta manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e3b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a124c",
   "metadata": {},
   "source": [
    "Alternativamente, puede obtener cualquiera de las inicializaciones enumeradas en la Tabla 11-1 y más utilizando el inicializador de `VarianceScaling`. Por ejemplo, si desea la inicialización de He con una distribución uniforme y basada en fanavg (en lugar de fanin), puede usar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b44513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c865891",
   "metadata": {},
   "source": [
    "## Mejores funciones de activación\n",
    "\n",
    "Una de las ideas en el documento de 2010 de Glorot y Bengio fue que los problemas con los gradientes inestables se debieron en parte a una mala elección de la función de activación. Hasta entonces, la mayoría de la gente había asumido que si la Madre Naturaleza había elegido usar funciones de activación aproximadamente sigmoide en las neuronas biológicas, deben ser una excelente opción. Pero resulta que otras funciones de activación se comportan mucho mejor en las redes neuronales profundas, en particular, la función de activación ReLU, principalmente porque no se satura para valores positivos, y también porque es muy rápida de calcular.\n",
    "\n",
    "Desafortunadamente, la función de activación de ReLU no es perfecta. Sufre de un problema conocido como los ReLUs moribundos: durante el entrenamiento, algunas neuronas \"mueren\" de manera efectiva, lo que significa que dejan de emitir cualquier otra cosa que no sea 0. En algunos casos, puede encontrar que la mitad de las neuronas de su red están muertas, especialmente si utilizó una gran tasa de aprendizaje. Una neurona muere cuando sus pesos se ajustan de tal manera que la entrada de la función ReLU (es decir, la suma ponderada de las entradas de la neurona más su término de sesgo) es negativa para todos los casos del conjunto de entrenamiento. Cuando esto sucede, solo sigue emitiendo ceros, y el descenso del gradiente ya no lo afecta porque el gradiente de la función ReLU es cero cuando su entrada es negativa.⁠\n",
    "\n",
    "Para resolver este problema, es posible que desee utilizar una variante de la función ReLU, como la ReLU con fugas.\n",
    "\n",
    "### Fugas de ReLU\n",
    "\n",
    "La función de activación de ReLU con fugas se define como LeakyReLUα(z) = max(αz, z) (ver Figura 11-2). El hiperparámetro α define cuánto \"se filtra\" la función: es la pendiente de la función para z < 0. Tener una pendiente para z < 0 asegura que los ReLU con fugas nunca mueran; pueden entrar en un coma largo, pero finalmente tienen la oportunidad de despertarse. Un artículo de 2015 de Bing Xu et al.⁠5 comparó varias variantes de la función de activación ReLU, y una de sus conclusiones fue que las variantes con fugas siempre superaron a la estricta función de activación de ReLU. De hecho, establecer α = 0,2 (una fuga enorme) parecía resultar en un mejor rendimiento que α = 0,01 (una pequeña fuga). El documento también evaluó el ReLU con fugas aleatoria (RReLU), donde α se recoge al azar en un rango determinado durante el entrenamiento y se fija a un valor promedio durante la prueba. RReLU también se desempeñó bastante bien y parecía actuar como un regularizador, reduciendo el riesgo de sobreajuste del conjunto de entrenamiento. Finalmente, el documento evaluó el ReLU de fugas paramétricas (PReLU), donde se autoriza a aprender α durante el entrenamiento: en lugar de ser un hiperparámetro, se convierte en un parámetro que se puede modificar por repropagación como cualquier otro parámetro. Se informó que PReLU superó en gran medida a ReLU en grandes conjuntos de datos de imágenes, pero en conjuntos de datos más pequeños corre el riesgo de sobreaparar el conjunto de entrenamiento.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1102.png)\n",
    "\n",
    "(_Figura 11-2. ReLU con fugas: como ReLU, pero con una pequeña pendiente para valores negativos_)\n",
    "\n",
    "Keras incluye las clases `LeakyReLU` y `PReLU` en el paquete `tf.keras.layers`. Al igual que con otras variantes de ReLU, debes usar la inicialización He con estas. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a43fd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # predeterminado con alpha=0.3\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a23bd3",
   "metadata": {},
   "source": [
    "Si lo prefieres, también puedes usar `LeakyReLU` como una capa separada en tu modelo; no hace ninguna diferencia para el entrenamiento y las predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920db4de",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2413816191.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    [...]  # more layers\n",
    "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),  # activation as a separate layer\n",
    "    [...]  # more layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c133c",
   "metadata": {},
   "source": [
    "Para `PReLU`, reemplace `LeakyReLU` con `PReLU`. Actualmente no existe una implementación oficial de RReLU en Keras, pero puedes implementar la tuya propia con bastante facilidad (para aprender cómo hacerlo, consulta los ejercicios al final del Capítulo 12).\n",
    "\n",
    "ReLU, ReLU con fugas y PReLU sufren por el hecho de que no son funciones suaves: sus derivados cambian abruptamente (en z = 0). Como vimos en el capítulo 4 cuando hablamos de lazo, este tipo de discontinuidad puede hacer que el descenso del gradiente rebote alrededor del óptimo y ralentice la convergencia. Así que ahora veremos algunas variantes suaves de la función de activación de ReLU, comenzando con ELU y SELU.\n",
    "\n",
    "\n",
    "### ELU y SELU\n",
    "\n",
    "\n",
    "En 2015, un artículo de Djork-Arné Clevert et al.⁠ propuso una nueva función de activación, llamada unidad lineal exponencial (ELU), que superó a todas las variantes de ReLU en los experimentos de los autores: el tiempo de entrenamiento se redujo y la red neuronal funcionó mejor en el conjunto de pruebas. La ecuación 11-2 muestra la definición de esta función de activación.\n",
    "\n",
    "#### Ecuación 11-2. Función de activación de ELU\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/g4QQLqd/Captura-de-pantalla-2024-02-28-a-las-22-48-25.png\" alt=\"Captura-de-pantalla-2024-02-28-a-las-22-48-25\" border=\"0\"></a>\n",
    "\n",
    "La función de activación de ELU se parece mucho a la función ReLU (ver Figura 11-3), con algunas diferencias importantes:\n",
    "\n",
    "* Toma valores negativos cuando z < 0, lo que permite que la unidad tenga una salida promedio cercana a 0 y ayuda a aliviar el problema de los gradientes de desaparición. El hiperparámetro α define lo opuesto al valor al que se acerca la función ELU cuando z es un número negativo grande. Por lo general, se establece en 1, pero puedes ajustarlo como cualquier otro hiperparámetro.\n",
    "\n",
    "- Tiene un gradiente distinto de cero para z < 0, lo que evita el problema de las neuronas muertas.\n",
    "\n",
    "* Si α es igual a 1, entonces la función es suave en todas partes, incluyendo alrededor de z = 0, lo que ayuda a acelerar el descenso del gradiente, ya que no rebota tanto a la izquierda y a la derecha de z = 0.\n",
    "\n",
    "Usar ELU con Keras es tan fácil como configurar `activation=\"elu\"` y, al igual que con otras variantes de ReLU, debes usar la inicialización He. El principal inconveniente de la función de activación ELU es que es más lento de calcular que la función ReLU y sus variantes (debido al uso de la función exponencial). Su tasa de convergencia más rápida durante el entrenamiento puede compensar ese cálculo lento, pero aún así, en el momento de la prueba una red ELU será un poco más lenta que una red ReLU.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1103.png)\n",
    "\n",
    "(_Figura 11-3. Funciones de activación de ELU y SELU_)\n",
    "\n",
    "\n",
    "No mucho después, un artículo de 2017 de Günter Klambauer et al.⁠ introdujo la función de activación de ELU a escala (SELU): como su nombre indica, es una variante a escala de la función de activación de ELU (aproximadamente 1,05 veces ELU, usando α ≈ 1,67). Los autores mostraron que si se construye una red neuronal compuesta exclusivamente de una pila de capas densas (es decir, una MLP), y si todas las capas ocultas utilizan la función de activación SELU, entonces la red se autonormalizará: la salida de cada capa tenderá a preservar una media de 0 y una desviación estándar de 1 durante el entrenamiento, lo que resuelve el problema de los gradientes de desaparición/explosión. Como resultado, la función de activación de SELU puede superar a otras funciones de activación para los MLP, especialmente las profundas. Para usarlo con Keras, solo tienes que establecer `activation=\"selu\"` Sin embargo, hay algunas condiciones para que se ocurra la autonormalización (ver el documento para la justificación matemática):\n",
    "\n",
    "* Las características de entrada deben estar estandarizadas: media 0 y desviación estándar 1.\n",
    "\n",
    "- Los pesos de cada capa oculta deben inicializarse mediante la inicialización normal de LeCun. En Keras, esto significa settingkernel  `kernel_initializer=\"lecun_normal\"`\n",
    "\n",
    "* La propiedad de autonormalización solo está garantizada con MLP simples. Si intenta usar SELU en otras arquitecturas, como redes recurrentes (consulte el capítulo 15) o redes con conexiones de salto (es decir, conexiones que omiten capas, como en redes Wide & Deep), probablemente no superará a ELU.\n",
    "\n",
    "- No se pueden usar técnicas de regularización como la regularización ℓ1 o ℓ2, la norma máxima, la norma por lotes o la deserción regular (estas se discuten más adelante en este capítulo).\n",
    "\n",
    "Estas son restricciones significativas, por lo que a pesar de sus promesas, SELU no ganó mucha tracción. Además, tres funciones de activación más parecen superarlo de manera bastante consistente en la mayoría de las tareas: GELU, Swish y Mish.\n",
    "\n",
    "\n",
    "### GELU, Swish y Mish\n",
    "\n",
    "GELU fue presentado en un documento de 2016 por Dan Hendrycks y Kevin Gimpel. Una vez más, se puede pensar en ello como una variante suave de la función de activación de ReLU. Su definición se da en la ecuación 11-3, donde Φ es la función de distribución acumulativa gaussiana estándar (CDF): Φ(z) corresponde a la probabilidad de que un valor muestreado al azar de una distribución normal de media 0 y la varianza 1 sea menor thanz.\n",
    "\n",
    "#### Ecuación 11-3. Función de activación de GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d7a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
