{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fc489b",
   "metadata": {},
   "source": [
    "# Entrenamiento de redes neuronales profundas\n",
    "\n",
    "\n",
    "En el capítulo 10, construyeste, entrenaste y afinaste tus primeras redes neuronales artificiales. Pero eran redes poco profundas, con solo unas pocas capas ocultas. ¿Qué pasa si necesitas abordar un problema complejo, como detectar cientos de tipos de objetos en imágenes de alta resolución? Es posible que necesites entrenar un ANN mucho más profundo, tal vez con 10 capas o muchas más, cada una de las cuales contiene cientos de neuronas, unidas por cientos de miles de conexiones. Entrenar una red neuronal profunda no es un paseo por el parque. Estos son algunos de los problemas con los que podrías encontrarte:\n",
    "\n",
    "- Puede enfrentarse al problema de que los gradientes se vuelvan cada vez más pequeños o más grandes, cuando fluyan hacia atrás a través del DNN durante el entrenamiento. Ambos problemas hacen que las capas inferiores sean muy difíciles de entrenar.\n",
    "\n",
    "* Es posible que no tengas suficientes datos de entrenamiento para una red tan grande, o que sea demasiado costoso etiquetarlo.\n",
    "\n",
    "- El entrenamiento puede ser extremadamente lento.\n",
    "\n",
    "* Un modelo con millones de parámetros correría el riesgo de sobreaprobar el conjunto de entrenamiento, especialmente si no hay suficientes instancias de entrenamiento o si son demasiado ruidosas.\n",
    "\n",
    "En este capítulo repasaremos cada uno de estos problemas y presentaremos técnicas para resolverlos. Comenzaremos explorando los problemas de gradientes que desaparecen y explotan y algunas de sus soluciones más populares. A continuación, analizaremos el aprendizaje por transferencia y el preentrenamiento no supervisado, que puede ayudarte a abordar tareas complejas incluso cuando tienes pocos datos etiquetados. Luego discutiremos varios optimizadores que pueden acelerar enormemente el entrenamiento de grandes modelos. Finalmente, cubriremos algunas técnicas de regularización populares para grandes redes neuronales.\n",
    "\n",
    "Con estas herramientas, podrás entrenar redes muy profundas. ¡Bienvenido al aprendizaje profundo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3155e8",
   "metadata": {},
   "source": [
    "## Los problemas de los gradientes de desaparición(desvanecimiento)/explosión\n",
    "\n",
    "Como se discute en el capítulo 10, la segunda fase del algoritmo de repropagación funciona pasando de la capa de salida a la capa de entrada, propagando el gradiente de error a lo largo del camino. Una vez que el algoritmo ha calculado el gradiente de la función de costo con respecto a cada parámetro de la red, utiliza estos gradientes para actualizar cada parámetro con un paso de descenso de gradiente.\n",
    "\n",
    "Desafortunadamente, los gradientes a menudo se hacen cada vez más pequeños a medida que el algoritmo avanza hasta las capas inferiores. Como resultado, la actualización de descenso del gradiente deja los pesos de conexión de las capas inferiores prácticamente sin cambios, y el entrenamiento nunca converge en una buena solución. Esto se llama el problema de los gradientes de desaparición. En algunos casos, puede suceder lo contrario: los gradientes pueden crecer cada vez más hasta que las capas obtienen actualizaciones de peso increíblemente grandes y el algoritmo diverge. Este es el problema de los gradientes explosivos, que aparece con mayor frecuencia en redes neuronales recurrentes (véase el capítulo 15). De manera más general, las redes neuronales profundas sufren de gradientes inestables; diferentes capas pueden aprender a velocidades muy diferentes.\n",
    "\n",
    "Este desafortunado comportamiento se observó empíricamente hace mucho tiempo, y fue una de las razones por las que las redes neuronales profundas se abandonaron en su mayoría a principios de la década de 2000. No estaba claro qué causó que los gradientes fueran tan inestables al entrenar un DNN, pero se arrojó algo de luz en un artículo de 2010 de Xavier Glorot y Yoshua Bengio.⁠1 Los autores encontraron algunos sospechosos, incluida la combinación de la popular función de activación sigmoide (logística) y la técnica de inicialización de peso que era más popular en ese momento (es decir, una distribución normal con una media de 0 y una desviación estándar de 1). En resumen, mostraron que con esta función de activación y este esquema de inicialización, la varianza de las salidas de cada capa es mucho mayor que la varianza de sus entradas. En el futuro en la red, la variación sigue aumentando después de cada capa hasta que la función de activación se satura en las capas superiores. Esta saturación en realidad se ve empeorada por el hecho de que la función sigmoide tiene una media de 0,5, no 0 (la función tangente hiperbólica tiene una media de 0 y se comporta ligeramente mejor que la función sigmoide en redes profundas).\n",
    "\n",
    "Mirando la función de activación sigmoide (ver Figura 11-1), se puede ver que cuando las entradas se vuelven grandes (negativas o positivas), la función se satura en 0 o 1, con una derivada extremadamente cercana a 0 (es decir, la curva es plana en ambos extremos). Por lo tanto, cuando la propagación posterior se inicia, prácticamente no tiene ningún gradiente para propagarse a través de la red, y el pequeño gradiente que existe se sigue diluyendo a medida que la propagación posterior progresa hacia abajo a través de las capas superiores, por lo que realmente no queda nada para las capas inferiores.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1101.png)\n",
    "\n",
    "_(Figura 11-1. Saturación de la función de activación sigmoide)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42cdc2",
   "metadata": {},
   "source": [
    "### Inicialización de Glorot y He\n",
    "\n",
    "En su artículo, Glorot y Bengio proponen una forma de aliviar significativamente el problema de los gradientes inestables. Señalan que necesitamos que la señal fluya correctamente en ambas direcciones: en la dirección hacia adelante al hacer predicciones, y en la dirección inversa cuando se retroceden los gradientes. No queremos que la señal se apare, ni queremos que explote y se sature. Para que la señal fluya correctamente, los autores argumentan que necesitamos que la varianza de las salidas de cada capa sea igual a la varianza de sus entradas,⁠ y necesitamos que los gradientes tengan la misma varianza antes y después de fluir a través de una capa en la dirección inversa (consulte el documento si está interesado en los detalles matemáticos). En realidad, no es posible garantizar ambas cosas a menos que la capa tenga un número igual de entradas y salidas (estos números se llaman fan-in y fan-out de la capa), pero Glorot y Bengio propusieron un buen compromiso que ha demostrado funcionar muy bien en la práctica: los pesos de conexión de cada capa deben inicializarse al azar como se describe en la Ecuación 11-1, donde **fanavg = (fanin + fanout) / 2**. Esta estrategia de inicialización se llama inicialización de Xavier o inicialización de Glorot, por el primer autor del artículo.\n",
    "\n",
    "#### Ecuación 11-1. Inicialización de Glorot (cuando se utiliza la función de activación sigmoide)\n",
    "\n",
    "<a href=\"https://ibb.co/mSvWd5z\"><img src=\"https://i.ibb.co/jHVXqk4/Captura-de-pantalla-2024-02-27-a-las-22-17-45.png\" alt=\"Captura-de-pantalla-2024-02-27-a-las-22-17-45\" border=\"0\"></a>\n",
    "\n",
    "Si reemplazas fanavg por fanin en la ecuación 11-1, obtienes una estrategia de inicialización que Yann LeCun propuso en la década de 1990. Lo llamó inicialización de LeCun. Genevieve Orr y Klaus-Robert Müller incluso lo recomendaron en su libro de 1998 Neural Networks: Tricks of the Trade (Springer). La inicialización de LeCun es equivalente a la inicialización de Glorot cuando fanin = fanout. Los investigadores tardaron más de una década en darse cuenta de lo importante que es este truco. El uso de la inicialización de Glorot puede acelerar considerablemente el entrenamiento, y es una de las prácticas que llevaron al éxito del aprendizaje profundo.\n",
    "\n",
    "Algunos documentos⁠ han proporcionado estrategias similares para diferentes funciones de activación. Estas estrategias difieren solo por la escala de la varianza y si usanfanavg o fanin, como se muestra en la Tabla 11-1 (para la distribución uniforme, solo use\n",
    "R=sqrt(3σ^2) ). La estrategia de inicialización propuesta para la función de activación ReLU y sus variantes se llama inicialización He o inicialización Kaiming, por el primer autor del artículo. Para SELU, utilice el método de inicialización de Yann LeCun, preferiblemente con una distribución normal. Cubriremos todas estas funciones de activación en breve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad3a70",
   "metadata": {},
   "source": [
    "| **Inicialización** | **Funciones de activación**              | **σ² (Normal)** |\n",
    "| ------------------ | ---------------------------------------- | --------------- |\n",
    "| **Glorot**         | Ninguno, tanh, sigmoid, softmax          | 1 / _fanavg_    |\n",
    "| **He**             | ReLU, Leaky ReLU, ELU, GELU, Swish, Mish | 2 / _fanin_     |\n",
    "| **LeCun**          | SELU                                     | 1 / _fanin_     |\n",
    "\n",
    "(_Tabla 11-1. Parámetros de inicialización para cada tipo de función de activación_)\n",
    "\n",
    "De forma predeterminada, Keras utiliza la inicialización de Glorot con una distribución uniforme. Cuando creas una capa, puedes cambiar a la inicialización He configurando `kernel_initializer=\"he_uniform\"` o `kernel_initializer=\"he_normal\"` de esta manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e3b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 12:42:38.626400: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a124c",
   "metadata": {},
   "source": [
    "Alternativamente, puede obtener cualquiera de las inicializaciones enumeradas en la Tabla 11-1 y más utilizando el inicializador de `VarianceScaling`. Por ejemplo, si desea la inicialización de He con una distribución uniforme y basada en fanavg (en lugar de fanin), puede usar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b44513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c865891",
   "metadata": {},
   "source": [
    "## Mejores funciones de activación\n",
    "\n",
    "Una de las ideas en el documento de 2010 de Glorot y Bengio fue que los problemas con los gradientes inestables se debieron en parte a una mala elección de la función de activación. Hasta entonces, la mayoría de la gente había asumido que si la Madre Naturaleza había elegido usar funciones de activación aproximadamente sigmoide en las neuronas biológicas, deben ser una excelente opción. Pero resulta que otras funciones de activación se comportan mucho mejor en las redes neuronales profundas, en particular, la función de activación ReLU, principalmente porque no se satura para valores positivos, y también porque es muy rápida de calcular.\n",
    "\n",
    "Desafortunadamente, la función de activación de ReLU no es perfecta. Sufre de un problema conocido como los ReLUs moribundos: durante el entrenamiento, algunas neuronas \"mueren\" de manera efectiva, lo que significa que dejan de emitir cualquier otra cosa que no sea 0. En algunos casos, puede encontrar que la mitad de las neuronas de su red están muertas, especialmente si utilizó una gran tasa de aprendizaje. Una neurona muere cuando sus pesos se ajustan de tal manera que la entrada de la función ReLU (es decir, la suma ponderada de las entradas de la neurona más su término de sesgo) es negativa para todos los casos del conjunto de entrenamiento. Cuando esto sucede, solo sigue emitiendo ceros, y el descenso del gradiente ya no lo afecta porque el gradiente de la función ReLU es cero cuando su entrada es negativa.⁠\n",
    "\n",
    "Para resolver este problema, es posible que desee utilizar una variante de la función ReLU, como la ReLU con fugas.\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "La función de activación de ReLU con fugas se define como LeakyReLUα(z) = max(αz, z) (ver Figura 11-2). El hiperparámetro α define cuánto \"se filtra\" la función: es la pendiente de la función para z < 0. Tener una pendiente para z < 0 asegura que los ReLU con fugas nunca mueran; pueden entrar en un coma largo, pero finalmente tienen la oportunidad de despertarse. Un artículo de 2015 de Bing Xu et al.⁠5 comparó varias variantes de la función de activación ReLU, y una de sus conclusiones fue que las variantes con fugas siempre superaron a la estricta función de activación de ReLU. De hecho, establecer α = 0,2 (una fuga enorme) parecía resultar en un mejor rendimiento que α = 0,01 (una pequeña fuga). El documento también evaluó el ReLU con fugas aleatoria (RReLU), donde α se recoge al azar en un rango determinado durante el entrenamiento y se fija a un valor promedio durante la prueba. RReLU también se desempeñó bastante bien y parecía actuar como un regularizador, reduciendo el riesgo de sobreajuste del conjunto de entrenamiento. Finalmente, el documento evaluó el ReLU de fugas paramétricas (PReLU), donde se autoriza a aprender α durante el entrenamiento: en lugar de ser un hiperparámetro, se convierte en un parámetro que se puede modificar por repropagación como cualquier otro parámetro. Se informó que PReLU superó en gran medida a ReLU en grandes conjuntos de datos de imágenes, pero en conjuntos de datos más pequeños corre el riesgo de sobreaparar el conjunto de entrenamiento.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1102.png)\n",
    "\n",
    "(_Figura 11-2. ReLU con fugas: como ReLU, pero con una pequeña pendiente para valores negativos_)\n",
    "\n",
    "Keras incluye las clases `LeakyReLU` y `PReLU` en el paquete `tf.keras.layers`. Al igual que con otras variantes de ReLU, debes usar la inicialización He con estas. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43fd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # predeterminado con alpha=0.3\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a23bd3",
   "metadata": {},
   "source": [
    "Si lo prefieres, también puedes usar `LeakyReLU` como una capa separada en tu modelo; no hace ninguna diferencia para el entrenamiento y las predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920db4de",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2413816191.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    [...]  # more layers\n",
    "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),  # activation as a separate layer\n",
    "    [...]  # more layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c133c",
   "metadata": {},
   "source": [
    "Para `PReLU`, reemplace `LeakyReLU` con `PReLU`. Actualmente no existe una implementación oficial de RReLU en Keras, pero puedes implementar la tuya propia con bastante facilidad (para aprender cómo hacerlo, consulta los ejercicios al final del Capítulo 12).\n",
    "\n",
    "ReLU, ReLU con fugas y PReLU sufren por el hecho de que no son funciones suaves: sus derivados cambian abruptamente (en z = 0). Como vimos en el capítulo 4 cuando hablamos de lazo, este tipo de discontinuidad puede hacer que el descenso del gradiente rebote alrededor del óptimo y ralentice la convergencia. Así que ahora veremos algunas variantes suaves de la función de activación de ReLU, comenzando con ELU y SELU.\n",
    "\n",
    "\n",
    "### ELU y SELU\n",
    "\n",
    "\n",
    "En 2015, un artículo de Djork-Arné Clevert et al.⁠ propuso una nueva función de activación, llamada unidad lineal exponencial (ELU), que superó a todas las variantes de ReLU en los experimentos de los autores: el tiempo de entrenamiento se redujo y la red neuronal funcionó mejor en el conjunto de pruebas. La ecuación 11-2 muestra la definición de esta función de activación.\n",
    "\n",
    "#### Ecuación 11-2. Función de activación de ELU\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/g4QQLqd/Captura-de-pantalla-2024-02-28-a-las-22-48-25.png\" alt=\"Captura-de-pantalla-2024-02-28-a-las-22-48-25\" border=\"0\"></a>\n",
    "\n",
    "La función de activación de ELU se parece mucho a la función ReLU (ver Figura 11-3), con algunas diferencias importantes:\n",
    "\n",
    "* Toma valores negativos cuando z < 0, lo que permite que la unidad tenga una salida promedio cercana a 0 y ayuda a aliviar el problema de los gradientes de desaparición. El hiperparámetro α define lo opuesto al valor al que se acerca la función ELU cuando z es un número negativo grande. Por lo general, se establece en 1, pero puedes ajustarlo como cualquier otro hiperparámetro.\n",
    "\n",
    "- Tiene un gradiente distinto de cero para z < 0, lo que evita el problema de las neuronas muertas.\n",
    "\n",
    "* Si α es igual a 1, entonces la función es suave en todas partes, incluyendo alrededor de z = 0, lo que ayuda a acelerar el descenso del gradiente, ya que no rebota tanto a la izquierda y a la derecha de z = 0.\n",
    "\n",
    "Usar ELU con Keras es tan fácil como configurar `activation=\"elu\"` y, al igual que con otras variantes de ReLU, debes usar la inicialización He. El principal inconveniente de la función de activación ELU es que es más lento de calcular que la función ReLU y sus variantes (debido al uso de la función exponencial). Su tasa de convergencia más rápida durante el entrenamiento puede compensar ese cálculo lento, pero aún así, en el momento de la prueba una red ELU será un poco más lenta que una red ReLU.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1103.png)\n",
    "\n",
    "(_Figura 11-3. Funciones de activación de ELU y SELU_)\n",
    "\n",
    "\n",
    "No mucho después, un artículo de 2017 de Günter Klambauer et al.⁠ introdujo la función de activación de ELU a escala (SELU): como su nombre indica, es una variante a escala de la función de activación de ELU (aproximadamente 1,05 veces ELU, usando α ≈ 1,67). Los autores mostraron que si se construye una red neuronal compuesta exclusivamente de una pila de capas densas (es decir, una MLP), y si todas las capas ocultas utilizan la función de activación SELU, entonces la red se autonormalizará: la salida de cada capa tenderá a preservar una media de 0 y una desviación estándar de 1 durante el entrenamiento, lo que resuelve el problema de los gradientes de desaparición/explosión. Como resultado, la función de activación de SELU puede superar a otras funciones de activación para los MLP, especialmente las profundas. Para usarlo con Keras, solo tienes que establecer `activation=\"selu\"` Sin embargo, hay algunas condiciones para que se ocurra la autonormalización (ver el documento para la justificación matemática):\n",
    "\n",
    "* Las características de entrada deben estar estandarizadas: media 0 y desviación estándar 1.\n",
    "\n",
    "- Los pesos de cada capa oculta deben inicializarse mediante la inicialización normal de LeCun. En Keras, esto significa settingkernel  `kernel_initializer=\"lecun_normal\"`\n",
    "\n",
    "* La propiedad de autonormalización solo está garantizada con MLP simples. Si intenta usar SELU en otras arquitecturas, como redes recurrentes (consulte el capítulo 15) o redes con conexiones de salto (es decir, conexiones que omiten capas, como en redes Wide & Deep), probablemente no superará a ELU.\n",
    "\n",
    "- No se pueden usar técnicas de regularización como la regularización ℓ1 o ℓ2, la norma máxima, la norma por lotes o la deserción regular (estas se discuten más adelante en este capítulo).\n",
    "\n",
    "Estas son restricciones significativas, por lo que a pesar de sus promesas, SELU no ganó mucha tracción. Además, tres funciones de activación más parecen superarlo de manera bastante consistente en la mayoría de las tareas: GELU, Swish y Mish.\n",
    "\n",
    "\n",
    "### GELU, Swish y Mish\n",
    "\n",
    "GELU fue presentado en un documento de 2016 por Dan Hendrycks y Kevin Gimpel. Una vez más, se puede pensar en ello como una variante suave de la función de activación de ReLU. Su definición se da en la ecuación 11-3, donde Φ es la función de distribución acumulativa gaussiana estándar (CDF): Φ(z) corresponde a la probabilidad de que un valor muestreado al azar de una distribución normal de media 0 y la varianza 1 sea menor thanz.\n",
    "\n",
    "#### Ecuación 11-3. Función de activación de GELU\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/mHwy9Lw/Captura-de-pantalla-2024-02-28-a-las-22-55-07.png\" alt=\"Captura-de-pantalla-2024-02-28-a-las-22-55-07\" border=\"0\"></a>\n",
    "\n",
    "Como puedes ver en la Figura 11-4, GELU se parece a ReLU: se acerca a 0 cuando su entrada z es muy negativa, y se acerca a z cuando z es muy positiva. Sin embargo, mientras que todas las funciones de activación que hemos discutido hasta ahora eran convexas y monótónicas, la función de activación de GELU no es ninguna de las dos: de izquierda a derecha, comienza yendo recto, luego se mueve hacia abajo, alcanza un punto bajo alrededor de -0,17 (cerca de z ≈ -0,75), y finalmente rebota hacia arriba y termina yendo directamente hacia la parte superior derecha. Esta forma bastante compleja y el hecho de que tiene una curvatura en cada punto pueden explicar por qué funciona tan bien, especialmente para tareas complejas: el descenso de gradiente puede ser más fácil adaptarse a patrones complejos. En la práctica, a menudo supera a todas las demás funciones de activación discutidas hasta ahora. Sin embargo, es un poco más intensivo desde el punto de vista computacional, y el aumento del rendimiento que proporciona no siempre es suficiente para justificar el costo adicional. Dicho esto, es posible mostrar que es aproximadamente igual a tozσ (1,702 z), donde σ es la función sigmoide: usar esta aproximación también funciona muy bien, y tiene la ventaja de ser mucho más rápido de calcular.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1104.png)\n",
    "\n",
    "(_Figura 11-4. Funciones de activación de GELU, Swish, Swish parametrizado y Mish_)\n",
    "\n",
    "El documento de GELU también introdujo la función de activación de la unidad lineal sigmoide (SiLU), que es igual a zσ(z), pero fue superada por GELU en las pruebas de los autores. Curiosamente, un artículo de 2017 de Prajit Ramachandran et al. redescubrió la función SiLU mediante la búsqueda automática de buenas funciones de activación. Los autores lo llamaron Swish, y el nombre se puso de día. En su artículo, Swish superó a todas las demás funciones, incluida GELU. Ramachandran et al. más tarde generalizaron Swish añadiendo un hiperparámetro adicional β para escalar la entrada de la función sigmoide. La función Swish generalizada es Swishβ(z) = zσ(βz), por lo que GELU es aproximadamente igual a la función Swish generalizada usando β = 1,702. Puedes ajustar β como cualquier otro hiperparámetro. Alternativamente, también es posible hacer que β se pueda entrenar y dejar que el descenso de gradiente lo optimice: al igual que PReLU, esto puede hacer que su modelo sea más potente, pero también corre el riesgo de sobreajuste de los datos.\n",
    "\n",
    "Otra función de activación bastante similar es Mish, que fue presentada en un documento de 2019 por Diganta Misra. Se define como mish(z) = ztanh(softplus(z)), donde softplus(z) = log(1 + exp(z)). Al igual que GELU y Swish, es una variante suave, no convexa y no monótona de ReLU, y una vez más el autor realizó muchos experimentos y descubrió que Mish generalmente superó a otras funciones de activación, incluso Swish y GELU, por un pequeño margen. La figura 11-4 muestra GELU, Swish (tanto con el β por defecto = 1 como con β = 0,6) y, por último, Mish. Como puedes ver, Mish se superpone casi perfectamente con Swish cuando z es negativo, y casi perfectamente con GELU cuando z es positivo.\n",
    "\n",
    "### TIP\n",
    "\n",
    "Entonces, ¿qué función de activación deberías usar para las capas ocultas de tus redes neuronales profundas? ReLU sigue siendo un buen valor predeterminado para tareas simples: a menudo es tan bueno como las funciones de activación más sofisticadas, además de que es muy rápido de calcular, y muchas bibliotecas y aceleradores de hardware proporcionan optimizaciones específicas de ReLU. Sin embargo, Swish es probablemente un mejor valor predeterminado para tareas más complejas, e incluso puedes probar Swish parametrizado con un parámetro β que se puede aprender para las tareas más complejas. Mish puede darte resultados ligeramente mejores, pero requiere un poco más de computación. Si te importa mucho la latencia en tiempo de ejecución, entonces es posible que prefieras ReLU con fugas, o ReLU con fugas parametrizada para tareas más complejas. Para los MLP profundos, prueba a SELU, pero asegúrate de respetar las restricciones enumeradas anteriormente. Si tiene tiempo libre y potencia de cálculo, también puede utilizar la validación cruzada para evaluar otras funciones de activación.\n",
    "\n",
    "#### ------------------------------------------------------------------------------------\n",
    "\n",
    "Keras admite GELU y Swish desde el primer momento; simplemente use `activation=\"gelu\"` o `acativation=\"swish\"`. Sin embargo, todavía no es compatible con Mish o la función de activación generalizada Swish (pero consulte el Capítulo 12 para ver cómo implementar sus propias funciones y capas de activación).\n",
    "\n",
    "¡Eso es todo para las funciones de activación! Ahora, echemos un vistazo a una forma completamente diferente de resolver el problema de los gradientes inestables: la normalización por lotes.\n",
    "\n",
    "\n",
    "## Normalización por lotes\n",
    "\n",
    "Aunque el uso de la inicialización de He junto con ReLU (o cualquiera de sus variantes) puede reducir significativamente el peligro de los problemas de gradientes de desaparición/explosiación al comienzo del entrenamiento, no garantiza que no vuelvan durante el entrenamiento.\n",
    "\n",
    "En un artículo de 2015⁠, Sergey Ioffe y Christian Szegedy propusieron una técnica llamada normalización por lotes (BN) que aborda estos problemas. La técnica consiste en añadir una operación en el modelo justo antes o después de la función de activación de cada capa oculta. Esta operación simplemente centra en cero y normaliza cada entrada, luego escala y desplaza el resultado utilizando dos nuevos vectores de parámetros por capa: uno para escalar y el otro para desplazar. En otras palabras, la operación permite que el modelo aprenda la escala óptima y la media de cada una de las entradas de la capa. En muchos casos, si agrega una capa BN como primera capa de su red neuronal, no necesita estandarizar su conjunto de entrenamiento. Es decir, no hay necesidad de \"StandardScaler\" o \"Normalización\"; la capa BN lo hará por usted (bueno, aproximadamente, ya que solo analiza un lote a la vez y también puede cambiar la escala y cambiar cada característica de entrada).\n",
    "\n",
    "Con el fin de centrar a cero y normalizar las entradas, el algoritmo necesita estimar la media y la desviación estándar de cada entrada. Lo hace evaluando la media y la desviación estándar de la entrada sobre el minilote actual (de ahí el nombre de \"normalización del lote\"). Toda la operación se resume paso a paso en la Ecuación 11-4.\n",
    "\n",
    "### Ecuación 11-4. Algoritmo de normalización por lotes\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/TM8qBjW/Captura-de-pantalla-2024-02-29-a-las-12-50-54.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-12-50-54\" border=\"0\"></a>\n",
    "\n",
    "En este algoritmo:\n",
    "\n",
    "* **μB** es el vector de los medios de entrada, evaluados en todo el minilote B (contiene una media por entrada).\n",
    "\n",
    "- **mB** es el número de instancias en el minilote.\n",
    "\n",
    "* **σB** es el vector de las desviaciones estándar de entrada, también evaluada en todo el minilote (contiene una desviación estándar por entrada).\n",
    "\n",
    "- **X^(i)** es el vector de entradas centradas en cero y normalizadas, por ejemplo, i.\n",
    "\n",
    "* ε es un número diminuto que evita la división por cero y asegura que los gradientes no crezcan demasiado (normalmente de 10 a 5). Esto se llama un término suavizante.\n",
    "\n",
    "- **γ** es el vector de parámetros de escala de salida para la capa (contiene un parámetro de escala por entrada).\n",
    "\n",
    "* **⊗** representa la multiplicación por elementos (cada entrada se multiplica por su correspondiente parámetro de escala de salida).\n",
    "\n",
    "- **β** es el vector de parámetros de desplazamiento de salida (offset) para la capa (contiene un parámetro de desplazamiento por entrada). Cada entrada está compensada por su parámetro de cambio correspondiente.\n",
    "\n",
    "* **z(i)** es el resultado de la operación BN. Es una versión reescalada y desplazada de las entradas.\n",
    "\n",
    "\n",
    "Así que durante el entrenamiento, BN estandariza sus entradas, luego las reescala y las compensa. ¡Bien! ¿Qué tal a la hora del examen? Bueno, no es tan sencillo. De hecho, es posible que tengamos que hacer predicciones para instancias individuales en lugar de para lotes de instancias: en este caso, no tendremos forma de calcular la media y la desviación estándar de cada entrada. Además, incluso si tenemos un lote de instancias, puede ser demasiado pequeño, o las instancias pueden no ser independientes y distribuidas de manera idéntica, por lo que el cálculo de estadísticas sobre las instancias de lote no sería fiable. Una solución podría ser esperar hasta el final del entrenamiento, luego ejecutar todo el conjunto de entrenamiento a través de la red neuronal y calcular la media y la desviación estándar de cada entrada de la capa BN. Estos medios de entrada \"finales\" y las desviaciones estándar podrían usarse en lugar de los medios de entrada y las desviaciones estándar del lote al hacer predicciones. Sin embargo, la mayoría de las implementaciones de normalización por lotes estiman estas estadísticas finales durante el entrenamiento utilizando un promedio móvil de los promedios de entrada de la capa y las desviaciones estándar. Esto es lo que Keras hace automáticamente cuando se utiliza la capa `BatchNormalization`. En resumen, se aprenden cuatro vectores de parámetros en cada capa normalizada por lotes: γ (el vector de escala de salida) y β (el vector de desplazamiento de salida) se aprenden a través de la contrapropagación regular, y μ (el vector medio de entrada final) y σ (el vector de desviación estándar de entrada final) se estiman utilizando una media móvil exponencial. Tenga en cuenta que μ y σ se estiman durante el entrenamiento, pero solo se utilizan después del entrenamiento (para reemplazar las medias de entrada del lote y las desviaciones estándar en la ecuación 11-4).\n",
    "\n",
    "Ioffe y Szegedy demostraron que la normalización por lotes mejoró considerablemente todas las redes neuronales profundas con las que experimentaron, lo que llevó a una gran mejora en la tarea de clasificación de ImageNet (ImageNet es una gran base de datos de imágenes clasificadas en muchas clases, comúnmente utilizadas para evaluar los sistemas de visión por ordenador). El problema de los gradientes de desaparición se redujo considerablemente, hasta el punto de que podían usar funciones de activación saturante como el tanh e incluso la función de activación sigmoide. Las redes también eran mucho menos sensibles a la inicialización del peso. Los autores pudieron utilizar tasas de aprendizaje mucho mayores, lo que aceleró significativamente el proceso de aprendizaje. Específicamente, señalan que:\n",
    "\n",
    "_Aplicada a un modelo de clasificación de imágenes de última generación, la normalización por lotes logra la misma precisión con 14 veces menos pasos de entrenamiento, y supera al modelo original por un margen significativo. [... ] Utilizando un conjunto de redes normalizadas por lotes, mejoramos el mejor resultado publicado en la clasificación de ImageNet: alcanzando el 4,9 % de error de validación de los 5 primeros (y el 4,8 % de error de prueba), superando la precisión de los evaluadores humanos._\n",
    "\n",
    "\n",
    "Finalmente, como un regalo que sigue dando, la normalización por lotes actúa como un regularizador, reduciendo la necesidad de otras técnicas de regularización (como la deserción, descrita más adelante en este capítulo).\n",
    "\n",
    "Sin embargo, la normalización por lotes añade cierta complejidad al modelo (aunque puede eliminar la necesidad de normalizar los datos de entrada, como se discutió anteriormente). Además, hay una penalización en tiempo de ejecución: la red neuronal hace predicciones más lentas debido a los cálculos adicionales requeridos en cada capa. Afortunadamente, a menudo es posible fusionar la capa BN con la capa anterior después del entrenamiento, evitando así la penalización en tiempo de ejecución. Esto se hace actualizando los pesos y sesgos de la capa anterior para que produzca directamente salidas de la escala y el desplazamiento apropiados. Por ejemplo, si la capa anterior calcula **XW + b**, entonces la capa BN calculará **γ ⊗ (XW + b - μ) / σ + β** (ignorando el término suavizado ε en el denominador). Si definimos **W′ = γ⊗W / σ y b′ = γ ⊗ (b - μ) / σ + β**, la ecuación se simplifica a **XW′ + b′**. Por lo tanto, si reemplazamos los pesos y sesgos de la capa anterior **(W y b)** con los pesos y sesgos actualizados **(W′ y b′)**, podemos deshacernos de la capa BN (el convertidor de TFLite lo hace automáticamente; consulte el capítulo 19).\n",
    "\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "Es posible que encuentres que el entrenamiento es bastante lento, porque cada época lleva mucho más tiempo cuando usas la normalización por lotes. Esto generalmente se compensa con el hecho de que la convergencia es mucho más rápida con BN, por lo que se necesitarán menos épocas para alcanzar el mismo rendimiento. En general, el tiempo de la pared suele ser más corto (este es el tiempo medido por el reloj en la pared).\n",
    "\n",
    "#### -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Implementación de la normalización por lotes con Keras\n",
    "\n",
    "\n",
    "Como ocurre con la mayoría de las cosas con Keras, implementar la normalización por lotes es sencillo e intuitivo. Simplemente agregue una capa `BatchNormalization` antes o después de la función de activación de cada capa oculta. También puede agregar una capa BN como primera capa en su modelo, pero una capa de `Normalization` simple generalmente funciona igual de bien en esta ubicación (su único inconveniente es que primero debe llamar a su método `adapt()`). Por ejemplo, este modelo aplica BN después de cada capa oculta y como primera capa del modelo (después de aplanar las imágenes de entrada):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90d7a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 12:58:20.192074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334bd79c",
   "metadata": {},
   "source": [
    "¡Eso es todo! En este pequeño ejemplo, con solo dos capas ocultas, es poco probable que la normalización por lotes tenga un gran impacto, pero para las redes más profundas puede marcar una gran diferencia.\n",
    "\n",
    "Vamos a mostrar el resumen del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e1443ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf80ed2",
   "metadata": {},
   "source": [
    "Como puede ver, cada capa de BN añade cuatro parámetros por entrada: γ, β, μ y σ (por ejemplo, la primera capa de BN añade 3136 parámetros, que es 4 × 784). Los dos últimos parámetros, μ y σ, son los promedios móviles; no se ven afectados por la contrapropagación, por lo que Keras los llama \"no entrenables\"⁠ (si cuenta el número total de parámetros BN, 3.136 + 1.200 + 400, y divide por 2, obtiene 2.368, que es el número total de parámetros no entrenables en este modelo).\n",
    "\n",
    "Echemos un vistazo a los parámetros de la primera capa de BN. Dos se pueden entrenar (por contrapropagación) y dos no lo son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "065fc7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa9f6fd",
   "metadata": {},
   "source": [
    "Los autores del artículo de BN argumentaron a favor de agregar las capas de BN antes de las funciones de activación, en lugar de después (como acabamos de hacer). Existe cierto debate sobre esto, ya que cuál es preferible parece depender de la tarea; también puedes experimentar con esto para ver qué opción funciona mejor en tu conjunto de datos. Para agregar las capas BN antes de la función de activación, debe eliminar las funciones de activación de las capas ocultas y agregarlas como capas separadas después de las capas BN. Además, dado que una capa de normalización por lotes incluye un parámetro de compensación por entrada, puede eliminar el término de sesgo de la capa anterior pasando `use_bias=False` al crearlo. Por último, normalmente puede soltar la primera capa de BN para evitar intercalar la primera capa oculta entre dos capas de BN. El código actualizado se ve así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144b5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de5e33",
   "metadata": {},
   "source": [
    "La clase `BatchNormalization` tiene bastantes hiperparámetros que puedes modificar. Los valores predeterminados normalmente estarán bien, pero ocasionalmente es posible que necesites modificar el impulso (`momentum`). Este hiperparámetro lo utiliza la capa `BatchNormalization` cuando actualiza los promedios móviles exponenciales; dado un nuevo valor v (es decir, un nuevo vector de medias de entrada o desviaciones estándar calculadas sobre el lote actual), la capa actualiza el promedio móvil **V^**\n",
    "usando la siguiente ecuación:\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/CBbq6F7/Captura-de-pantalla-2024-02-29-a-las-13-01-55.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-13-01-55\" border=\"0\"></a>\n",
    "\n",
    "Un buen valor de momentum suele estar cerca de 1; por ejemplo, 0,9, 0,99 o 0,999. Quieres más 9 para conjuntos de datos más grandes y para minilotes más pequeños.\n",
    "\n",
    "Otro hiperparámetro importante es `axis`: determina qué eje debe normalizarse. Por defecto es -1, lo que significa que por defecto normalizará el último eje (utilizando las medias y las desviaciones estándar calculadas a través de los otros ejes). Cuando el lote de entrada es 2D (es decir, la forma del lote es [tamaño del lote, características]), esto significa que cada característica de entrada se normalizará en función de la media y la desviación estándar calculadas en todas las instancias del lote. Por ejemplo, la primera capa BN en el ejemplo de código anterior normalizará de forma independiente (y reescalará y cambiará) cada una de las características de entrada 784. Si movemos la primera capa BN antes de la capa de `Flatten`, entonces los lotes de entrada serán 3D, con forma [tamaño del lote, altura, ancho]; por lo tanto, la capa BN calculará 28 medias y 28 desviaciones estándar (1 por columna de píxeles, calculada en todas las instancias del lote y en todas las filas de la columna), y normalizará todos los píxeles de una columna determinada utilizando la misma media y desviación estándar. También habrá solo 28 parámetros de escala y 28 parámetros de cambio. Si en su lugar todavía quieres tratar cada uno de los 784 píxeles de forma independiente, entonces deberías establecer `axis=[1, 2]`\n",
    "\n",
    "La normalización por lotes se ha convertido en una de las capas más utilizadas en las redes neuronales profundas, especialmente en las redes neuronales convolucionales profundas discutidas en (Capítulo 14), hasta el punto de que a menudo se omite en los diagramas de arquitectura: se supone que BN se agrega después de cada capa. Ahora echemos un vistazo a una última técnica para estabilizar los gradientes durante el entrenamiento: el recorte de gradientes.\n",
    "\n",
    "\n",
    "## Recorte de gradiente (Clipping Gradient)\n",
    "\n",
    "Otra técnica para mitigar el problema de los gradientes de explosión es recortar los gradientes durante la contrapropagación para que nunca superen algún umbral. Esto se llama recorte de gradiente.⁠ Esta técnica se utiliza generalmente en redes neuronales recurrentes, donde el uso de la normalización por lotes es complicado (como se verá en el capítulo 15).\n",
    "\n",
    "En Keras, implementar el recorte de gradiente es solo una cuestión de configurar el argumento `clipvalue` o `clipnorm` al crear un optimizador, como este:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9f7f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compile() got multiple values for argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(clipvalue\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;31mTypeError\u001b[0m: compile() got multiple values for argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
    "\n",
    "model.compile([...], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a0a91a",
   "metadata": {},
   "source": [
    "Este optimizador recortará cada componente del vector de gradiente a un valor entre –1,0 y 1,0. Esto significa que todas las derivadas parciales de la pérdida (con respecto a todos y cada uno de los parámetros entrenables) se recortarán entre –1,0 y 1,0. El umbral es un hiperparámetro que puede ajustar. Tenga en cuenta que puede cambiar la orientación del vector de gradiente. Por ejemplo, si el vector de gradiente original es [0,9, 100,0], apunta principalmente en la dirección del segundo eje; pero una vez que lo recortas por valor, obtienes [0.9, 1.0], que apunta aproximadamente a la diagonal entre los dos ejes. En la práctica, este enfoque funciona bien. Si desea asegurarse de que el recorte de degradado no cambie la dirección del vector de degradado, debe recortar según la norma configurando `clipnorm` en lugar de `clipvalue`. Esto recortará todo el gradiente si su norma ℓ2 es mayor que el umbral que eligió. Por ejemplo, si establece `clipnorm=1.0`, entonces el vector [0.9, 100.0] se recortará a [0.00899964, 0.9999595], conservando su orientación pero casi eliminando el primer componente. Si observa que los gradientes explotan durante el entrenamiento (puede realizar un seguimiento del tamaño de los gradientes usando TensorBoard), puede intentar recortar por valor o recortar por norma, con diferentes umbrales, y ver qué opción funciona mejor en el conjunto de validación.\n",
    "\n",
    "\n",
    "## Reutilización de capas preentrenadas\n",
    "\n",
    "\n",
    "Por lo general, no es una buena idea entrenar un DNN muy grande desde cero sin tratar primero de encontrar una red neuronal existente que realice una tarea similar a la que está tratando de abordar (discutiré cómo encontrarlos en el Capítulo 14). Si encuentras una red neuronal, entonces generalmente puedes reutilizar la mayoría de sus capas, excepto las superiores. Esta técnica se llama aprendizaje por transferencia. No solo acelerará el entrenamiento considerablemente, sino que también requerirá significativamente menos datos de entrenamiento.\n",
    "\n",
    "Supongamos que tiene acceso a un DNN que fue entrenado para clasificar imágenes en 100 categorías diferentes, incluyendo animales, plantas, vehículos y objetos cotidianos, y ahora quiere entrenar a un DNN para clasificar tipos específicos de vehículos. Estas tareas son muy similares, incluso parcialmente superpuestas, por lo que debe intentar reutilizar partes de la primera red (ver Figura 11-5).\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "Si las imágenes de entrada para su nueva tarea no tienen el mismo tamaño que las utilizadas en la tarea original, generalmente tendrá que agregar un paso de preprocesamiento para cambiar su tamaño al tamaño esperado por el modelo original. De manera más general, el aprendizaje por transferencia funcionará mejor cuando las entradas tengan características similares de bajo nivel.\n",
    "\n",
    "#### ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1105.png)\n",
    "\n",
    "(_Figura 11-5. Reutilización de capas preentrenadas_)\n",
    "\n",
    "La capa de salida del modelo original generalmente debe reemplazarse porque lo más probable es que no sea útil en absoluto para la nueva tarea, y probablemente no tenga el número correcto de salidas.\n",
    "\n",
    "Del mismo modo, es menos probable que las capas ocultas superiores del modelo original sean tan útiles como las capas inferiores, ya que las características de alto nivel que son más útiles para la nueva tarea pueden diferir significativamente de las que fueron más útiles para la tarea original. Quieres encontrar el número correcto de capas para reutilizar.\n",
    "\n",
    "#### TIP\n",
    "\n",
    "Cuanto más similares sean las tareas, más capas querrás reutilizar (comenzo con las capas inferiores). Para tareas muy similares, intente mantener todas las capas ocultas y simplemente reemplace la capa de salida.\n",
    "\n",
    "#### ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "Intente congelar primero todas las capas reutilizadas (es decir, haga que sus pesos no se puedan entrenar para que el descenso del gradiente no los modifique y permanezcan fijos), luego entrene su modelo y vea cómo funciona. Luego intente descongelar una o dos de las capas ocultas superiores para dejar que la contrapropagación las modifique y ver si el rendimiento mejora. Cuantos más datos de entrenamiento tengas, más capas puedes descongelar. También es útil reducir la tasa de aprendizaje cuando se descongelan las capas reutilizadas: esto evitará destruir sus pesos afinados.\n",
    "\n",
    "Si todavía no puedes obtener un buen rendimiento y tienes pocos datos de entrenamiento, intenta eliminar la(s) capa(s) oculta(s) superior(s) y congelar todas las capas ocultas restantes de nuevo. Puedes iterar hasta que encuentres el número correcto de capas para reutilizar. Si tienes muchos datos de entrenamiento, puedes intentar reemplazar las capas ocultas superiores en lugar de soltarlas, e incluso añadir más capas ocultas.\n",
    "\n",
    "\n",
    "## Transferir el aprendizaja con Keras\n",
    "\n",
    "\n",
    "Echemos un vistazo a un ejemplo. Supongamos que el conjunto de datos de Fashion MNIST solo contenía ocho clases, por ejemplo, todas las clases, excepto la sandalia y la camisa. Alguien construyó y entrenó un modelo de Keras en ese set y obtujo un rendimiento razonablemente bueno (>90 % de precisión). Llamemos a este modelo A. Ahora quieres abordar una tarea diferente: tienes imágenes de camisetas y jerséis, y quieres entrenar un clasificador binario: positivo para camisetas (y tops), negativo para sandalias. Su conjunto de datos es bastante pequeño; solo tiene 200 imágenes etiquetadas. Cuando entrenas un nuevo modelo para esta tarea (llamémoslo modelo B) con la misma arquitectura que el modelo A, obtienes un 91,85 % de precisión de prueba. Mientras bebes tu café de la mañana, te das cuenta de que tu tarea es bastante similar a la tarea A, así que ¿tal vez transferir el aprendizaje pueda ayudar? ¡Vamos a averiguarlo!\n",
    "\n",
    "En primer lugar, necesitas cargar el modelo A y crear un nuevo modelo basado en las capas de ese modelo. Usted decide reutilizar todas las capas excepto la capa de salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bda537",
   "metadata": {},
   "outputs": [],
   "source": [
    "[...]  # Assuming model A was already trained and saved to \"my_model_A\"\n",
    "model_A = tf.keras.models.load_model(\"my_model_A\")\n",
    "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f44805",
   "metadata": {},
   "source": [
    "Tenga en cuenta que `model_A` y `model_B_on_A` ahora comparten algunas capas. Cuando entrenes `model_B_on_A`, también afectará a `model_A`. Si quieres evitar eso, necesitas clonar `model_A` antes de reutilizar sus capas. Para hacer esto, clonas la arquitectura del modelo A con `clone_model()` y luego copias sus pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18063982",
   "metadata": {},
   "source": [
    "#### ADVERTENCIA\n",
    "\n",
    "`tf.keras.models.clone_model()` solo clona la arquitectura, no los pesos. Si no los copia manualmente usando `set_weights()`, se inicializarán al azar cuando se utilice por primera vez el modelo clonado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48845d",
   "metadata": {},
   "source": [
    "Ahora podría entrenar `model_B_on_A` para la tarea B, pero dado que la nueva capa de salida se inicializó aleatoriamente, cometerá grandes errores (al menos durante las primeras épocas), por lo que habrá grandes gradientes de error que pueden arruinar los pesos reutilizados. Para evitar esto, un enfoque es congelar las capas reutilizadas durante las primeras épocas, dándole a la nueva capa algo de tiempo para aprender pesos razonables. Para hacer esto, establezca el atributo `trainable` de cada capa en `False` y compile el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e6bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1806a",
   "metadata": {},
   "source": [
    "#### NOTA\n",
    "\n",
    "Siempre debe compilar su modelo después de congelar o descongelar las capas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58c4b0",
   "metadata": {},
   "source": [
    "Ahora puede entrenar el modelo durante algunas épocas, luego descongelar las capas reutilizadas (lo que requiere compilar el modelo de nuevo) y continuar entrenando para ajustar las capas reutilizadas para la tarea B. Después de descongelar las capas reutilizadas, por lo general es una buena idea reducir la tasa de aprendizaje, una vez más para evitar dañar los pesos reutilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83796bf1",
   "metadata": {},
   "source": [
    "Entonces, ¿cuál es el veredicto final? Bueno, la precisión de la prueba de este modelo es del 93,85 %, ¡exactamente dos puntos porcentuales más que el 91,85 %! Esto significa que el aprendizaje por transferencia redujo la tasa de error en casi un 25 %:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ccc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87af19",
   "metadata": {},
   "source": [
    "¿Estás convencido? No deberías serlo: ¡yo engañé! Probé muchas configuraciones hasta que encontré una que demostró una gran mejora. Si intentas cambiar las clases o la semilla aleatoria, verás que la mejora generalmente cae, o incluso desaparece o se invierte. Lo que hice se llama \"torturar los datos hasta que se confiesen\". Cuando un artículo se ve demasiado positivo, deberías sospechar: tal vez la nueva técnica llamativa en realidad no ayude mucho (de hecho, incluso puede degradar el rendimiento), pero los autores probaron muchas variantes e informaron solo de los mejores resultados (lo que puede deberse a la pura suerte), sin mencionar cuántos fracasos encontraron en el camino. La mayoría de las veces, esto no es malicioso en absoluto, pero es parte de la razón por la que tantos resultados en la ciencia nunca se pueden reproducir.\n",
    "\n",
    "¿Por qué hice trampa? Resulta que el aprendizaje por transferencia no funciona muy bien con redes pequeñas y densas, presumiblemente porque las redes pequeñas aprenden pocos patrones, y las redes densas aprenden patrones muy específicos, que es poco probable que sean útiles en otras tareas. El aprendizaje por transferencia funciona mejor con redes neuronales convolucionales profundas, que tienden a aprender detectores de características que son mucho más generales (especialmente en las capas inferiores). Volveremos a revisar el aprendizaje por transferencia en el capítulo 14, utilizando las técnicas que acabamos de discutir (¡y esta vez no habrá trampas, lo prometo!).\n",
    "\n",
    "\n",
    "### Preentreno no supervisado\n",
    "\n",
    "Supongamos que quieres abordar una tarea compleja para la que no tienes muchos datos de entrenamiento etiquetados, pero desafortunadamente no puedes encontrar un modelo entrenado en una tarea similar. ¡No pierdas la esperanza! En primer lugar, debe tratar de recopilar más datos de entrenamiento etiquetados, pero si no puede, es posible que aún pueda realizar un entrenamiento previo sin supervisión (ver Figura 11-6). De hecho, a menudo es barato reunir ejemplos de entrenamiento sin etiquetar, pero caro etiquetarlos. Si puede recopilar una gran gran focía de datos de entrenamiento sin etiquetar, puede intentar usarlos para entrenar un modelo no supervisado, como un autocodificador o una red adversaria generativa (GAN; véase el capítulo 17). Luego puede reutilizar las capas inferiores del autoencoder o las capas inferiores del discriminador del GAN, agregar la capa de salida para su tarea en la parte superior y ajustar la red final utilizando el aprendizaje supervisado (es decir, con los ejemplos de entrenamiento etiquetados).\n",
    "\n",
    "Es esta técnica que Geoffrey Hinton y su equipo utilizaron en 2006, y que llevó al renacimiento de las redes neuronales y al éxito del aprendizaje profundo. Hasta 2010, el preentrenamiento no supervisado, generalmente con máquinas Boltzmann restringidas (RBM; ver el cuaderno en https://homl.info/extra-anns), era la norma para las redes profundas, y solo después de que se aliviara el problema de los gradientes de desaparición se hizo mucho más común entrenar DNN puramente utilizando el aprendizaje supervisado. El preentrenamiento no supervisado (hoy en día generalmente usando autocodificadores o GAN en lugar de RBM) sigue siendo una buena opción cuando tienes una tarea compleja que resolver, no hay un modelo similar que puedas reutilizar y pocos datos de entrenamiento etiquetados, pero muchos datos de entrenamiento sin etiquetar.\n",
    "\n",
    "Tenga en cuenta que en los primeros días del aprendizaje profundo era difícil entrenar modelos profundos, por lo que la gente usaría una técnica llamada preentrenamiento en capas codiciosas (que se representa en la Figura 11-6). Primero entrenarían a un modelo no supervisado con una sola capa, normalmente una RBM, luego congelarían esa capa y agregarían otra encima de ella, luego entrenarían el modelo de nuevo (efectivamente solo entrenando la nueva capa), luego congelarían la nueva capa y agregarían otra capa encima de ella, entrenarían al modelo de nuevo, y así sucesivo. Hoy en día, las cosas son mucho más simples: la gente generalmente entrena el modelo completo sin supervisión de una sola vez y usa autocodificadores o GAN en lugar de RBM.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1106.png)\n",
    "\n",
    "(_Figura 11-6. En la capacitación no supervisada, un modelo se entrena en todos los datos, incluidos los datos no etiquetados, utilizando una técnica de aprendizaje no supervisada, luego se ajusta a la tarea final en solo los datos etiquetados utilizando una técnica de aprendizaje supervisada; la parte no supervisada puede entrenar una capa a la vez como se muestra aquí, o puede entrenar el modelo completo directamente_)\n",
    "\n",
    "\n",
    "### Preentrenamiento en una tarea auxiliar\n",
    "\n",
    "Si no tiene muchos datos de entrenamiento etiquetados, una última opción es entrenar una primera red neuronal en una tarea auxiliar para la que pueda obtener o generar fácilmente datos de entrenamiento etiquetados, y luego reutilizar las capas inferiores de esa red para su tarea real. Las capas inferiores de la primera red neuronal aprenderán detectores de características que probablemente serán reutilizables por la segunda red neuronal.\n",
    "\n",
    "Por ejemplo, si quieres construir un sistema para reconocer caras, es posible que solo tengas unas pocas fotos de cada individuo, claramente no lo suficiente como para entrenar a un buen clasificador. Reunir cientos de fotos de cada persona no sería práctico. Sin embargo, podrías reunir muchas imágenes de personas al azar en la web y entrenar una primera red neuronal para detectar si dos imágenes diferentes presentan o no a la misma persona. Tal red aprendería buenos detectores de características para las caras, por lo que reutilizar sus capas inferiores le permitiría entrenar a un buen clasificador de caras que utilice pocos datos de entrenamiento.\n",
    "\n",
    "Para las aplicaciones de procesamiento de lenguaje natural (PNL), puede descargar un corpus de millones de documentos de texto y generar automáticamente datos etiquetados a partir de él. Por ejemplo, podrías enmascarar al azar algunas palabras y entrenar a un modelo para predecir cuáles son las palabras que faltan (por ejemplo, debería predecir que la palabra que falta en la frase \"¿Qué ___ estás diciendo?\" es probablemente \"son\" o \"eran\"). Si puede entrenar a un modelo para lograr un buen rendimiento en esta tarea, entonces ya sabrá mucho sobre el lenguaje, y sin duda puede reutilizarlo para su tarea real y ajustarlo en sus datos etiquetados (discutiremos más tareas de pre-entrenamiento en el capítulo 15).\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "El aprendizaje autosupervisado es cuando genera automáticamente las etiquetas a partir de los datos en sí, como en el ejemplo de enmascaramiento de texto, luego entrena un modelo en el conjunto de datos \"etiquetado\" resultante utilizando técnicas de aprendizaje supervisado.\n",
    "\n",
    "#### ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## Optimizadores más rápidos\n",
    "\n",
    "El entrenamiento de una red neuronal profunda muy grande puede ser dolorosamente lento. Hasta ahora hemos visto cuatro formas de acelerar el entrenamiento (y llegar a una mejor solución): aplicar una buena estrategia de inicialización para los pesos de conexión, usar una buena función de activación, usar la normalización por lotes y reutilizar partes de una red preentrenada (posiblemente construida para una tarea auxiliar o usar el aprendizaje no supervisado). Otro gran aumento de velocidad proviene del uso de un optimizador más rápido que el optimizador de descenso de gradiente normal. En esta sección presentaremos los algoritmos de optimización más populares: momentum, gradiente acelerado de Nesterov, AdaGrad, RMSProp y, finalmente, Adam y sus variantes.\n",
    "\n",
    "\n",
    "### Momentum (momento)\n",
    "\n",
    "Imagina una bola de bolos rodando por una pendiente suave sobre una superficie lisa: comenzará lentamente, pero rápidamente recuperará el momentum hasta que finalmente alcance la velocidad terminal (si hay alguna fricción o resistencia al aire). Esta es la idea central detrás de la optimización del momentum, propuesta por Boris Polyak en 1964.⁠ Por el contrario, el descenso de gradiente regular dará pequeños pasos cuando la pendiente sea suave y grandes pasos cuando la pendiente sea empinada, pero nunca tomará velocidad. Como resultado, el descenso de gradiente regular es generalmente mucho más lento para alcanzar el mínimo que la optimización del momentum.\n",
    "\n",
    "Recuerde que el descenso del gradiente actualiza los pesos θ restando directamente el gradiente de la función de costo J(θ) con respecto a los pesos (∇θJ(θ)) multiplicados por la tasa de aprendizaje η. La ecuación es θ ← θ – η∇θJ(θ). No le importa cuáles eran los gradientes anteriores. Si el gradiente local es pequeño, va muy despacio.\n",
    "\n",
    "La optimización del momento se preocupa mucho por lo que eran los gradientes anteriores: en cada iteración, resta el gradiente local del vector de momento m (multiplicado por la tasa de aprendizaje η), y actualiza los pesos agregando este vector de momentum (ver Ecuación 11-5). En otras palabras, el gradiente se utiliza como una aceleración, no como una velocidad. Para simular algún tipo de mecanismo de fricción y evitar que el momento crezca demasiado, el algoritmo introduce un nuevo hiperparámetro β, llamado momentum, que debe establecerse entre 0 (alta fricción) y 1 (sin fricción). Un valor de momentum típico es 0,9.\n",
    "\n",
    "#### Ecuación 11-5. Algoritmo de momentum\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/m5V8mLh/Captura-de-pantalla-2024-02-29-a-las-13-19-59.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-13-19-59\" border=\"0\"></a>\n",
    "\n",
    "Puede verificar que si el gradiente permanece constante, la velocidad terminal (es decir, el tamaño máximo de las actualizaciones de peso) es igual a ese gradiente multiplicado por la tasa de aprendizaje η multiplicada por 1 / (1 - β) (ignorando el signo). Por ejemplo, si β = 0,9, entonces la velocidad terminal es igual a 10 veces el gradiente multiplicado por la tasa de aprendizaje, ¡por lo que la optimización del momemtum termina siendo 10 veces más rápido que el descenso del gradiente! Esto permite que la optimización del momemtum escape de las mesetas mucho más rápido que el descenso del gradiente. Vimos en el capítulo 4 que cuando las entradas tienen escalas muy diferentes, la función de costo se verá como un tazón alargado (ver Figura 4-7). El descenso en pendiente baja por la empinada pendiente bastante rápido, pero luego se tarda mucho tiempo en bajar por el valle. Por el contrario, la optimización del momemtum rodará por el valle cada vez más rápido hasta que llegue al fondo (el óptimo). En las redes neuronales profundas que no utilizan la normalización por lotes, las capas superiores a menudo terminarán teniendo entradas con escalas muy diferentes, por lo que el uso de la optimización del momemtum ayuda mucho. También puede ayudar a superar la óptima local.\n",
    "\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "Debido al momemtum, el optimizador puede sobrecersar un poco, luego volver, sobresparar de nuevo y oscilar así muchas veces antes de estabilizarse como mínimo. Esta es una de las razones por las que es bueno tener un poco de fricción en el sistema: se deshace de estas oscilaciones y, por lo tanto, acelera la convergencia.\n",
    "\n",
    "#### ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Implementar la optimización del momemtum en Keras es una obviedad: simplemente use el optimizador `SGD` y establezca su hiperparámetro de `momentum`, luego recuéstese y obtenga ganancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d483211",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10543f9",
   "metadata": {},
   "source": [
    "El único inconveniente de la optimización del impulso es que añade otro hiperparámetro para afinar. Sin embargo, el valor de impulso de 0,9 generalmente funciona bien en la práctica y casi siempre va más rápido que el descenso de gradiente regular.\n",
    "\n",
    "### Gradiente acelerado de Nesterov\n",
    "\n",
    "Una pequeña variante de la optimización del impulso, propuesta por Yurii Nesterov en 1983,⁠ es casi siempre más rápida que la optimización del impulso regular. El método de gradiente acelerado de Nesterov (NAG), también conocido como optimización del momento de Nesterov, mide el gradiente de la función de costo no en la posición local θ, sino ligeramente por delante en la dirección del momento, en θ + βm (ver Ecuación 11-6).\n",
    "\n",
    "#### Ecuación 11-6. Algoritmo de gradiente acelerado de Nesterov\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/7r7MKGM/Captura-de-pantalla-2024-02-29-a-las-13-24-39.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-13-24-39\" border=\"0\"></a>\n",
    "\n",
    "Este pequeño ajuste funciona porque, en general, el vector de impulso apuntará en la dirección correcta (es decir, hacia el óptimo), por lo que será ligeramente más preciso usar el gradiente medido un poco más lejos en esa dirección en lugar del gradiente en la posición original, como se puede ver en la Figura 11-7 (donde ∇1 representa el gradiente de la función de costo medida en el punto de partida θ, y ∇2 representa el gradiente en el punto ubicado en θ + βm).\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1107.png)\n",
    "\n",
    "(_Figura 11-7. Optimización de impulso regular frente a Nesterov: el primero aplica los gradientes calculados antes del paso de impulso, mientras que el segundo aplica los gradientes calculados después_)\n",
    "\n",
    "Como puedes ver, la actualización de Nesterov termina más cerca de lo óptimo. Después de un tiempo, estas pequeñas mejoras se suman y NAG termina siendo significativamente más rápido que la optimización regular del impulso. Además, tenga en cuenta que cuando el momento empuja los pesos a través de un valle, ∇1 continúa empujando más a través del valle, mientras que ∇2 empuja hacia atrás hacia el fondo del valle. Esto ayuda a reducir las oscilaciones y, por lo tanto, el NAG converge más rápido.\n",
    "\n",
    "Para usar NAG, simplemente establece `nesterov=True` cuando crees el optimizador `SGD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e902fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6bea70",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "Considere de nuevo el problema del cuenco alargado: el descenso de gradiente comienza bajando rápidamente por la pendiente más empinada, que no apunta directamente hacia el óptimo global, luego baja muy lentamente hasta el fondo del valle. Sería bueno que el algoritmo pudiera corregir su dirección antes para apuntar un poco más hacia el óptimo global. El algoritmo AdaGrad⁠ logra esta corrección reduciendo el vector de gradiente a lo largo de las dimensiones más empinadas (ver Ecuación 11-7).\n",
    "\n",
    "#### Ecuación 11-7. Algoritmo de AdaGrad\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/SwqWND2/Captura-de-pantalla-2024-02-29-a-las-13-27-01.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-13-27-01\" border=\"0\"></a>\n",
    "\n",
    "El primer paso acumula el cuadrado de los gradientes en el vector s (recuerde que el símbolo ⊗ representa la multiplicación en cuanto a elementos). Esta forma vectorizada es equivalente a calcular si ← si + (∂ J(θ) / ∂ θi)2 para cada elemento si del vector s; en otras palabras, cada si acumula los cuadrados de la derivada parcial de la función de costo con respecto al parámetro θi. Si la función de costo es empinada a lo largo de la enta dimensión, entonces si se hará cada vez más grande en cada iteración.\n",
    "\n",
    "El segundo paso es casi idéntico al descenso de gradiente, pero con una gran diferencia: el vector de gradiente se reduce en un factor de **sqrt(S+ε)** (el símbolo ⊘ representa la división en cuanto a elementos, y ε es un término suavizante para evitar la división por cero, normalmente establecido en 10-10). Esta forma vectorial es equivalente a la computación simultánea\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/X4mf5RB/Captura-de-pantalla-2024-02-29-a-las-13-30-34.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-13-30-34\" border=\"0\"></a>\n",
    "\n",
    "para todos los parámetros θi.\n",
    "\n",
    "En resumen, este algoritmo decae la tasa de aprendizaje, pero lo hace más rápido para dimensiones empinadas que para dimensiones con pendientes más suaves. Esto se llama tasa de aprendizaje adaptativa. Ayuda a apuntar las actualizaciones resultantes más directamente hacia el óptimo global (ver Figura 11-8). Un beneficio adicional es que requiere mucho menos ajuste del hiperparámetro de la tasa de aprendizaje η.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1108.png)\n",
    "\n",
    "(_Figure 11-8. AdaGrad versus gradient descent: the former can correct its direction earlier to point to the optimum_)\n",
    "\n",
    "AdaGrad frecuentemente funciona bien para problemas cuadráticos simples, pero a menudo se detiene demasiado pronto al entrenar redes neuronales: la tasa de aprendizaje se reduce tanto que el algoritmo termina deteniéndose por completo antes de alcanzar el óptimo global. Entonces, aunque Keras tiene un optimizador `Adagrad`, no debe usarlo para entrenar redes neuronales profundas (aunque puede ser eficiente para tareas más simples como la regresión lineal). Aún así, comprender AdaGrad es útil para comprender los otros optimizadores de tasa de aprendizaje adaptativo.\n",
    "\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "Como hemos visto, AdaGrad corre el riesgo de desacelerarse demasiado rápido y nunca converger al óptimo global. El algoritmo RMSProp⁠ soluciona este problema acumulando solo los gradientes de las iteraciones más recientes, en lugar de todos los gradientes desde el comienzo del entrenamiento. Lo hace utilizando una caída exponencial en el primer paso (consulte la ecuación 11-8).\n",
    "\n",
    "#### Equation 11-8. Algoritmo RMSProp\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/1850qCH/Captura-de-pantalla-2024-02-29-a-las-18-13-01.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-18-13-01\" border=\"0\"></a>\n",
    "\n",
    "La tasa de caída ρ generalmente se establece en 0,9.⁠ Sí, una vez más es un hiperparámetro nuevo, pero este valor predeterminado a menudo funciona bien, por lo que es posible que no necesite ajustarlo en absoluto.\n",
    "\n",
    "Como era de esperar, Keras tiene un optimizador `RMSprop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e7a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff5da7",
   "metadata": {},
   "source": [
    "Excepto en problemas muy simples, este optimizador casi siempre funciona mucho mejor que AdaGrad. De hecho, era el algoritmo de optimización preferido de muchos investigadores hasta que apareció la optimización de Adam.\n",
    "\n",
    "### Adam\n",
    "\n",
    "Adam,⁠ que significa estimación de momento adaptativo, combina las ideas de optimización de impulso y RMSProp: al igual que la optimización de impulso, realiza un seguimiento de un promedio que decae exponencialmente de gradientes pasados; y al igual que RMSProp, realiza un seguimiento de un promedio que decae exponencialmente de gradientes cuadrados pasados (consulte la ecuación 11-9). Estas son estimaciones de la media y la varianza (no centrada) de los gradientes. La media suele denominarse primer momento, mientras que la varianza suele denominarse segundo momento, de ahí el nombre del algoritmo.\n",
    "\n",
    "#### Ecuación 11-9. algoritmo de Adam\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/BLk35mL/Captura-de-pantalla-2024-02-29-a-las-18-16-56.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-18-16-56\" border=\"0\"></a>\n",
    "\n",
    "En esta ecuación, t representa el número de iteración (comenzando en 1).\n",
    "\n",
    "Si simplemente observa los pasos 1, 2 y 5, notará la gran similitud de Adam tanto con la optimización del impulso como con la RMSProp: β1 corresponde a β en la optimización del impulso y β2 corresponde a ρ en RMSProp. La única diferencia es que el paso 1 calcula un promedio que decrece exponencialmente en lugar de una suma que decae exponencialmente, pero en realidad son equivalentes excepto por un factor constante (el promedio decreciente es solo 1 – β1 veces la suma decreciente). Los pasos 3 y 4 son un detalle técnico: dado que mys se inicializan en 0, estarán sesgados hacia 0 al comienzo del entrenamiento, por lo que estos dos pasos ayudarán a impulsar mys al comienzo del entrenamiento.\n",
    "\n",
    "El hiperparámetro de caída de impulso β1 normalmente se inicializa a 0,9, mientras que el hiperparámetro de caída de escala β2 a menudo se inicializa a 0,999. Como antes, el término de suavizado ε generalmente se inicializa con un número pequeño como 10–7. Estos son los valores predeterminados para la clase `Adam`. A continuación se explica cómo crear un optimizador Adam usando Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a44953",
   "metadata": {},
   "source": [
    "Dado que Adam es un algoritmo de tasa de aprendizaje adaptativo, como AdaGrad y RMSProp, requiere menos ajuste del hiperparámetro de tasa de aprendizaje η. A menudo puedes usar el valor predeterminado η = 0,001, lo que hace que Adam sea aún más fácil de usar que el descenso de gradiente.\n",
    "\n",
    "#### TIP\n",
    "\n",
    "Si está empezando a sentirse abrumado por todas estas diferentes técnicas y se pregunta cómo elegir las adecuadas para su tarea, no se preocupe: al final de este capítulo se proporcionan algunas pautas prácticas.\n",
    "\n",
    "#### -------------------------------------------------------------------------------------\n",
    "\n",
    "Finalmente, vale la pena mencionar tres variantes de Adam: AdamMax, Nadam y AdamW.\n",
    "\n",
    "\n",
    "### AdaMax\n",
    "\n",
    "\n",
    "El artículo de Adam también presentó Adamax. Observe que en el paso 2 de la ecuación 11-9, Adam acumula los cuadrados de los gradientes en s (con un peso mayor para los gradientes más recientes). En el paso 5, si ignoramos ε y los pasos 3 y 4 (que de todos modos son detalles técnicos), Adam reduce las actualizaciones de parámetros en la raíz cuadrada de s. En resumen, Adam reduce las actualizaciones de parámetros según la norma ℓ2 de los gradientes decrecientes en el tiempo (recuerde que la norma ℓ2 es la raíz cuadrada de la suma de cuadrados).\n",
    "\n",
    "AdaMax reemplaza la norma ℓ2 con la norma ℓ∞ (una forma elegante de decir el máximo). Específicamente, reemplaza el paso 2 en la Ecuación 11-9 con\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/xjskb9F/Captura-de-pantalla-2024-02-29-a-las-18-22-04.png\" alt=\"Captura-de-pantalla-2024-02-29-a-las-18-22-04\" border=\"0\"></a>\n",
    "\n",
    ", abandona el paso 4 y, en el paso 5, reduce las actualizaciones de gradiente en un factor de s, que es el máximo del valor absoluto de los gradientes disminuidos en el tiempo.\n",
    "\n",
    "En la práctica, esto puede hacer que AdaMax sea más estable que Adam, pero realmente depende del conjunto de datos y, en general, Adam funciona mejor. Entonces, este es solo un optimizador más que puedes probar si tienes problemas con Adam en alguna tarea.\n",
    "\n",
    "\n",
    "### Nadam\n",
    "\n",
    "La optimización de Nadam es la optimización de Adam más el truco de Nesterov, por lo que a menudo convergerá un poco más rápido que Adam. En su informe que presenta esta técnica,⁠ el investigador Timothy Dozat compara muchos optimizadores diferentes en diversas tareas y descubre que Nadam generalmente supera a Adam, pero a veces es superado por RMSProp.\n",
    "\n",
    "\n",
    "### AdamW\n",
    "\n",
    "AdamW⁠ es una variante de Adam que integra una técnica de regularización llamada caída de peso. La caída de peso reduce el tamaño de los pesos del modelo en cada iteración de entrenamiento multiplicándolos por un factor de caída como 0,99. Esto puede recordarle a la regularización ℓ2 (introducida en el Capítulo 4), que también apunta a mantener los pesos pequeños y, de hecho, se puede demostrar matemáticamente que la regularización ℓ2 es equivalente a la disminución del peso cuando se usa SGD. Sin embargo, cuando se utiliza Adam o sus variantes, la regularización ℓ2 y la caída de peso no son equivalentes: en la práctica, combinar Adam con la regularización ℓ2 da como resultado modelos que a menudo no se generalizan tan bien como los producidos por SGD. AdamW soluciona este problema combinando adecuadamente a Adam con la pérdida de peso.\n",
    "\n",
    "\n",
    "#### CUIDADO!\n",
    "\n",
    "Los métodos de optimización adaptativa (incluida la optimización RMSProp, Adam, AdamMax, Nadam y AdamW) suelen ser excelentes y convergen rápidamente hacia una buena solución. Sin embargo, un artículo de 2017⁠ de Ashia C. Wilson et al. demostró que pueden conducir a soluciones que se generalizan mal en algunos conjuntos de datos. Entonces, cuando esté decepcionado por el rendimiento de su modelo, intente usar NAG en su lugar: es posible que su conjunto de datos sea alérgico a los gradientes adaptativos. Consulte también las últimas investigaciones, porque avanzan rápidamente.\n",
    "\n",
    "#### -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Para usar Nadam, AdaMax, o AdamW en Keras, cambia `tf.keras.optimizers.Adam` por `tf.keras.optimizers.Nadam`, `tf.keras.optimizers.Adamax`, o `tf.keras.optimizers.experimental.AdamW`. Para `AdamW`, probablemente quieras ajustar el hiperparámetro `weight_decay`.\n",
    "\n",
    "Todas las técnicas de optimización analizadas hasta ahora sólo se basan en las derivadas parciales de primer orden (jacobianas). La literatura sobre optimización también contiene algoritmos sorprendentes basados en las derivadas parciales de segundo orden (las hessianas, que son las derivadas parciales de las jacobianas). Desafortunadamente, estos algoritmos son muy difíciles de aplicar a redes neuronales profundas porque hay n2 hessianos por salida (donde n es el número de parámetros), en lugar de solo n jacobianos por salida. Dado que los DNN suelen tener decenas de miles de parámetros o más, los algoritmos de optimización de segundo orden a menudo ni siquiera caben en la memoria, e incluso cuando lo hacen, calcular los hessianos es demasiado lento.\n",
    "\n",
    "\n",
    "#### ENTRENAMIENTO DE MODELOS DISPERSOS\n",
    "\n",
    "Todos los algoritmos de optimización que acabamos de analizar producen modelos densos, lo que significa que la mayoría de los parámetros serán distintos de cero. Si necesita un modelo increíblemente rápido en tiempo de ejecución, o si necesita que ocupe menos memoria, es posible que prefiera terminar con un modelo disperso.\n",
    "\n",
    "Una forma de lograr esto es entrenar el modelo como de costumbre y luego deshacerse de los pesos pequeños (ponerlos a cero). Sin embargo, esto normalmente no dará como resultado un modelo muy escaso y puede degradar el rendimiento del modelo.\n",
    "\n",
    "Una mejor opción es aplicar una fuerte regularización ℓ1 durante el entrenamiento (verá cómo más adelante en este capítulo), ya que empuja al optimizador a poner a cero tantos pesos como pueda (como se analiza en “Regresión de lasso”).\n",
    "\n",
    "Si estas técnicas siguen siendo insuficientes, consulte el kit de herramientas de optimización de modelos de TensorFlow (TF-MOT), que proporciona una API de poda capaz de eliminar conexiones de forma iterativa durante el entrenamiento en función de su magnitud.\n",
    "\n",
    "#### -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "La Tabla 11-2 compara todos los optimizadores que hemos analizado hasta ahora (* es malo, ** es promedio y *** es bueno).\n",
    "\n",
    "(_Table 11-2. Optimizer comparison_)\n",
    "\n",
    "| Clase                            | Velocidad Convergencia | Calidad Convergencia |\n",
    "|----------------------------------|------------------------|----------------------|\n",
    "| SGD                              | *                      | ***                  |\n",
    "| SGD(momentum=...)                | **                     | ***                  |\n",
    "| SGD(momentum=..., nesterov=True) | **                     | ***                  |\n",
    "| Adagrad                          | ***                    | *(stops too early)   |\n",
    "| RMSprop                          | ***                    | **or***              |\n",
    "| Adam                             | ***                    | **or***              |\n",
    "| AdaMax                           | ***                    | **or***              |\n",
    "| Nadam                            | ***                    | **or***              |\n",
    "| AdamW                            | ***                    | **or***              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af5d8f",
   "metadata": {},
   "source": [
    "## Programación de la tasa de aprendizaje\n",
    "\n",
    "\n",
    "Encontrar una buena tasa de aprendizaje es muy importante. Si lo configuras demasiado alto, el entrenamiento puede divergir (como se explica en “Descenso de gradiente”). Si lo estableces demasiado bajo, el entrenamiento eventualmente convergerá al óptimo, pero llevará mucho tiempo. Si lo configuras un poco demasiado alto, progresará muy rápidamente al principio, pero terminará bailando alrededor del óptimo y nunca se estabilizará realmente. Si tiene un presupuesto informático limitado, es posible que tenga que interrumpir el entrenamiento antes de que converja adecuadamente, lo que producirá una solución subóptima (consulte la Figura 11-9).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1109.png)\n",
    "\n",
    "(_Figure 11-9. Learning curves for various learning rates η_)\n",
    "\n",
    "Como se analizó en el Capítulo 10, puede encontrar una buena tasa de aprendizaje entrenando el modelo durante unos cientos de iteraciones, aumentando exponencialmente la tasa de aprendizaje desde un valor muy pequeño a un valor muy grande, y luego observando la curva de aprendizaje y eligiendo una tasa de aprendizaje. tasa ligeramente inferior a aquella en la que la curva de aprendizaje comienza a dispararse. Luego puedes reinicializar tu modelo y entrenarlo con esa tasa de aprendizaje.\n",
    "\n",
    "Pero puede hacerlo mejor que una tasa de aprendizaje constante: si comienza con una tasa de aprendizaje alta y luego la reduce una vez que el entrenamiento deja de progresar rápidamente, puede alcanzar una buena solución más rápido que con la tasa de aprendizaje constante óptima. Existen muchas estrategias diferentes para reducir la tasa de aprendizaje durante el entrenamiento. También puede resultar beneficioso comenzar con una tasa de aprendizaje baja, aumentarla y luego bajarla nuevamente. Estas estrategias se denominan programas de aprendizaje (introduje brevemente este concepto en el Capítulo 4). Estos son los horarios de aprendizaje más utilizados:\n",
    "\n",
    "* **Programación de energía**\n",
    "\n",
    "    Establezca la tasa de aprendizaje en función del número de iteración t: η(t) = η0 / (1 + t/s)c. La tasa de aprendizaje inicial η0, la potencia c (normalmente establecida en 1) y los pasos s son hiperparámetros. La tasa de aprendizaje cae en cada paso. Después de s pasos, la tasa de aprendizaje desciende a η0/2. Después de s pasos más, desciende a η0/3, luego desciende a η0/4, luego a η0/5, y así sucesivamente. Como puede ver, este horario primero disminuye rápidamente y luego cada vez más lentamente. Por supuesto, la programación de energía requiere ajustar η0 y s (y posiblemente c).\n",
    "\n",
    "\n",
    "- **Programación exponencial**\n",
    "\n",
    "    Establezca la tasa de aprendizaje en η(t) = η0 0,1t/s. La tasa de aprendizaje disminuirá gradualmente en un factor de 10 en cada paso. Mientras que la programación energética reduce la tasa de aprendizaje cada vez más lentamente, la programación exponencial sigue reduciéndola en un factor de 10 en cada s pasos.\n",
    "    \n",
    "    \n",
    "* **Programación constante por partes**\n",
    "\n",
    "    Utilice una tasa de aprendizaje constante para varias épocas (por ejemplo, η0 = 0,1 para 5 épocas), luego una tasa de aprendizaje menor para otra cantidad de épocas (por ejemplo, η1 = 0,001 para 50 épocas), y así sucesivamente. Aunque esta solución puede funcionar muy bien, requiere experimentar para determinar la secuencia correcta de tasas de aprendizaje y cuánto tiempo usar cada una de ellas.\n",
    "\n",
    "- **Programación de desempeño**\n",
    "\n",
    "    Mida el error de validación cada N pasos (al igual que para la parada anticipada) y reduzca la tasa de aprendizaje en un factor de λ cuando el error deje de disminuir.\n",
    "    \n",
    "* **Programación de 1 ciclo**\n",
    "\n",
    "    1ciclo fue introducido en un artículo de 2018 por Leslie Smith.⁠24 A diferencia de los otros enfoques, comienza aumentando la tasa de aprendizaje inicial η0, creciendo linealmente hasta η1 a mitad del entrenamiento. Luego disminuye la tasa de aprendizaje linealmente hasta η0 nuevamente durante la segunda mitad del entrenamiento, terminando las últimas épocas reduciendo la tasa en varios órdenes de magnitud (aún linealmente). La tasa de aprendizaje máxima η1 se elige utilizando el mismo enfoque que utilizamos para encontrar la tasa de aprendizaje óptima, y la tasa de aprendizaje inicial η0 suele ser 10 veces menor. Cuando usamos un impulso, primero comenzamos con un impulso alto (por ejemplo, 0,95), luego lo bajamos a un impulso más bajo durante la primera mitad del entrenamiento (por ejemplo, hasta 0,85, linealmente) y luego lo volvemos a subir al máximo. valor máximo (por ejemplo, 0,95) durante la segunda mitad del entrenamiento, terminando las últimas épocas con ese valor máximo. Smith hizo muchos experimentos que demostraron que este enfoque a menudo podía acelerar considerablemente el entrenamiento y alcanzar un mejor rendimiento. Por ejemplo, en el popular conjunto de datos de imágenes CIFAR10, este enfoque alcanzó una precisión de validación del 91,9 % en solo 100 épocas, en comparación con una precisión del 90,3 % en 800 épocas mediante un enfoque estándar (con la misma arquitectura de red neuronal). Esta hazaña se denominó superconvergencia.\n",
    "    \n",
    "Un artículo de 2013 de Andrew Senior et al.⁠ comparó el rendimiento de algunos de los programas de aprendizaje más populares cuando se utiliza la optimización del impulso para entrenar redes neuronales profundas para el reconocimiento de voz. Los autores concluyeron que, en este contexto, tanto la programación de rendimiento como la programación exponencial funcionaron bien. Favorecían la programación exponencial porque era fácil de ajustar y convergía ligeramente más rápido hacia la solución óptima. También mencionaron que era más fácil de implementar que la programación del rendimiento, pero en Keras ambas opciones son fáciles. Dicho esto, el enfoque de 1 ciclo parece funcionar aún mejor.\n",
    "\n",
    "Implementar la programación de energía en Keras es la opción más sencilla: simplemente configure el hiperparámetro `decay` al crear un optimizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c72df",
   "metadata": {},
   "source": [
    "La `decay` es la inversa de s (el número de pasos que se necesitan para dividir la tasa de aprendizaje por una unidad más), y Keras supone que c es igual a 1.\n",
    "\n",
    "La programación exponencial y la programación por partes también son bastante simples. Primero debe definir una función que tome la época actual y devuelva la tasa de aprendizaje. Por ejemplo, implementemos la programación exponencial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26314752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1 ** (epoch / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f173a14",
   "metadata": {},
   "source": [
    "Si no desea codificar η0 y s, puede crear una función que devuelva una función configurada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8684112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 ** (epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff93318",
   "metadata": {},
   "source": [
    "A continuación, cree una devolución de llamada de `LearningRateScheduler`, dándole la función de programación y pase esta devolución de llamada al método `fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c2d45",
   "metadata": {},
   "source": [
    "`LearningRateScheduler` actualizará el atributo `learning_rate` del optimizador al comienzo de cada época. Actualizar la tasa de aprendizaje una vez por época suele ser suficiente, pero si desea que se actualice con más frecuencia, por ejemplo en cada paso, siempre puede escribir su propia devolución de llamada (consulte la sección \"Programación exponencial\" del cuaderno de este capítulo para ver un ejemplo). ). Actualizar la tasa de aprendizaje en cada paso puede ser útil si hay muchos pasos por época. Alternativamente, puede utilizar el enfoque `tf.keras.optimiz⁠ers.schedules`, que se describe en breve.\n",
    "\n",
    "#### TIP\n",
    "\n",
    "Después del entrenamiento, `history.history[\"lr\"]` le brinda acceso a la lista de tasas de aprendizaje utilizadas durante el entrenamiento.\n",
    "\n",
    "#### ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Opcionalmente, la función de programación puede tomar la tasa de aprendizaje actual como segundo argumento. Por ejemplo, la siguiente función de programación multiplica la tasa de aprendizaje anterior por 0,1^(1/20), lo que da como resultado la misma caída exponencial (excepto que la caída ahora comienza al comienzo de la época 0 en lugar de 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c1d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1 ** (1 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2f4af",
   "metadata": {},
   "source": [
    "Esta implementación se basa en la tasa de aprendizaje inicial del optimizador (a diferencia de la implementación anterior), así que asegúrese de configurarla adecuadamente.\n",
    "\n",
    "Cuando guarda un modelo, el optimizador y su tasa de aprendizaje se guardan junto con él. Esto significa que con esta nueva función de programación, puedes simplemente cargar un modelo entrenado y continuar entrenando donde lo dejó, sin problema. Sin embargo, las cosas no son tan simples si su función de programación usa el argumento `epoch`: la época no se guarda y se restablece a 0 cada vez que llama al método `fit()`. Si continuara entrenando un modelo donde lo dejó, esto podría generar una tasa de aprendizaje muy alta, lo que probablemente dañaría los pesos de su modelo. Una solución es configurar manualmente el argumento de la `initial_epoch` del método `fit()` para que la época comience en el valor correcto.\n",
    "\n",
    "Para una programación constante por partes, puede usar una función de programación como la siguiente (como antes, puede definir una función más general si lo desea; consulte la sección \"Programación constante por partes\" del cuaderno para ver un ejemplo), luego cree un `LearningRateScheduler` Devuelve la llamada con esta función y pásala al método `fit()`, al igual que para la programación exponencial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69e9e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0792ced",
   "metadata": {},
   "source": [
    "Para la programación del rendimiento, utilice la devolución de llamada `ReduceLROnPlateau`. Por ejemplo, si pasa la siguiente devolución de llamada al método `fit()`, multiplicará la tasa de aprendizaje por 0,5 siempre que la mejor pérdida de validación no mejore durante cinco épocas consecutivas (hay otras opciones disponibles; consulte la documentación para obtener más detalles). :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8280b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e9468",
   "metadata": {},
   "source": [
    "Por último, Keras ofrece una forma alternativa de implementar la programación de la tasa de aprendizaje: puede definir una tasa de aprendizaje programada utilizando una de las clases disponibles en `tf.keras.opti⁠mizers.schedules` y luego pasarla a cualquier optimizador. Este enfoque actualiza la tasa de aprendizaje en cada paso en lugar de en cada época. Por ejemplo, aquí se explica cómo implementar el mismo programa exponencial que la función `exponential_decay_fn()` que definimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 25\n",
    "n_steps = n_epochs * math.ceil(len(X_train) / batch_size)\n",
    "scheduled_learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01, decay_steps=n_steps, decay_rate=0.1)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=scheduled_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2b4b5",
   "metadata": {},
   "source": [
    "Esto es agradable y simple, además, cuando guardas el modelo, la tasa de aprendizaje y su cronograma (incluido su estado) también se guardan.\n",
    "\n",
    "En cuanto a 1cycle, Keras no lo admite, pero es posible implementarlo en menos de 30 líneas de código creando una devolución de llamada personalizada que modifica la tasa de aprendizaje en cada iteración. Para actualizar la tasa de aprendizaje del optimizador desde el método `on_batch_begin()` de la devolución de llamada, debe llamar a `tf.keras.back⁠end.set_value(self.model.optimizer.learning_rate, new_learning_rate)`. Consulte la sección \"Programación de 1 ciclo\" del cuaderno para ver un ejemplo.\n",
    "\n",
    "En resumen, la caída exponencial, la programación del rendimiento y 1 ciclo pueden acelerar considerablemente la convergencia, ¡así que pruébalos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ddbc35",
   "metadata": {},
   "source": [
    "## Evitar el Sobreajuste a través de la regularización\n",
    "\n",
    "**_Con cuatro parámetros puedo encajar un elefante y con cinco puedo hacerle mover la trompa_**.\n",
    "\n",
    "- John von Neumann, citado por Enrico Fermi en Nature 427\n",
    "\n",
    "\n",
    "Con miles de parámetros, puedes adaptar todo el zoológico. Las redes neuronales profundas suelen tener decenas de miles de parámetros, a veces incluso millones. Esto les brinda una increíble libertad y significa que pueden adaptarse a una gran variedad de conjuntos de datos complejos. Pero esta gran flexibilidad también hace que la red sea propensa a sobreajustar el conjunto de entrenamiento. A menudo es necesaria la regularización para evitar esto.\n",
    "\n",
    "Ya implementamos una de las mejores técnicas de regularización en el Capítulo 10: la parada anticipada. Además, aunque la normalización por lotes se diseñó para resolver los problemas de gradientes inestables, también actúa como un regularizador bastante bueno. En esta sección examinaremos otras técnicas de regularización populares para redes neuronales: regularización ℓ1 y ℓ2, abandono y regularización de norma máxima.\n",
    "\n",
    "### Regularización ℓ1 y ℓ2\n",
    "\n",
    "Tal como lo hizo en el Capítulo 4 para modelos lineales simples, puede usar la regularización ℓ2 para restringir los pesos de conexión de una red neuronal y/o la regularización ℓ1 si desea un modelo disperso (con muchos pesos iguales a 0). A continuación se explica cómo aplicar la regularización ℓ2 a los pesos de conexión de una capa de Keras, utilizando un factor de regularización de 0,01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb7d762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13efbe",
   "metadata": {},
   "source": [
    "La función `l2()` devuelve un regularizador que se llamará en cada paso durante el entrenamiento para calcular la pérdida de regularización. Esto luego se suma a la pérdida final. Como era de esperar, puede usar `tf.keras.regularizers.l1()` si desea la regularización ℓ1; si desea la regularización ℓ1 y ℓ2, use `tf.keras.regularizers.l1_l2()` (especificando ambos factores de regularización).\n",
    "\n",
    "Dado que normalmente querrás aplicar el mismo regularizador a todas las capas de tu red, además de utilizar la misma función de activación y la misma estrategia de inicialización en todas las capas ocultas, es posible que te encuentres repitiendo los mismos argumentos. Esto hace que el código sea feo y propenso a errores. Para evitar esto, puedes intentar refactorizar tu código para usar bucles. Otra opción es usar la función `functools.partial()` de Python, que le permite crear un contenedor delgado para cualquier invocable, con algunos valores de argumento predeterminados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1529c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(tf.keras.layers.Dense,\n",
    "                           activation=\"relu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151b704",
   "metadata": {},
   "source": [
    "#### ADVERTENCIA\n",
    "\n",
    "Como vimos anteriormente, la regularización de ℓ2 está bien cuando se usa SGD, la optimización del momento y la optimización del momento de Nesterov, pero no con Adam y sus variantes. Si quieres usar Adam con decaimiento de peso, entonces no uses la regularización ℓ2: usa AdamW en su lugar.\n",
    "\n",
    "#### -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### Persona que deja los estudios\n",
    "\n",
    "La deserción es una de las técnicas de regularización más populares para las redes neuronales profundas. Fue propuesto en un artículo⁠ de Geoffrey Hinton et al. en 2012 y detallado en un documento de 2014⁠ de Nitish Srivastava et al., y ha demostrado ser un gran éxito: muchas redes neuronales de última generación utilizan la deserción, ya que les da un aumento de precisión del 1 % al 2 %. Puede que esto no parezca mucho, pero cuando un modelo ya tiene una precisión del 95 %, obtener un aumento de precisión del 2 % significa bajar la tasa de error en casi un 40 % (pasando del 5 % de error a aproximadamente el 3 %).\n",
    "\n",
    "Es un algoritmo bastante simple: en cada paso de entrenamiento, cada neurona (incluidas las neuronas de entrada, pero siempre excluyendo las neuronas de salida) tiene una probabilidad p de ser \"deser eliminada\" temporalmente, lo que significa que será ignorada por completo durante este paso de entrenamiento, pero puede estar activa durante el siguiente paso (ver Figura 11-10). El hiperparámetro p se llama la tasa de abandono, y normalmente se establece entre el 10% y el 50%: más cerca del 20%-30% en las redes neuronales recurrentes (ver Capítulo 15), y más cerca del 40%-50% en las redes neuronales convolucionales (ver Capítulo 14). Después del entrenamiento, las neuronas ya no se caen. Y eso es todo (excepto por un detalle técnico que discutiremos momentáneamente).\n",
    "\n",
    "Es sorprendente al principio que esta técnica destructiva funcione en absoluto. ¿Se desempeñaría mejor una empresa si se les dijera a sus empleados que lanzaran una moneda todas las mañanas para decidir si ir o no a trabajar? Bueno, quién sabe; ¡tal vez lo haría! La empresa se vería obligada a adaptar su organización; no podría confiar en ninguna persona para trabajar en la máquina de café o realizar cualquier otra tarea crítica, por lo que esta experiencia tendría que extenderse entre varias personas. Los empleados tendrían que aprender a cooperar con muchos de sus compañeros de trabajo, no solo con un puñado de ellos. La empresa se volvería mucho más resistente. Si una persona renunciara, no haría mucha diferencia. No está claro si esta idea realmente funcionaría para las empresas, pero ciertamente funciona para las redes neuronales. Las neuronas entrenadas con abandono no pueden coadaptarse con sus neuronas vecinas; tienen que ser lo más útiles posible por sí solas. Tampoco pueden depender excesivamente de solo unas pocas neuronas de entrada; deben prestar atención a cada una de sus neuronas de entrada. Terminan siendo menos sensibles a los ligeros cambios en las entradas. Al final, obtienes una red más robusta que se generaliza mejor.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1110.png)\n",
    "\n",
    "(_Figura 11-10. Con la regularización de abandono, en cada iteración de entrenamiento se \"descarta\" un subconjunto aleatorio de todas las neuronas en una o más capas, excepto la capa de salida; estas neuronas salen 0 en esta iteración (representadas por las flechas con dises)_)\n",
    "\n",
    "Otra forma de entender el poder del abandono es darse cuenta de que se genera una red neuronal única en cada paso del entrenamiento. Dado que cada neurona puede estar presente o ausente, hay un total de 2N redes posibles (donde N es el número total de neuronas que se pueden soltar). Este es un número tan grande que es prácticamente imposible que la misma red neuronal se muestree dos veces. Una vez que has ejecutado 10.000 pasos de entrenamiento, esencialmente has entrenado 10.000 redes neuronales diferentes, cada una con solo una instancia de entrenamiento. Obviamente, estas redes neuronales no son independientes porque comparten muchos de sus pesos, pero sin embargo todas son diferentes. La red neuronal resultante se puede ver como un conjunto promedio de todas estas redes neuronales más pequeñas.\n",
    "\n",
    "#### TIP\n",
    "\n",
    "En la práctica, generalmente se puede aplicar la caída solo a las neuronas de una a tres capas superiores (excluyendo la capa de salida).\n",
    "\n",
    "#### ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Hay un pequeño pero importante detalle técnico. Supongamos que p = 75 %: en promedio, solo el 25 % de todas las neuronas están activas en cada paso durante el entrenamiento. Esto significa que después del entrenamiento, una neurona estaría conectada a cuatro veces más neuronas de entrada de lo que estaría durante el entrenamiento. Para compensar este hecho, necesitamos multiplicar los pesos de conexión de entrada de cada neurona por cuatro durante el entrenamiento. Si no lo hacemos, la red neuronal no funcionará bien, ya que verá diferentes datos durante y después del entrenamiento. De manera más general, necesitamos dividir los pesos de conexión por la probabilidad de mantenimiento (1 - p) durante el entrenamiento.\n",
    "\n",
    "Para implementar el abandono usando Keras, puedes usar la capa `tf.keras.layers.Dropout` Durante el entrenamiento, deja caer al azar algunas entradas (establecéndolas en 0) y divide las entradas restantes por la probabilidad de mantenimiento. Después del entrenamiento, no hace nada en absoluto; solo pasa las entradas a la siguiente capa. El siguiente código aplica la regularización de abandono antes de cada capa densa, utilizando una tasa de abandono de 0,2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c4f5ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ellipsis]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "[...]  # compile and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28c6c0",
   "metadata": {},
   "source": [
    "#### ADVERTENCIA\n",
    "\n",
    "Dado que la interrupción solo está activa durante el entrenamiento, comparar la pérdida de entrenamiento y la pérdida de validación puede ser engañoso. En particular, un modelo puede estar sobreapesado en el conjunto de entrenamiento y, sin embargo, tener pérdidas similares de entrenamiento y validación. Por lo tanto, asegúrese de evaluar la pérdida de entrenamiento sin abandono (por ejemplo, después del entrenamiento).\n",
    "\n",
    "#### -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Si observa que el modelo está sobreajustado, puede aumentar la tasa de abandono. Por el contrario, deberías intentar reducir la tasa de abandono si el modelo no se ajusta al conjunto de entrenamiento. También puede ayudar a aumentar la tasa de abandono para las capas grandes y reducirla para las pequeñas. Además, muchas arquitecturas de última generación solo usan la caída después de la última capa oculta, por lo que es posible que desee probar esto si la caída completa es demasiado fuerte.\n",
    "\n",
    "La deserción tiende a ralentizar significativamente la convergencia, pero a menudo resulta en un mejor modelo cuando se sintoniza correctamente. Por lo tanto, generalmente vale la pena el tiempo y el esfuerzo extra, especialmente para los modelos grandes.\n",
    "\n",
    "#### TIP\n",
    "\n",
    "Si desea regularizar una red de autonormalización basada en la función de activación SELU (como se discutió anteriormente), debe usar la caída alfa: esta es una variante de la caída que preserva la media y la desviación estándar de sus entradas. Se introdujo en el mismo documento que SELU, ya que el abandono regular rompería la autonormalización.\n",
    "\n",
    "#### ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91707e02",
   "metadata": {},
   "source": [
    "## Abandono de Monte Carlo (MC)\n",
    "\n",
    "En 2016, un artículo⁠ de Yarin Gal y Zoubin Ghahramani agregó algunas buenas razones más para usar el abandono:\n",
    "\n",
    "* En primer lugar, el documento estableció una profunda conexión entre las redes de abandono (es decir, las redes neuronales que contienen capas `Dropout`) y la inferencia bayesiana aproximada, dando al abandono una sólida justificación matemática.\n",
    "\n",
    "\n",
    "- En segundo lugar, los autores introdujeron una poderosa técnica llamada abandono de MC, que puede aumentar el rendimiento de cualquier modelo de abandono entrenado sin tener que volver a entrenarlo o incluso modificarlo en absoluto. También proporciona una medida mucho mejor de la incertidumbre del modelo, y se puede implementar en solo unas pocas líneas de código.\n",
    "\n",
    "\n",
    "Si todo esto suena como un clic de \"un truco extraño\", entonces eche un vistazo al siguiente código. Es la implementación completa de la deserción de MC, impulsando el modelo de abandono que entrenamos anteriormente sin volver a entrenarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_probas = np.stack([model(X_test, training=True)\n",
    "                     for sample in range(100)])\n",
    "\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67178b6c",
   "metadata": {},
   "source": [
    "Tenga en cuenta que `model(X)` es similar a `model.predict(X)` excepto que devuelve un tensor en lugar de una matriz NumPy y admite el argumento de `training`. En este ejemplo de código, establecer `training=True` garantiza que la capa de abandono permanezca activa, por lo que todas las predicciones serán un poco diferentes. Simplemente hacemos 100 predicciones sobre el conjunto de prueba y calculamos su promedio. Más específicamente, cada llamada al modelo devuelve una matriz con una fila por instancia y una columna por clase. Debido a que hay 10.000 instancias en el conjunto de prueba y 10 clases, esta es una matriz de forma [10000, 10]. Apilamos 100 de estas matrices, por lo que `y_probas` es una matriz 3D de forma [100, 10000, 10]. Una vez que promediamos la primera dimensión (`axis = 0`), obtenemos `y_proba`, una matriz de forma [10000, 10], como la que obtendríamos con una sola predicción. ¡Eso es todo! Promediar múltiples predicciones con la deserción activada nos da una estimación de Monte Carlo que generalmente es más confiable que el resultado de una sola predicción con la deserción desactivada. Por ejemplo, veamos la predicción del modelo para la primera instancia en el conjunto de pruebas Fashion MNIST, con la deserción desactivada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a071e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test[:1]).round(3)\n",
    "\n",
    "#array([[0.   , 0.   , 0.   , 0.   , 0.   , 0.024, 0.   , 0.132, 0.   , 0.844]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a08cef",
   "metadata": {},
   "source": [
    "El modelo está bastante seguro (84,4%) de que esta imagen pertenece a la clase 9 (botines). Compare esto con la predicción de abandono de MC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba[0].round(3)\n",
    "\n",
    "#array([0.   , 0.   , 0.   , 0.   , 0.   , 0.067, 0.   , 0.209, 0.001, 0.723], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2a55f",
   "metadata": {},
   "source": [
    "El modelo todavía parece preferir la clase 9, pero su confianza se redujo al 72,3 %, y las probabilidades estimadas para las clases 5 (sandalia) y 7 (sneaker) han aumentado, lo que tiene sentido dado que también son calzado.\n",
    "\n",
    "La deserterante de MC tiende a mejorar la fiabilidad de las estimaciones de probabilidad del modelo. Esto significa que es menos probable que esté seguro de sí mismo, pero equivocado, lo que puede ser peligroso: imagínese un coche autónomo ignorando con confianza una señal de stop. También es útil saber exactamente qué otras clases son más probables. Además, puede echar un vistazo a la desviación estándar de las estimaciones de probabilidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "y_std[0].round(3)\n",
    "\n",
    "#array([0.   , 0.   , 0.   , 0.001, 0.   , 0.096, 0.   , 0.162, 0.001, 0.183], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08c37d",
   "metadata": {},
   "source": [
    "Aparentemente hay bastante variación en las estimaciones de probabilidad para la clase 9: la desviación estándar es de 0,183, que debe compararse con la probabilidad estimada de 0,723: si estuviera construyendo un sistema sensible al riesgo (por ejemplo, un sistema médico o financiero), probablemente trataría una predicción tan incierta con extrema precaución. Definitivamente no lo tratarías como una predicción segura del 84,4%. La precisión del modelo también recibió un (muy) pequeño aumento del 87,0 % al 87,2 %:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_proba.argmax(axis=1)\n",
    "accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "accuracy\n",
    "\n",
    "#0.8717"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df266fa",
   "metadata": {},
   "source": [
    "#### NOTA\n",
    "\n",
    "El número de muestras de Monte Carlo que usas (100 en este ejemplo) es un hiperparámetro que puedes ajustar. Cuanto más alto sea, más precisas serán las predicciones y sus estimaciones de incertidumbre. Sin embargo, si lo duplicas, el tiempo de inferencia también se duplicará. Además, por encima de un cierto número de muestras, notarás poca mejora. Su trabajo es encontrar el contrato correcto entre latencia y la precisión, dependiendo de su aplicación.\n",
    "\n",
    "#### -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd6c21",
   "metadata": {},
   "source": [
    "Si su modelo contiene otras capas que se comportan de una manera especial durante el entrenamiento (como las capas `BatchNormalization`), entonces no debe forzar el modo de entrenamiento como acabamos de hacer. En su lugar, debes reemplazar las capas de `Dropout` con la siguiente clase `MCDropout`:⁠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04ccd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs, training=False):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15780db",
   "metadata": {},
   "source": [
    "Aquí, simplemente subclasificamos la capa `Dropout` y anulamos el método `call()` para forzar su argumento de `trainint` a `True` (consulte el Capítulo 12). De manera similar, podría definir una clase `MCAlphaDropout` subclasificando `AlphaDropout`. Si está creando un modelo desde cero, sólo es cuestión de utilizar `MCDropout` en lugar de `Dropout`. Pero si tiene un modelo que ya fue entrenado usando `Dropout`, necesita crear un nuevo modelo que sea idéntico al modelo existente excepto con `Dropout` en lugar de `MCDropout`, luego copie los pesos del modelo existente a su nuevo modelo.\n",
    "\n",
    "En resumen, el abandono de MC es una gran técnica que impulsa los modelos de abandono y proporciona mejores estimaciones de incertidumbre. Y, por supuesto, dado que es solo un abandono regular durante el entrenamiento, también actúa como un regularizador.\n",
    "\n",
    "### Regularización de normas máximas\n",
    "\n",
    "Otra técnica de regularización popular para las redes neuronales se llama regularización de la norma máxima: para cada neurona, limita los pesos w de las conexiones entrantes de tal manera que ∥ w ∥2 ≤ r, donde r es el hiperparámetro de la norma máxima y ∥ · ∥2 es la norma ℓ2.\n",
    "\n",
    "La regularización de la norma máxima no añade un término de pérdida de regularización a la función de pérdida general. En su lugar, normalmente se implementa calculando ∥ w ∥2 después de cada paso de entrenamiento y reescalando w si es necesario (w ← w r / ∥ w ∥2).\n",
    "\n",
    "La reducción de r aumenta la cantidad de regularización y ayuda a reducir el sobreajuste. La regularización de la norma máxima también puede ayudar a aliviar los problemas de gradientes inestables (si no está utilizando la normalización por lotes).\n",
    "\n",
    "To implement max-norm regularization in Keras, set the `kernel_constraint` argument of each hidden layer to a `max_norm()` constraint with the appropriate max value, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88ebdefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(\n",
    "    100, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "    kernel_constraint=tf.keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7f875",
   "metadata": {},
   "source": [
    "Después de cada iteración de entrenamiento, el método `fit()` del modelo llamará al objeto devuelto por `max_norm()`, le pasará los pesos de la capa y obtendrá a cambio pesos reescalados, que luego reemplazan los pesos de la capa. Como verá en el Capítulo 12, puede definir su propia función de restricción personalizada si es necesario y usarla como restricción `kernel_constraint`. También puede restringir los términos de sesgo configurando el argumento `bias_constraint`.\n",
    "\n",
    "La función `max_norm()` tiene un argumento de `axis` cuyo valor predeterminado es `0`. Una capa `Dense` generalmente tiene pesos de forma [número de entradas, número de neuronas], por lo que usar `axis=0` significa que la restricción de norma máxima se aplicará de forma independiente a cada neurona. vector de peso. Si desea utilizar max-norm con capas convolucionales (consulte el Capítulo 14), asegúrese de establecer apropiadamente el argumento del eje de la restricción `max_norm()` (generalmente `axis=[0, 1, 2]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695d521",
   "metadata": {},
   "source": [
    "## Resumen y directrices prácticas\n",
    "\n",
    "En este capítulo hemos cubierto una amplia gama de técnicas, y es posible que te preguntes cuáles deberías usar. Esto depende de la tarea, y todavía no hay un consenso claro, pero he encontrado que la configuración de la Tabla 11-3 funciona bien en la mayoría de los casos, sin requerir mucho ajuste de hiperparámetros. Dicho esto, ¡por favor, no consideres estos valores predeterminados como reglas estrictas!\n",
    "\n",
    "Tabla 11-3: Configuración predeterminada de DNNs:\n",
    "\n",
    "| **Hiperparametro**            | **Valor predeterminado**                                                 |\n",
    "|-------------------------------|--------------------------------------------------------------------------|\n",
    "| Inicializador Kernel (nucleo) | Inicialización                                                           |\n",
    "| Función de activación         | ReLU si es poco profunda, SWISH si es profunda                           |\n",
    "| Normalización                 | Ninguna si es poco profunda, (BN) Normalización del Batch si es profunda |\n",
    "| Regularización                | Parada temprana; Decay de pesos si es necesario                          |\n",
    "| Optimizador                   | Aceleracion del gradiente de Nesterov o AdamW                            |\n",
    "| Programación tasa aprendizaje | Programación de Rendimiento o 1 Ciclo                                    |\n",
    "\n",
    "Si la red es una simple pila de capas densas, entonces puede autonormalizarse, y debe usar la configuración de la Tabla 11-4 en su lugar.\n",
    "\n",
    "Tabla 11-4. Configuración de DNN para una red de autonormalización:\n",
    "\n",
    "| **Hiperparametro**            | **Valor predeterminado**              |\n",
    "|-------------------------------|---------------------------------------|\n",
    "| Inicializador Kernel (nucleo) | Inicialización de LeCun               |\n",
    "| Función de activación         | SELU                                  |\n",
    "| Normalización                 | Ninguno (autonormalización)           |\n",
    "| Regularización                | Abandono Alfa si es necesario         |\n",
    "| Optimizador                   | Gradiente acelerado de Nesterov       |\n",
    "| Programación tasa aprendizaje | Programación de Rendimiento o 1 Ciclo |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484902a0",
   "metadata": {},
   "source": [
    "¡No olvides normalizar las funciones de entrada! También debe tratar de reutilizar partes de una red neuronal preentrenada si puede encontrar una que resuelva un problema similar, o usar el preentrenamiento no supervisado si tiene muchos datos sin etiquetar, o usar el preentrenamiento en una tarea auxiliar si tiene muchos datos etiquetados para una tarea similar.\n",
    "\n",
    "Si bien las directrices anteriores deberían cubrir la mayoría de los casos, aquí hay algunas excepciones:\n",
    "\n",
    "Si necesitas un modelo disperso, puedes usar la regularización ℓ1 (y, opcionalmente, poner a cero los pequeños pesos después del entrenamiento). Si necesita un modelo aún más escaso, puede utilizar el kit de herramientas de optimización de modelos de TensorFlow. Esto romperá la autonormalización, por lo que debe usar la configuración predeterminada en este caso.\n",
    "Si necesita un modelo de baja latencia (uno que realice predicciones ultrarrápidas), es posible que necesite usar menos capas, usar una función de activación rápida como ReLU o ReLU con fugas, y doblar las capas de normalización por lotes en las capas anteriores después del entrenamiento. Tener un modelo disperso también ayudará. Por último, es posible que desee reducir la precisión de flotación de 32 bits a 16 o incluso 8 bits (consulte \"Despliegue de un modelo en un dispositivo móvil o integrado\"). Una vez más, echa un vistazo a TF-MOT.\n",
    "Si está construyendo una aplicación sensible al riesgo, o la latencia de inferencia no es muy importante en su aplicación, puede usar la deserción de MC para aumentar el rendimiento y obtener estimaciones de probabilidad más confiables, junto con estimaciones de incertidumbre.\n",
    "¡Con estas pautas, ahora estás listo para entrenar redes muy profundas! Espero que ahora estés convencido de que puedes recer un largo camino usando solo la práctica API de Keras. Sin embargo, puede llegar un momento en el que necesites tener aún más control; por ejemplo, para escribir una función de pérdida personalizada o para ajustar el algoritmo de entrenamiento. Para tales casos, tendrá que utilizar la API de nivel inferior de TensorFlow, como verá en el siguiente capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b42bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
