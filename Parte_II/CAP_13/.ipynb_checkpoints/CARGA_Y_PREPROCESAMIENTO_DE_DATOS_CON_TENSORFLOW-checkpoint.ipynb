{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20194ab6",
   "metadata": {},
   "source": [
    "En el capítulo 2, viste que la carga y el preprocesamiento de datos es una parte importante de cualquier proyecto de aprendizaje automático. Usó Pandas para cargar y explorar el conjunto de datos de viviendas de California (modificado), que se almacenó en un archivo CSV, y aplicó los transformadores de Scikit-Learn para el preprocesamiento. Estas herramientas son bastante convenientes, y probablemente las usarás a menudo, especialmente cuando explores y experimentes con datos.\n",
    "\n",
    "Sin embargo, al entrenar modelos de TensorFlow en grandes conjuntos de datos, es posible que prefiera utilizar la propia API de carga y preprocesamiento de datos de TensorFlow, llamada tf.data. Es capaz de cargar y procesar previamente datos de manera extremadamente eficiente, leyendo desde múltiples archivos en paralelo utilizando multihilo y colas, barajando y mezclando muestras por lotes, y más. Además, puede hacer todo esto sobre la marcha: carga y preprocesa el siguiente lote de datos a través de múltiples núcleos de CPU, mientras que sus GPU o TPU están ocupados entrenando el lote actual de datos.\n",
    "\n",
    "La API de tf.data le permite manejar conjuntos de datos que no caben en la memoria, y le permite hacer un uso completo de sus recursos de hardware, acelerando así la formación. Fuera del estante, la API tf.data puede leer desde archivos de texto (como archivos CSV), archivos binarios con registros de tamaño fijo y archivos binarios que utilizan el formato TFRecord de TensorFlow, que admite registros de diferentes tamaños.\n",
    "\n",
    "TFRecord es un formato binario flexible y eficiente que generalmente contiene búferes de protocolo (un formato binario de código abierto). La API tf.data también es compatible con la lectura de bases de datos SQL. Además, muchas extensiones de código abierto están disponibles para leer desde todo tipo de fuentes de datos, como el servicio BigQuery de Google (consulte https://tensorflow.org/io).\n",
    "\n",
    "Keras también viene con capas de preprocesamiento potentes pero fáciles de usar que se pueden incrustar en sus modelos: de esta manera, cuando implementa un modelo en producción, podrá ingerir datos sin procesar directamente, sin tener que agregar ningún código de preprocesamiento adicional. Esto elimina el riesgo de desajuste entre el código de preprocesamiento utilizado durante el entrenamiento y el código de preprocesamiento utilizado en la producción, lo que probablemente causaría un sesgo de entrenamiento/servicio. Y si implementa su modelo en múltiples aplicaciones codificadas en diferentes lenguajes de programación, no tendrá que volver a implementar el mismo código de preprocesamiento varias veces, lo que también reduce el riesgo de desajuste.\n",
    "\n",
    "Como verá, ambas API se pueden utilizar conjuntamente, por ejemplo, para beneficiarse de la carga eficiente de datos que ofrece tf.data y la comodidad de las capas de preprocesamiento de Keras.\n",
    "\n",
    "En este capítulo, primero cubriremos la API de tf.data y el formato TFRecord. Luego exploraremos las capas de preprocesamiento de Keras y cómo usarlas con la API tf.data. Por último, echaremos un vistazo rápido a algunas bibliotecas relacionadas que pueden resultarle útiles para cargar y procesar datos, como los conjuntos de datos de TensorFlow y TensorFlow Hub. Así que, ¡comencemos!\n",
    "\n",
    "\n",
    "# La API de tf.data\n",
    "\n",
    "\n",
    "Toda la API de tf.data gira en torno al concepto de un `tf.data.Dataset`: esto representa una secuencia de elementos de datos. Por lo general, utilizará conjuntos de datos que leen gradualmente los datos del disco, pero por simplicidad, creemos un conjunto de datos a partir de un tensor de datos simple usando `tf.data.Dataset.from_tensor_slices()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06e46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 01:30:32.652103: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 01:30:38.766635: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.range(10)  # cualquier tensor de datos\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb743de5",
   "metadata": {},
   "source": [
    "La función `from_tensor_slices()` toma un tensor y crea un `tf.data.Dataset` cuyos elementos son todos los sectores de `X` a lo largo de la primera dimensión, por lo que este conjunto de datos contiene 10 elementos: tensores 0, 1, 2,…, 9. En este En este caso, habríamos obtenido el mismo conjunto de datos si hubiéramos usado `tf.data.Dataset.range(10)` (excepto que los elementos serían enteros de 64 bits en lugar de enteros de 32 bits).\n",
    "\n",
    "Simplemente puede iterar sobre los elementos de un conjunto de datos de esta manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ffecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568fe15",
   "metadata": {},
   "source": [
    "#### NOTA\n",
    "\n",
    "La API de tf.data es una API de streaming: puedes iterar de manera muy eficiente a través de los elementos de un conjunto de datos, pero la API no está diseñada para indexar o cortar.\n",
    "\n",
    "#### ----------------------------------------------------------------------------\n",
    "\n",
    "Un conjunto de datos también puede contener tuplas de tensores, o diccionarios de pares de nombre/tensor, o incluso tuplas anidadas y diccionarios de tensores. Al cortar una tupla, un diccionario o una estructura anidada, el conjunto de datos solo cortará los tensores que contiene, al tiempo que preservará la estructura de tupla/diccionario. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af6dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    }
   ],
   "source": [
    "X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b398b",
   "metadata": {},
   "source": [
    "## Transformaciones de encadenamiento\n",
    "\n",
    "Una vez que tenga un conjunto de datos, puede aplicarle todo tipo de transformaciones llamando a sus métodos de transformación. Cada método devuelve un nuevo conjunto de datos, por lo que puede encadenar transformaciones como esta (esta cadena se ilustra en la Figura 13-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a7f1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaeed0",
   "metadata": {},
   "source": [
    "En este ejemplo, primero llamamos al método `repeat()` en el conjunto de datos original y devuelve un nuevo conjunto de datos que repite los elementos del conjunto de datos original tres veces. ¡Por supuesto, esto no copiará todos los datos en la memoria tres veces! Si llama a este método sin argumentos, el nuevo conjunto de datos repetirá el conjunto de datos de origen para siempre, por lo que el código que itera sobre el conjunto de datos tendrá que decidir cuándo detenerse.\n",
    "\n",
    "Luego llamamos al método `batch()` en este nuevo conjunto de datos y nuevamente esto crea un nuevo conjunto de datos. Éste agrupará los elementos del conjunto de datos anterior en lotes de siete elementos.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1301.png)\n",
    "\n",
    "(_Figura 13-1. Encadenar las transformaciones del conjunto de datos_)\n",
    "\n",
    "Finalmente, iteramos sobre los elementos de este conjunto de datos final. El método `batch()` tenía que generar un lote final de tamaño dos en lugar de siete, pero puede llamar a `batch()` con `drop_remainder=True` si desea que elimine este lote final, de modo que todos los lotes tengan exactamente el mismo tamaño.\n",
    "\n",
    "\n",
    "#### ADVERTENCIA\n",
    "\n",
    "Los métodos de los conjuntos de datos no modifican los conjuntos de datos, sino que crean otros nuevos. Así que asegúrese de mantener una referencia a estos nuevos conjuntos de datos (por ejemplo, con `dataset = ...`), o de lo contrario no pasará nada.\n",
    "\n",
    "#### ------------------------------------------------------------------------\n",
    "\n",
    "También puedes transformar los elementos llamando al método `map()`. Por ejemplo, esto crea un nuevo conjunto de datos con todos los lotes multiplicados por dos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b04950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)  # x is a batch\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba9576",
   "metadata": {},
   "source": [
    "Este método `map()` es al que llamará para aplicar cualquier preprocesamiento a sus datos. A veces, esto incluirá cálculos que pueden ser bastante intensivos, como remodelar o rotar una imagen, por lo que normalmente querrás generar varios subprocesos para acelerar las cosas. Esto se puede hacer estableciendo el argumento `num_parallel_calls` en la cantidad de subprocesos a ejecutar o en `tf.data.AUTOTUNE`. Tenga en cuenta que la función que pase al método `map()` debe poder convertirse en una función TF (consulte el Capítulo 12).\n",
    "\n",
    "También es posible simplemente filtrar el conjunto de datos utilizando el método `filter()`. Por ejemplo, este código crea un conjunto de datos que solo contiene los lotes cuya suma es superior a 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbccbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa03a6",
   "metadata": {},
   "source": [
    "A menudo querrás ver sólo unos pocos elementos de un conjunto de datos. Puedes usar el método `take()` para eso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b5ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311bc356",
   "metadata": {},
   "source": [
    "## Alear los datos\n",
    "\n",
    "Como discutimos en el capítulo 4, el descenso de gradiente funciona mejor cuando las instancias del conjunto de entrenamiento son independientes y están distribuidas de manera idéntica (IID). Una forma sencilla de asegurar esto es mezclar las instancias, utilizando el método shuffle(). Creará un nuevo conjunto de datos que comenzará llenando un búfer con los primeros elementos del conjunto de datos de origen. Luego, cada vez que se le pida un elemento, sacará uno al azar del búfer y lo reemplazará por uno nuevo del conjunto de datos de origen, hasta que haya iterado por completo a través del conjunto de datos de origen. En este punto, continuará sacando artículos al azar del búfer hasta que esté vacío. Debe especificar el tamaño del búfer, y es importante que sea lo suficientemente grande, o de lo contrario, el barajamiento no será muy efectivo.⁠ Simplemente no exceda la cantidad de RAM que tiene, aunque incluso si tiene suficiente, no hay necesidad de ir más allá del tamaño del conjunto de datos. Puedes proporcionar una semilla aleatoria si quieres el mismo orden aleatorio cada vez que ejecutes tu programa. Por ejemplo, el siguiente código crea y muestra un conjunto de datos que contiene los números enteros 0 a 9, repetidos dos veces, mezclado con un búfer de tamaño 4 y una semilla aleatoria de 42, y por lotes con un tamaño de lote de 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93296038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216aff5",
   "metadata": {},
   "source": [
    "#### PROPINA\n",
    "\n",
    "Si llama a `repeat()` en un conjunto de datos mezclado, de forma predeterminada generará un nuevo orden en cada iteración. Generalmente, esto es una buena idea, pero si prefiere reutilizar el mismo orden en cada iteración (por ejemplo, para pruebas o depuración), puede configurar `reshuffle_each_itera⁠tion=False` al llamar a `shuffle()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592076a1",
   "metadata": {},
   "source": [
    "Para un conjunto de datos grande que no cabe en la memoria, este simple enfoque de búfer aleatorio puede no ser suficiente, ya que el búfer será pequeño en comparación con el conjunto de datos. Una solución es mezclar los datos de origen en sí (por ejemplo, en Linux puedes mezclar archivos de texto usando el comando `shuf`). ¡Esto definitivamente mejorará mucho el barajo! Incluso si los datos de origen se barajan, por lo general querrá barajarlos un poco más, o de lo contrario se repetirá el mismo orden en cada época, y el modelo puede terminar siendo sesgado (por ejemplo, debido a algunos patrones espurios presentes por casualidad en el orden de los datos de origen). Para mezclar un poco más las instancias, un enfoque común es dividir los datos de origen en varios archivos y luego leerlos en un orden aleatorio durante el entrenamiento. Sin embargo, las instancias ubicadas en el mismo archivo aún terminarán cerca unas de otras. Para evitar esto, puede elegir varios archivos al azar y leerlos simultáneamente, intercalando sus registros. Además de eso, puedes añadir un búfer de barajar usando el método `shuffle()`. Si esto suena a mucho trabajo, no te preocupes: la API tf.data hace que todo esto sea posible en solo unas pocas líneas de código. Vamos a repasar cómo puedes hacer esto.\n",
    "\n",
    "\n",
    "## Líneas entrelazar desde varios archivos\n",
    "\n",
    "En primer lugar, supongamos que ha cargado el conjunto de datos de viviendas de California, lo ha mezclado (a menos que ya se haya barajado) y lo ha dividido en un conjunto de entrenamiento, un conjunto de validación y un conjunto de pruebas. Luego divide cada conjunto en muchos archivos CSV que se ven así (cada fila contiene ocho características de entrada más el valor medio de la casa objetivo):\n",
    "\n",
    "`MedInc,HouseAge,AveRooms,AveBedrms,Popul...,AveOccup,Lat...,Long...,MedianHouseValue 3.5214,15,0,3.050,1.107,1447.0,1.606,37.63,-122.43,1.442 5.3275,5.0,6.490,0.991,3464.0,3.443,33.69,-117.39,1.687 3.1,29.0,7.542,1.592,1328.0,2.251,38.44,-122.98,1.621 [...]\n",
    "`\n",
    "\n",
    "Supongamos también que `train_filepaths` contiene la lista de rutas de archivos de entrenamiento (y también tiene `valid_filepaths` y `test_filepaths`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecfeea26",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1436994433.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    -> ['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', ...]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "train_filepaths\n",
    "\n",
    "-> ['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59910a8",
   "metadata": {},
   "source": [
    "Alternativamente, podría usar patrones de archivo; por ejemplo, `train_filepaths =\"datasets/housing/my_train_*.csv\"` Ahora vamos a crear un conjunto de datos que contenga solo estas rutas de archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66edc33a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filepath_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mlist_files(\u001b[43mtrain_filepaths\u001b[49m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_filepaths' is not defined"
     ]
    }
   ],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87debbf0",
   "metadata": {},
   "source": [
    "De forma predeterminada, la función `list_files()` devuelve un conjunto de datos que mezcla las rutas de los archivos. En general, esto es algo bueno, pero puedes configurar `shuffle=False` si no lo deseas por algún motivo.\n",
    "\n",
    "A continuación, puede llamar al método `interleave()` para leer cinco archivos a la vez e intercalar sus líneas. También puedes omitir la primera línea de cada archivo (que es la fila del encabezado) usando el método `skip()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396914a",
   "metadata": {},
   "source": [
    "El método `interleave()` creará un conjunto de datos que extraerá cinco rutas de archivo del `filepath_dataset`, y para cada una llamará a la función que le asignó (una lambda en este ejemplo) para crear un nuevo conjunto de datos (en este caso, un `TextLineDataset`). Para ser claros, en esta etapa habrá siete conjuntos de datos en total: el conjunto de datos de ruta de archivo, el conjunto de datos entrelazados y los cinco conjuntos de datos TextLine creados internamente por el conjunto de datos entrelazados. Cuando itera sobre el conjunto de datos entrelazados, recorrerá estos cinco `TextLineDatasets`, leyendo una línea a la vez de cada uno hasta que todos los conjuntos de datos se queden sin elementos. Luego, buscará las siguientes cinco rutas de archivo del `filepath_dataset` y las intercalará de la misma manera, y así sucesivamente hasta que se quede sin rutas de archivo. Para que el intercalado funcione mejor, es preferible tener archivos de idéntica longitud; de lo contrario, el final del archivo más largo no se intercalará.\n",
    "\n",
    "De forma predeterminada, `interleave()` no utiliza paralelismo; simplemente lee una línea a la vez de cada archivo, de forma secuencial. Si desea que realmente lea archivos en paralelo, puede configurar el argumento `num_parallel_calls` del método `interleave()` en la cantidad de subprocesos que desee (recuerde que el método map() también tiene este argumento). Incluso puede configurarlo en `tf.data.AUTOTUNE` para que TensorFlow elija dinámicamente la cantidad correcta de subprocesos según la CPU disponible. Veamos qué contiene el conjunto de datos ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee5deb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 4 2 0 1 8 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 9 0 3 4 1 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 5 8 2 7 9], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340cf3b",
   "metadata": {},
   "source": [
    "Estas son las primeras filas (ignorando la fila de encabezado) de cinco archivos CSV, elegidos al azar. ¡Se ve bien!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5e863",
   "metadata": {},
   "source": [
    "#### NOTA\n",
    "\n",
    "Es posible pasar una lista de rutas de archivos al constructor `TextLineDataset`: revisará cada archivo en orden, línea por línea. Si también establece el argumento `num_parallel_reads` en un número mayor que uno, entonces el conjunto de datos leerá esa cantidad de archivos en paralelo e intercalará sus líneas (sin tener que llamar al método `interleave()`). Sin embargo, no mezclará los archivos ni se saltará las líneas del encabezado.\n",
    "\n",
    "#### -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934be9c",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos\n",
    "\n",
    "Ahora que tenemos un conjunto de datos de vivienda que devuelve cada instancia como un tensor que contiene una cadena de bytes, necesitamos hacer un poco de preprocesamiento, incluido el análisis de las cadenas y el escalado de los datos. Implementemos un par de funciones personalizadas que realizarán este preprocesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec602a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean, X_std = [...]  # mean and scale of each feature in the training set\n",
    "n_inputs = 8\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "def preprocess(line):\n",
    "    x, y = parse_csv_line(line)\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafd5a5",
   "metadata": {},
   "source": [
    "Vamos a revisar este código:\n",
    "\n",
    "+ Primero, el código supone que hemos calculado previamente la media y la desviación estándar de cada característica en el conjunto de entrenamiento. `X_mean` y `X_std` son solo tensores 1D (o matrices NumPy) que contienen ocho flotantes, uno por característica de entrada. Esto se puede hacer utilizando Scikit-Learn `StandardScaler` en una muestra aleatoria suficientemente grande del conjunto de datos. Más adelante en este capítulo, usaremos una capa de preprocesamiento de Keras.\n",
    "\n",
    "- La función `parse_csv_line()` toma una línea CSV y la analiza. Para ayudar con eso, utiliza la función `tf.io.decode_csv()`, que toma dos argumentos: el primero es la línea a analizar y el segundo es una matriz que contiene el valor predeterminado para cada columna en el archivo CSV. Esta matriz (`defs`) le dice a TensorFlow no solo el valor predeterminado para cada columna, sino también el número de columnas y sus tipos. En este ejemplo, le decimos que todas las columnas de características son flotantes y que los valores faltantes deben ser cero por defecto, pero proporcionamos una matriz vacía de tipo `tf.float32` como valor predeterminado para la última columna (el destino): la matriz le dice a TensorFlow que esta columna contiene flotantes, pero que no hay un valor predeterminado, por lo que generará una excepción si encuentra un valor faltante.\n",
    "\n",
    "+ La función `tf.io.decode_csv()` devuelve una lista de tensores escalares (uno por columna), pero necesitamos devolver una matriz de tensores 1D. Entonces llamamos a `tf.stack()` en todos los tensores excepto en el último (el objetivo): esto apilará estos tensores en una matriz 1D. Luego hacemos lo mismo con el valor objetivo: esto lo convierte en una matriz tensorial 1D con un valor único, en lugar de un tensor escalar. La función `tf.io.decode_csv()` está completa, por lo que devuelve las características de entrada y el objetivo.\n",
    "\n",
    "- Finalmente, la función `preprocess()` personalizada simplemente llama a la función `parse_csv_line()`, escala las características de entrada restando las medias de las características y luego dividiéndolas por las desviaciones estándar de las características, y devuelve una tupla que contiene las características escaladas y el objetivo.\n",
    "\n",
    "Vamos a probar esta función de preprocesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35d7e05c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpreprocess\u001b[49m(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m(<tf.Tensor: shape=(8,), dtype=float32, numpy=\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n",
    "\n",
    "'''\n",
    "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
    " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
    "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
    " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a010be",
   "metadata": {},
   "source": [
    "¡Se ve bien! La función `preprocess()` puede convertir una instancia de una cadena de bytes a un tensor escalado agradable, con su etiqueta correspondiente. Ahora podemos usar el método `map()` del conjunto de datos para aplicar la función `preprocess()` a cada muestra del conjunto de datos.\n",
    "\n",
    "\n",
    "## Juntandolo todo\n",
    "\n",
    "Para hacer que el código sea más reutilizable, juntemos todo lo que hemos discutido hasta ahora en otra función de ayuda; creará y devolverá un conjunto de datos que cargará de manera eficiente los datos de vivienda de California de múltiples archivos CSV, los procesará previamente, los barajará y los agrupará por lotes (ver Figura 13-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fac7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n",
    "                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n",
    "                       batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f782e",
   "metadata": {},
   "source": [
    "Tenga en cuenta que utilizamos el método `prefetch()` en la última línea. Esto es importante para el rendimiento, como verá ahora.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1302.png)\n",
    "\n",
    "(_Figura 13-2. Carga y preprocesamiento de datos de varios archivos CSV_)\n",
    "\n",
    "\n",
    "## Pre-recarga\n",
    "\n",
    "Al llamar a `prefetch(1)` al final de la función personalizada `csv_reader_dataset()`, estamos creando un conjunto de datos que hará todo lo posible para estar siempre un lote por delante.⁠2 En otras palabras, mientras nuestro algoritmo de entrenamiento está trabajando en un lote, el conjunto de datos ya estará trabajando en paralelo para preparar el siguiente lote (por ejemplo, leyendo los datos del disco y preprocesándolos). Esto puede mejorar drásticamente el rendimiento, como se ilustra en la Figura 13-3.\n",
    "\n",
    "Si también nos aseguramos de que la carga y el preprocesamiento sean multiproceso (al configurar `num_parallel_calls` al llamar a `interleave()` y `map()`), podemos explotar múltiples núcleos de CPU y, con suerte, hacer que la preparación de un lote de datos sea más corta que ejecutar un paso de entrenamiento en la GPU: esto De esta manera, la GPU se utilizará casi al 100% (excepto el tiempo de transferencia de datos de la CPU a la GPU⁠3) y el entrenamiento se ejecutará mucho más rápido.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1303.png)\n",
    "\n",
    "(_Figura 13-3. Con la precarga, la CPU y la GPU funcionan en paralelo: como la GPU funciona en un lote, la CPU funciona en el siguiente_)\n",
    "\n",
    "#### TIP\n",
    "\n",
    "Si planea comprar una tarjeta GPU, su potencia de procesamiento y su tamaño de memoria son, por supuesto, muy importantes (en particular, una gran cantidad de RAM es crucial para grandes modelos de visión por ordenador o de procesamiento de lenguaje natural). Igualmente importante para un buen rendimiento es el ancho de banda de memoria de la GPU; este es el número de gigabytes de datos que puede entrar o salir de su RAM por segundo.\n",
    "\n",
    "#### --------------------------------------------------------\n",
    "\n",
    "Si el conjunto de datos es lo suficientemente pequeño como para caber en la memoria, puede acelerar significativamente el entrenamiento utilizando el método `cache()` del conjunto de datos para almacenar en caché su contenido en la RAM. Por lo general, debe hacer esto después de cargar y preprocesar los datos, pero antes de mezclarlos, repetirlos, agruparlos y buscarlos previamente. De esta manera, cada instancia solo se leerá y preprocesará una vez (en lugar de una vez por época), pero los datos se seguirán mezclando de forma diferente en cada época y el siguiente lote se seguirá preparando con antelación.\n",
    "\n",
    "Ahora ha aprendido cómo crear canales de entrada eficientes para cargar y preprocesar datos de múltiples archivos de texto. Hemos analizado los métodos de conjuntos de datos más comunes, pero hay algunos más que quizás quieras ver, como `concatenate()`, `zip()`, `window()`, `reduce()`, `shard()`, `flat_map()`, `apply( )`, `desbatch()` y `padded_batch()`. También hay algunos métodos de clase más, como `from_generator()` y `from_ten⁠sors()`, que crean un nuevo conjunto de datos a partir de un generador de Python o una lista de tensores, respectivamente. Consulte la documentación de la API para obtener más detalles. También tenga en cuenta que hay funciones experimentales disponibles en `tf.data.experimental`, muchas de las cuales probablemente llegarán a la API principal en futuras versiones (por ejemplo, consulte la clase `CsvDataset`, así como el método `make_csv_dataset()`, que se encarga de de inferir el tipo de cada columna).\n",
    "\n",
    "\n",
    "## Uso del conjunto de datos con Keras\n",
    "\n",
    "Ahora podemos usar la función personalizada `csv_reader_dataset()` que escribimos anteriormente para crear un conjunto de datos para el conjunto de entrenamiento, y para el conjunto de validación y el conjunto de prueba. El conjunto de entrenamiento se barajará en cada época (tenga en cuenta que el conjunto de validación y el conjunto de prueba también se barajarán, aunque realmente no los necesitamos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a76576",
   "metadata": {},
   "source": [
    "Ahora puedes simplemente construir y entrenar un modelo de Keras usando estos conjuntos de datos. Cuando llamas al método `fit()` del modelo, pasas train_set en lugar de `X_train`, `y_train`, y pasas `validation_data=valid_set` en lugar de `validation_data=(X_valid, y_valid)`. El método `fit()` se encargará de repetir el conjunto de datos de entrenamiento una vez por época, utilizando un orden aleatorio diferente en cada época:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([...])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fab4b",
   "metadata": {},
   "source": [
    "Simiarmente, puedes pasar el dataset a los métodos `evaluate` y `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3097b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = model.evaluate(test_set)\n",
    "new_set = test_set.take(3)  # pretend we have 3 new samples\n",
    "y_pred = model.predict(new_set)  # or you could just pass a NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4999e92",
   "metadata": {},
   "source": [
    "A diferencia de los otros conjuntos, el `new_set` normalmente no contendrá etiquetas. Si es así, como es el caso aquí, Keras los ignorará. Tenga en cuenta que en todos estos casos, aún puede usar matrices NumPy en lugar de conjuntos de datos si lo prefiere (pero, por supuesto, primero deben haberse cargado y preprocesado).\n",
    "\n",
    "Si desea construir su propio bucle de entrenamiento personalizado (como se discute en el capítulo 12), puede iterar sobre el conjunto de entrenamiento, de forma muy natural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        [...]  # perform one gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f6e9e",
   "metadata": {},
   "source": [
    "De hecho, incluso es posible crear una función TF (ver Capítulo 12) que entrene el modelo para toda una época. Esto realmente puede acelerar el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1496547",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
    "    train_one_epoch(model, optimizer, loss_fn, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61515136",
   "metadata": {},
   "source": [
    "En Keras, el argumento `steps_per_execution` del método `compile()` le permite definir el número de lotes que procesará el método `fit()` durante cada llamada a la función `tf.function` utiliza para el entrenamiento. El valor predeterminado es sólo 1, por lo que si lo configura en 50, a menudo verá una mejora significativa en el rendimiento. Sin embargo, los métodos `on_batch_*()` de las devoluciones de llamada de Keras solo se llamarán cada 50 lotes.\n",
    "\n",
    "¡Enhorabuena, ahora sabes cómo construir potentes tuberías de entrada usando la API tf.data! Sin embargo, hasta ahora hemos estado utilizando archivos CSV, que son comunes, simples y convenientes, pero no realmente eficientes, y no admiten muy bien estructuras de datos grandes o complejas (como imágenes o audio). Así que, veamos cómo usar TFRecords en su lugar.\n",
    "\n",
    "#### TIP\n",
    "\n",
    "Si está satisfecho con los archivos CSV (o cualquier otro formato que esté utilizando), no tiene que usar TFRecords. Como dice el refrán, si no está roto, ¡no lo arregles! Los TFRecords son útiles cuando el cuello de botella durante el entrenamiento está cargando y anandando los datos.\n",
    "\n",
    "#### --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39fa4c5",
   "metadata": {},
   "source": [
    "# El formato TFRecord\n",
    "\n",
    "El formato TFRecord es el formato preferido de TensorFlow para almacenar grandes cantidades de datos y leerlos de manera eficiente. Es un formato binario muy simple que solo contiene una secuencia de registros binarios de diferentes tamaños (cada registro se compone de una longitud, una suma de comprobación de CRC para comprobar que la longitud no estaba dañada, luego los datos reales y, finalmente, una suma de comprobación de CRC para los datos). Puedes crear fácilmente un archivo TFRecord usando la clase `tf.io.TFRecordWriter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93fe42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681dcaa",
   "metadata": {},
   "source": [
    "Y luego puedes usar `tf.data.TFRecordDataset` para leer uno o más archivos TFRecord:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "987c96fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1151d",
   "metadata": {},
   "source": [
    "Esto mostrará:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37505c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Tensor(b'Este es el primer registro', shape=(), dtype=string) tf.Tensor(b'Y este es el segundo registro', shape=(), dtype=string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903629aa",
   "metadata": {},
   "source": [
    "#### TIP\n",
    "\n",
    "De forma predeterminada, un `TFRecordDataset` leerá los archivos uno por uno, pero puede hacer que lea varios archivos en paralelo e intercalar sus registros pasando al constructor una lista de rutas de archivos y configurando `num_parallel_reads` en un número mayor que uno. Alternativamente, puede obtener el mismo resultado usando `list_files()` e `interleave()` como hicimos antes para leer múltiples archivos CSV.\n",
    "\n",
    "#### --------------------------------------------------------\n",
    "\n",
    "\n",
    "## Archivos TFRecord comprimidos\n",
    "\n",
    "A veces puede ser útil comprimir sus archivos TFRecord, especialmente si necesitan ser cargados a través de una conexión de red. Puedes crear un archivo TFRecord comprimido estableciendo el argumento `options`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aea1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"Compress, compress, compress!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf338d0a",
   "metadata": {},
   "source": [
    "Al leer un archivo TFRecord comprimido, debe especificar el tipo de compresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "788d97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
    "                                  compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e6365",
   "metadata": {},
   "source": [
    "## Una breve introducción a los búferes de protocolo\n",
    "\n",
    "A pesar de que cada registro puede usar cualquier formato binario que desee, los archivos TFRecord suelen contener búferes de protocolo serializados (también llamados protobufs). Este es un formato binario portátil, extensible y eficiente desarrollado en Google en 2001 y hecho de código abierto en 2008; los protobufs ahora se utilizan ampliamente, en particular en gRPC, el sistema de llamadas de procedimientos remotos de Google. Se definen usando un lenguaje simple que se ve así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax = \"proto3\";\n",
    "message Person {\n",
    "    string name = 1;\n",
    "    int32 id = 2;\n",
    "    repeated string email = 3;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee89ba0",
   "metadata": {},
   "source": [
    "Esta definición de protobuf dice que estamos usando la versión 3 del formato protobuf y especifica que cada objeto `Person` puede (opcionalmente) tener un nombre `name` de tipo cadena, una identificación de tipo int32 y cero o más campos de correo electrónico `email`, cada uno de tipo cadena. Los números `1, 2 y 3` son los identificadores de campo: se utilizarán en la representación binaria de cada registro. Una vez que tenga una definición en un archivo .proto, puede compilarlo. Esto requiere `protoc`, el compilador de protobuf, para generar clases de acceso en Python (o algún otro lenguaje). Tenga en cuenta que las definiciones de protobuf que generalmente usará en TensorFlow ya han sido compiladas para usted y sus clases de Python son parte de la biblioteca de TensorFlow, por lo que no necesitará usar `protoc`. Todo lo que necesitas saber es cómo usar las clases de acceso de protobuf en Python. Para ilustrar los conceptos básicos, veamos un ejemplo simple que utiliza las clases de acceso generadas para el protobuf Person (el código se explica en los comentarios):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d71e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from person_pb2 import Person  # import the generated access class\n",
    "\n",
    "person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\n",
    "print(person)  # display the Person\n",
    "\n",
    "'''\n",
    "name: \"Al\"\n",
    "id: 123\n",
    "email: \"a@b.com\"\n",
    "'''\n",
    "\n",
    "person.name  # read a field\n",
    "\n",
    "'''\n",
    "'Al'\n",
    "'''\n",
    "\n",
    "person.name = \"Alice\"  # modify a field\n",
    "person.email[0]  # repeated fields can be accessed like arrays\n",
    "\n",
    "'''\n",
    "'a@b.com'\n",
    "'''\n",
    "\n",
    "person.email.append(\"c@d.com\")  # add an email address\n",
    "serialized = person.SerializeToString()  # serialize person to a byte string\n",
    "serialized\n",
    "\n",
    "'''\n",
    "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n",
    "'''\n",
    "\n",
    "person2 = Person()  # create a new Person\n",
    "person2.ParseFromString(serialized)  # parse the byte string (27 bytes long)\n",
    "\n",
    "'''\n",
    "27\n",
    "'''\n",
    "\n",
    "person == person2  # now they are equal\n",
    "\n",
    "'''\n",
    "True\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dc095",
   "metadata": {},
   "source": [
    "En resumen, importamos la clase `Person` generada por `protoc`, creamos una instancia y jugamos con ella, visualizándola y leyendo y escribiendo algunos campos, luego la serializamos usando el método `SerializeToString()`. Estos son los datos binarios que están listos para ser guardados o transmitidos a través de la red. Al leer o recibir estos datos binarios, podemos analizarlos usando el método ParseFromString() y obtenemos una copia del objeto que fue serializado.⁠\n",
    "\n",
    "Puede guardar el objeto `Person` serializado en un archivo TFRecord, luego cargarlo y analizarlo: todo funcionaría bien. Sin embargo, `ParseFromString()` no es una operación de TensorFlow, por lo que no podría usarlo en una función de preprocesamiento en una canalización tf.data (excepto envolviéndolo en una operación `tf.py_function()`, lo que haría que el código fuera más lento y menos portátil, como vio en el Capítulo 12). Sin embargo, puede usar la función `tf.io.decode_proto()`, que puede analizar cualquier protobuf que desee, siempre que le dé la definición de protobuf (consulte el cuaderno para ver un ejemplo). Dicho esto, en la práctica generalmente querrás utilizar los protobufs predefinidos para los cuales TensorFlow proporciona operaciones de análisis dedicadas. Veamos ahora estos protobufs predefinidos.\n",
    "\n",
    "\n",
    "## TensorFlow Protobufs\n",
    "\n",
    "El protobuf principal que se utiliza normalmente en un archivo TFRecord es el protobuf `Example`, que representa una instancia en un conjunto de datos. Contiene una lista de características con nombre, donde cada característica puede ser una lista de cadenas de bytes, una lista de flotantes o una lista de números enteros. Aquí está la definición de protobuf (del código fuente de TensorFlow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax = \"proto3\";\n",
    "message BytesList { repeated bytes value = 1; }\n",
    "message FloatList { repeated float value = 1 [packed = true]; }\n",
    "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
    "message Feature {\n",
    "    oneof kind {\n",
    "        BytesList bytes_list = 1;\n",
    "        FloatList float_list = 2;\n",
    "        Int64List int64_list = 3;\n",
    "    }\n",
    "};\n",
    "message Features { map<string, Feature> feature = 1; };\n",
    "message Example { Features features = 1; };"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712aa380",
   "metadata": {},
   "source": [
    "Las definiciones de `BytesList`, `FloatList` e `Int64List` son bastante sencillas. Tenga en cuenta que `[packed = true]` se utiliza para campos numéricos repetidos, para una codificación más eficiente. Una `Feature` contiene una `BytesList`, una `FloatList` o una `Int64List`. Una característica (con una `s`) contiene un diccionario que asigna un nombre de característica al valor de característica correspondiente. Y finalmente, un `Example` contiene solo un objeto `Features`.\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "¿Por qué se definió Ejemplo, ya que no contiene más que un objeto `Features`? Bueno, es posible que algún día los desarrolladores de TensorFlow decidan agregarle más campos. Siempre que la nueva definición de ejemplo todavía contenga el campo de `features`, con el mismo ID, será compatible con versiones anteriores. Esta extensibilidad es una de las grandes características de los protobufs.\n",
    "\n",
    "#### ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3832e",
   "metadata": {},
   "source": [
    "Así es como puedes crear un `tf.train.Example` que represente a la misma persona que antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
    "                                                          b\"c@d.com\"]))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202cdb8f",
   "metadata": {},
   "source": [
    "El código es un poco detallado y repetitivo, pero podrías envolverlo fácilmente dentro de una pequeña función de ayuda. Ahora que tenemos un protobuf `Example`, podemos serializarlo llamando a su método `SerializeToString()`, y luego escribir los datos resultantes en un archivo TFRecord. Escribámoslo cinco veces para fingir que tenemos varios contactos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d496838",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    for _ in range(5):\n",
    "        f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0e1b3",
   "metadata": {},
   "source": [
    "¡Normalmente escribirías mucho más de cinco `Examples`! Por lo general, crearía un script de conversión que lea su formato actual (por ejemplo, archivos CSV), cree un protobuf de `Example` para cada instancia, los serialice y los guarde en varios archivos TFRecord, idealmente mezclándolos en el proceso. Esto requiere un poco de trabajo, así que una vez más asegúrese de que sea realmente necesario (quizás su canalización funcione bien con archivos CSV).\n",
    "\n",
    "Ahora que tenemos un buen archivo TFRecord que contiene varios ejemplos serializados, intentamos cargarlo.\n",
    "\n",
    "\n",
    "## Ejemplos de carga y análisis\n",
    "\n",
    "Para cargar los protobufs de ejemplo serializados, usaremos `tf.data.TFRecordDataset` una vez más y analizaremos cada ejemplo usando `tf.io.parse_single_example()`. Requiere al menos dos argumentos: un tensor escalar de cadena que contenga los datos serializados y una descripción de cada característica. La descripción es un diccionario que asigna cada nombre de característica a un descriptor `tf.io.FixedLenFeature` que indica la forma, el tipo y el valor predeterminado de la característica, o a un descriptor `tf.io.VarLenFeature` que indica solo el tipo si la longitud de la lista de características puede varían (como para la función `\"emails\"`).\n",
    "\n",
    "El siguiente código define un diccionario de descripción, luego crea un `TFRecordDataset` y le aplica una función de preprocesamiento personalizada para analizar cada protobuf de `Example` serializado que contiene este conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse(serialized_example):\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).map(parse)\n",
    "for parsed_example in dataset:\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0bf67",
   "metadata": {},
   "source": [
    "Las características de longitud fija se ananzan como tensores regulares, pero las características de longitud variable se ananizan como tensores dispersos. Puedes convertir un tensor disperso en un tensor denso usando `tf.sparse.to_dense()`, pero en este caso es más sencillo acceder a sus valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ca825",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\n",
    "#<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
    "\n",
    "parsed_example[\"emails\"].values\n",
    "#<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df3845",
   "metadata": {},
   "source": [
    "En lugar de analizar los ejemplos uno por uno usando `tf.io.parse_single_example()`, es posible que desee analizarlos por lote usando `tf.io.parse_example()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized_examples):\n",
    "    return tf.io.parse_example(serialized_examples, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(2).map(parse)\n",
    "for parsed_examples in dataset:\n",
    "    print(parsed_examples)  # two examples at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ede07d",
   "metadata": {},
   "source": [
    "Por último, una `BytesList` puede contener cualquier dato binario que desee, incluido cualquier objeto serializado. Por ejemplo, puede usar `tf.io.encode_jpeg()` para codificar una imagen usando el formato JPEG y colocar estos datos binarios en una `BytesList`. Más tarde, cuando su código lea TFRecord, comenzará analizando el `Example`, luego necesitará llamar a tf.io.decode_jpeg() para analizar los datos y obtener la imagen original (o puede usar `tf.io.decode_image( )`, que puede decodificar cualquier imagen BMP, GIF, JPEG o PNG). También puede almacenar cualquier tensor que desee en `BytesList` serializando el tensor usando `tf.io.serialize_tensor()` y luego colocando la cadena de bytes resultante en una función BytesList. Más adelante, cuando analice TFRecord, podrá analizar estos datos usando `tf.io.parse_tensor()`. Consulte el cuaderno de este capítulo en https://homl.info/colab3 para ver ejemplos de cómo almacenar imágenes y tensores en un archivo TFRecord.\n",
    "\n",
    "Como puede ver, el protobuf de `Example` es bastante flexible, por lo que probablemente será suficiente para la mayoría de los casos de uso. Sin embargo, puede resultar un poco engorroso de utilizar cuando se trata de listas de listas. Por ejemplo, supongamos que desea clasificar documentos de texto. Cada documento se puede representar como una lista de oraciones, donde cada oración se representa como una lista de palabras. Y quizás cada documento también tenga una lista de comentarios, donde cada comentario se representa como una lista de palabras. También puede haber algunos datos contextuales, como el autor, el título y la fecha de publicación del documento. El protobuf `SequenceExample` de TensorFlow está diseñado para tales casos de uso.\n",
    "\n",
    "## Manejo de listas utilizando el ejemplo de secuencia Protobuf\n",
    "\n",
    "Aquí está la definición del protobuf `SequenceExample`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9886a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "message FeatureList { repeated Feature feature = 1; };\n",
    "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
    "message SequenceExample {\n",
    "    Features context = 1;\n",
    "    FeatureLists feature_lists = 2;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c11ce",
   "metadata": {},
   "source": [
    "Un `SequenceExample` contiene un objeto `Features` para los datos contextuales y un objeto `FeatureLists` que contiene uno o más objetos `FeatureList` con nombre (por ejemplo, una FeatureList denominada `\"content\"` y otra denominada `\"comments\"`). Cada `FeatureList` contiene una lista de objetos `Feature`, cada uno de los cuales puede ser una lista de cadenas de bytes, una lista de enteros de 64 bits o una lista de flotantes (en este ejemplo, cada `Feature` representaría una oración o un comentario, tal vez en en forma de lista de identificadores de palabras). Crear un `SequenceExample`, serializarlo y analizarlo es similar a crear, serializar y analizar un ejemplo, pero debe usar `tf.io.parse_single_sequence_example()` para analizar un único SequenceExample o `tf.io.parse_sequence_example()` para analizar un lote . Ambas funciones devuelven una tupla que contiene las características del contexto (como un diccionario) y las listas de características (también como un diccionario). Si las listas de características contienen secuencias de diferentes tamaños (como en el ejemplo anterior), es posible que desee convertirlas en tensores irregulares usando `tf.RaggedTensor.from_sparse()` (consulte el cuaderno para ver el código completo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
    "    serialized_sequence_example, context_feature_descriptions,\n",
    "    sequence_feature_descriptions)\n",
    "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bc995",
   "metadata": {},
   "source": [
    "Ahora que sabe cómo almacenar, cargar, analizar y preprocesar de manera eficiente los datos utilizando la API tf.data, TFRecords y protobufs, es hora de centrar nuestra atención en las capas de preprocesamiento de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea17676",
   "metadata": {},
   "source": [
    "# Capas de preprocesamiento de Keras\n",
    "\n",
    "Preparar sus datos para una red neuronal normalmente requiere normalizar las características numéricas, codificar las características categóricas y el texto, recortar y redimensionar las imágenes, y más. Hay varias opciones para esto:\n",
    "\n",
    "- El preprocesamiento se puede hacer con anticipación al preparar sus archivos de datos de entrenamiento, utilizando cualquier herramienta que le guste, como NumPy, Pandas o Scikit-Learn. Tendrá que aplicar exactamente los mismos pasos de preprocesamiento en la producción, para asegurarse de que su modelo de producción reciba entradas preprocesadas similares a las que fue entrenada.\n",
    "\n",
    "+ Alternativamente, puede preprocesar sus datos sobre la marcha mientras los carga con tf.data, aplicando una función de preprocesamiento a cada elemento de un conjunto de datos usando el método `map()` de ese conjunto de datos, como hicimos anteriormente en este capítulo. Nuevamente, deberá aplicar los mismos pasos de preprocesamiento en producción.\n",
    "\n",
    "- Un último enfoque es incluir capas de preprocesamiento directamente dentro de su modelo para que pueda procesar previamente todos los datos de entrada sobre la marcha durante el entrenamiento, y luego utilizar las mismas capas de preprocesamiento en producción. El resto de este capítulo andará este último enfoque.\n",
    "\n",
    "Keras ofrece muchas capas de preprocesamiento que puede incluir en sus modelos: se pueden aplicar a características numéricas, características categóricas, imágenes y texto. Repasaremos las características numéricas y categóricas en las siguientes secciones, así como el preprocesamiento básico de texto, y cubriremos el preprocesamiento de imágenes en el capítulo 14 y el preprocesamiento de texto más avanzado en el capítulo 16."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab3241",
   "metadata": {},
   "source": [
    "## La capa de normalización\n",
    "\n",
    "Como vimos en el Capítulo 10, Keras proporciona una capa de `Normalization` que podemos usar para estandarizar las características de entrada. Podemos especificar la media y la varianza de cada característica al crear la capa o, más simplemente, pasar el conjunto de entrenamiento al método `adapt()` de la capa antes de ajustar el modelo, para que la capa pueda medir las medias y las variaciones de las características en su propio antes del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "model = tf.keras.models.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "norm_layer.adapt(X_train)  # computes the mean and variance of every feature\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f847a34",
   "metadata": {},
   "source": [
    "#### TIP\n",
    "\n",
    "La muestra de datos pasada al método `adapt()` debe ser lo suficientemente grande como para ser representativa de su conjunto de datos, pero no tiene que ser el conjunto de entrenamiento completo: para la capa de `Normalization`, generalmente se necesitarán unos cientos de instancias muestreadas aleatoriamente del conjunto de entrenamiento. ser suficiente para obtener una buena estimación de las medias y variaciones de las características.\n",
    "\n",
    "#### -------------------------------------------------\n",
    "\n",
    "Dado que incluimos la capa de `Normalization` dentro del modelo, ahora podemos implementar este modelo en producción sin tener que preocuparnos por la normalización nuevamente: el modelo simplemente se encargará de ello (consulte la Figura 13-4). ¡Fantástico! Este enfoque elimina por completo el riesgo de discrepancia en el preprocesamiento, que ocurre cuando las personas intentan mantener un código de preprocesamiento diferente para capacitación y producción, pero actualizan uno y se olvidan de actualizar el otro. Luego, el modelo de producción termina recibiendo datos preprocesados de una manera que no esperaba. Si tienen suerte, obtendrán un error claro. De lo contrario, la precisión del modelo simplemente se degrada silenciosamente.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1304.png)\n",
    "\n",
    "(_Figura 13-4. Incluyendo capas de preprocesamiento dentro de un modelo_)\n",
    "\n",
    "Incluir la capa de preprocesamiento directamente en el modelo es agradable y sencillo, pero ralentizará el entrenamiento (sólo muy ligeramente en el caso de la capa de `Normalization`): de hecho, dado que el preprocesamiento se realiza sobre la marcha durante el entrenamiento, ocurre una vez por época. Podemos hacerlo mejor normalizando todo el conjunto de entrenamiento solo una vez antes del entrenamiento. Para hacer esto, podemos usar la capa de `Normalization` de forma independiente (muy parecida a un Scikit-Learn `StandardScaler`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e02bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "X_train_scaled = norm_layer(X_train)\n",
    "X_valid_scaled = norm_layer(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0184b3",
   "metadata": {},
   "source": [
    "Ahora podemos entrenar un modelo en los datos a escala, esta vez sin una capa `Normalization`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626bc70",
   "metadata": {},
   "source": [
    "¡Bien! Esto debería acelerar un poco el entrenamiento. Pero ahora el modelo no preprocesará sus entradas cuando lo implementemos en producción. Para solucionar este problema, sólo necesitamos crear un nuevo modelo que envuelva tanto la capa de `Normalization` adaptada como el modelo que acabamos de entrenar. Luego podemos implementar este modelo final en producción, y este se encargará tanto de preprocesar sus entradas como de hacer predicciones (consulte la Figura 13-5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = tf.keras.Sequential([norm_layer, model])\n",
    "X_new = X_test[:3]  # pretend we have a few new instances (unscaled)\n",
    "y_pred = final_model(X_new)  # preprocesses the data and makes predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e932b",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1305.png)\n",
    "\n",
    "(_Figura 13-5. Preprocesar los datos solo una vez antes de entrenar utilizando capas de preprocesamiento, luego implementar estas capas dentro del modelo final_)\n",
    "\n",
    "Ahora tenemos lo mejor de ambos mundos: el entrenamiento es rápido porque solo procesamos previamente los datos una vez antes de que comience el entrenamiento, y el modelo final puede procesar previamente sus entradas sobre la marcha sin ningún riesgo de desajuste de preprocesamiento.\n",
    "\n",
    "Además, las capas de preprocesamiento de Keras funcionan muy bien con la API tf.data. Por ejemplo, es posible pasar un `tf.data.Dataset` al método `adapt()` de una capa de preprocesamiento. También es posible aplicar una capa de preprocesamiento de Keras a un `tf.data.Dataset` utilizando el método `map()` del conjunto de datos. Por ejemplo, así es como podría aplicar una capa de `Normalization` adaptada a las entidades de entrada de cada lote en un conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X, y: (norm_layer(X), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a2896d",
   "metadata": {},
   "source": [
    "Por último, si alguna vez necesita más funciones de las que proporcionan las capas de preprocesamiento de Keras, siempre puede escribir su propia capa de Keras, tal como lo comentamos en el Capítulo 12. Por ejemplo, si la capa de `Normalization` no existiera, podría obtener un resultado similar. usando la siguiente capa personalizada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyNormalization(tf.keras.layers.Layer):\n",
    "    def adapt(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
    "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        eps = tf.keras.backend.epsilon()  # a small smoothing term\n",
    "        return (inputs - self.mean_) / (self.std_ + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2716dc9",
   "metadata": {},
   "source": [
    "A continuación, echemos un vistazo a otra capa de preprocesamiento de Keras para características numéricas: la capa `Discretization`.\n",
    "\n",
    "\n",
    "## La capa de discreción\n",
    "\n",
    "El objetivo de la capa de `Discretization` es transformar una característica numérica en una característica categórica asignando rangos de valores (llamados contenedores) a categorías. A veces, esto resulta útil para entidades con distribuciones multimodales o con entidades que tienen una relación altamente no lineal con el objetivo. Por ejemplo, el siguiente código asigna una característica de edad numérica a tres categorías: menos de 18, 18 a 50 (no incluido) y 50 o más:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n",
    "discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories\n",
    "# <tf.Tensor: shape=(6, 1), dtype=int64, numpy=array([[0],[2],[2],[1],[1],[0]])>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39995786",
   "metadata": {},
   "source": [
    "En este ejemplo, proporcionamos los límites de contenedor deseados. Si lo prefiere, puede proporcionar la cantidad de contenedores que desea y luego llamar al método `adapt()` de la capa para permitirle encontrar los límites de contenedor apropiados según los percentiles de valor. Por ejemplo, si configuramos `num_bins=3`, entonces los límites del contenedor se ubicarán en los valores justo debajo de los percentiles 33 y 66 (en este ejemplo, en los valores 10 y 37):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
    "discretize_layer.adapt(age)\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories\n",
    "# <tf.Tensor: shape=(6, 1), dtype=int64, numpy=array([[1],[2],[2],[1],[2],[0]])>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c7d2b",
   "metadata": {},
   "source": [
    "Los identificadores de categorías como estos generalmente no deben pasarse directamente a una red neuronal, ya que sus valores no se pueden comparar de manera significativa. En su lugar, deben codificarse, por ejemplo, usando una codificación en caliente. Echemos un vistazo a cómo hacer esto ahora.\n",
    "\n",
    "## La capa de codificación de categorías\n",
    "\n",
    "Cuando sólo hay unas pocas categorías (por ejemplo, menos de una docena o dos), la codificación one-hot suele ser una buena opción (como se analiza en el Capítulo 2). Para hacer esto, Keras proporciona la capa `CategoryEncoding`. Por ejemplo, codifiquemos en caliente la característica `age_categories` que acabamos de crear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a20e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
    "onehot_layer(age_categories)\n",
    "'''\n",
    "<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
    "array([[0., 1., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [1., 0., 0.]], dtype=float32)>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07040b2e",
   "metadata": {},
   "source": [
    "Si intenta codificar más de una característica categórica a la vez (lo que solo tiene sentido si todas usan las mismas categorías), la clase `CategoryEncoding` realizará codificación multi-caliente de forma predeterminada: el tensor de salida contendrá un 1 para cada categoría presente en cualquier característica de entrada. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])\n",
    "onehot_layer(two_age_categories)\n",
    "'''\n",
    "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
    "array([[1., 1., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [1., 0., 1.]], dtype=float32)>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f80aa",
   "metadata": {},
   "source": [
    "Si cree que es útil saber cuántas veces ocurrió cada categoría, puede configurar `output_mode=\"count\"` al crear la capa `CategoryEncoding`, en cuyo caso el tensor de salida contendrá el número de ocurrencias de cada categoría. En el ejemplo anterior, el resultado sería el mismo excepto por la segunda fila, que sería `[0., 0., 2.]`.\n",
    "\n",
    "Tenga en cuenta que tanto la codificación multi-hot como la codificación de recuento pierden información, ya que no es posible saber de qué característica proviene cada categoría activa. Por ejemplo, tanto `[0, 1]` como `[1, 0]` están codificados como `[1., 1., 0.]`. Si desea evitar esto, debe codificar en caliente cada función por separado y concatenar las salidas. De esta manera, `[0, 1]` se codificaría como `[1., 0., 0., 0., 1., 0.]` y `[1, 0]` se codificaría como `[0., 1., 0. , 1., 0., 0.]`. Puede obtener el mismo resultado modificando los identificadores de categorías para que no se superpongan. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)\n",
    "onehot_layer(two_age_categories + [0, 3])  # adds 3 to the second feature\n",
    "'''\n",
    "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
    "array([[0., 1., 0., 1., 0., 0.],\n",
    "       [0., 0., 1., 0., 0., 1.],\n",
    "       [0., 0., 1., 1., 0., 0.]], dtype=float32)>\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb29c0",
   "metadata": {},
   "source": [
    "En esta salida, las tres primeras columnas corresponden a la primera característica, y las tres últimas corresponden a la segunda característica. Esto permite al modelo distinguir las dos características. Sin embargo, también aumenta el número de características que se alimentan al modelo y, por lo tanto, requiere más parámetros del modelo. Es difícil saber de antemano si una sola codificación multi-hot o una codificación de one-hot por característica funcionará mejor: depende de la tarea, y es posible que tengas que probar ambas opciones.\n",
    "\n",
    "Ahora puedes codificar características enteras categóricas usando la codificación de uno o más caliente. Pero, ¿qué pasa con las características categóricas del texto? Para esto, puedes usar la capa `StringLookup`.\n",
    "\n",
    "\n",
    "## La capa de búsqueda de cuerdas\n",
    "\n",
    "Usemos la capa Keras `StringLookup` para codificar en caliente una característica de `cities`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71407969",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])\n",
    "\n",
    "# <tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[1], [3], [3], [0]])>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c96d6f",
   "metadata": {},
   "source": [
    "Primero creamos una capa `StringLookup`, luego la adaptamos a los datos: descubre que hay tres categorías distintas. Luego usamos la capa para codificar algunas ciudades. Están codificados como enteros de forma predeterminada. Las categorías desconocidas se asignan a 0, como es el caso de \"Montreal\" en este ejemplo. Las categorías conocidas están numeradas a partir de 1, desde la categoría más frecuente hasta la menos frecuente.\n",
    "\n",
    "Convenientemente, si configura `output_mode=\"one_hot\"` al crear la capa `StringLookup`, generará un vector one-hot para cada categoría, en lugar de un número entero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ccbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])\n",
    "'''\n",
    "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
    "array([[0., 1., 0., 0.],\n",
    "       [0., 0., 0., 1.],\n",
    "       [0., 0., 0., 1.],\n",
    "       [1., 0., 0., 0.]], dtype=float32)>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7983fa7",
   "metadata": {},
   "source": [
    "#### TIP\n",
    "\n",
    "Keras también incluye una capa `IntegerLookup` que actúa de manera muy similar a la capa `StringLookup` pero toma números enteros como entrada, en lugar de cadenas.\n",
    "\n",
    "#### -----------------------------------------------\n",
    "\n",
    "Si el conjunto de entrenamiento es muy grande, puede ser conveniente adaptar la capa solo a un subconjunto aleatorio del conjunto de entrenamiento. En este caso, el método `adapt()` de la capa puede omitir algunas de las categorías más raras. De forma predeterminada, los asignaría a todos a la categoría 0, haciéndolos indistinguibles para el modelo. Para reducir este riesgo (sin dejar de adaptar la capa solo en un subconjunto del conjunto de entrenamiento), puede establecer `num_oov_indices` en un número entero mayor que 1. Esta es la cantidad de depósitos fuera de vocabulario (OOV) que se usarán: cada uno desconocido La categoría se asignará pseudoaleatoriamente a uno de los depósitos OOV, utilizando una función hash que mide el número de depósitos OOV. Esto permitirá que el modelo distinga al menos algunas de las categorías raras. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9525c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Foo\"], [\"Bar\"], [\"Baz\"]])\n",
    "\n",
    "# <tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[5], [7], [4], [3], [4]])>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c1ffc",
   "metadata": {},
   "source": [
    "Dado que hay cinco depósitos OOV, el ID de la primera categoría conocida ahora es 5 (`\"París\"`). Pero `\"Foo\"`, `\"Bar\"` y `\"Baz\"` son desconocidos, por lo que cada uno de ellos se asigna a uno de los depósitos OOV. `\"Bar\"` tiene su propio depósito dedicado (con ID 3), pero lamentablemente `\"Foo\"` y `\"Baz\"` están asignados al mismo depósito (con ID 4), por lo que no se pueden distinguir según el modelo. Esto se llama colisión de hash. La única forma de reducir el riesgo de colisión es aumentar la cantidad de depósitos OOV. Sin embargo, esto también aumentará el número total de categorías, lo que requerirá más RAM y parámetros de modelo adicionales una vez que las categorías estén codificadas en caliente. Por lo tanto, no aumente demasiado ese número.\n",
    "\n",
    "Esta idea de asignar categorías pseudoal azar a cubos se llama el truco de hashing. Keras proporciona una capa dedicada que hace precisamente eso: la capa `Hashing`.\n",
    "\n",
    "\n",
    "## La capa de hasing\n",
    "\n",
    "Para cada categoría, la capa Keras `Hashing` calcula un hash, módulo el número de depósitos (o \"contenedores\"). El mapeo es completamente pseudoaleatorio, pero estable entre ejecuciones y plataformas (es decir, la misma categoría siempre se asignará al mismo número entero, siempre que el número de contenedores no cambie). Por ejemplo, usemos la capa `Hashing` para codificar algunas ciudades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec085fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_layer = tf.keras.layers.Hashing(num_bins=10)\n",
    "hashing_layer([[\"Paris\"], [\"Tokyo\"], [\"Auckland\"], [\"Montreal\"]])\n",
    "\n",
    "# <tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[0], [1], [9], [1]])>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5797c493",
   "metadata": {},
   "source": [
    "El beneficio de esta capa es que no necesita ser adaptada en absoluto, lo que a veces puede ser útil, especialmente en un entorno fuera del núcleo (cuando el conjunto de datos es demasiado grande para caber en la memoria). Sin embargo, una vez más tenemos una colisión de hash: \"Tokio\" y \"Montreal\" están asignados a la misma identificación, lo que los hace indistinguibles por el modelo. Por lo tanto, por lo general es preferible atenerse a la capa `StringLookup`.\n",
    "\n",
    "Ahora echemos un vistazo a otra forma de codificar categorías: incrustaciones entrenables.\n",
    "\n",
    "## Codificación De Características Categóricas Mediante Incrustaciones\n",
    "\n",
    "Una incrustación es una representación densa de algunos datos de mayor dimensión, como una categoría o una palabra en un vocabulario. Si hay 50.000 categorías posibles, entonces la codificación en caliente produciría un vector disperso de 50.000 dimensiones (es decir, que contiene en su mayoría ceros). Por el contrario, una incrustación sería un vector denso comparativamente pequeño; por ejemplo, con solo 100 dimensiones.\n",
    "\n",
    "En el aprendizaje profundo, las incorporaciones generalmente se inicializan de forma aleatoria y luego se entrenan mediante descenso de gradiente, junto con los demás parámetros del modelo. Por ejemplo, la categoría `\"NEAR BAY\"` en el conjunto de datos de vivienda de California podría representarse inicialmente mediante un vector aleatorio como `[0,131, 0,890]`, mientras que la categoría `\"NEAR OCEAN\"` podría representarse mediante otro vector aleatorio como `[0,631, 0,791 ]`. En este ejemplo, utilizamos incrustaciones 2D, pero el número de dimensiones es un hiperparámetro que puedes modificar.\n",
    "\n",
    "Dado que estas incorporaciones se pueden entrenar, mejorarán gradualmente durante el entrenamiento; y como en este caso representan categorías bastante similares, el descenso del gradiente ciertamente terminará acercándolos, mientras que tenderá a alejarlos de la incorporación de la categoría `\"INLAND\"` (ver Figura 13-6). De hecho, cuanto mejor sea la representación, más fácil será para la red neuronal hacer predicciones precisas, por lo que el entrenamiento tiende a hacer que las incorporaciones sean representaciones útiles de las categorías. Esto se llama aprendizaje de representación (verá otros tipos de aprendizaje de representación en el Capítulo 17).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1306.png)\n",
    "\n",
    "(_Figura 13-6. Las incrustaciones mejorarán gradualmente durante el entrenamiento_)\n",
    "\n",
    "#### INSERCIONES DE PALABRAS\n",
    "\n",
    "Las incrustaciones generalmente serán representaciones útiles para la tarea en cuestión, sino que muy a menudo estas mismas incrustaciones se pueden reutilizar con éxito para otras tareas. El ejemplo más común de esto son las incrustaciones de palabras (es decir, incrustaciones de palabras individuales): cuando está trabajando en una tarea de procesamiento de lenguaje natural, a menudo es mejor reutilizar incrustaciones de palabras preentrenadas que entrenar a las suyas propias.\n",
    "\n",
    "La idea de usar vectores para representar palabras se remonta a la década de 1960, y se han utilizado muchas técnicas sofisticadas para generar vectores útiles, incluido el uso de redes neuronales. Pero las cosas realmente despegaron en 2013, cuando Tomáš Mikolov y otros investigadores de Google publicaron un artículo⁠6 que describía una técnica eficiente para aprender incrustaciones de palabras usando redes neuronales, superando significativamente los intentos anteriores. Esto les permitió aprender incrustaciones en un corpus muy grande de texto: entrenaron una red neuronal para predecir las palabras cerca de cualquier palabra y obtuvieron incrustaciones de palabras asombrosas. Por ejemplo, los sinónimos tenían incrustaciones muy cercanas, y palabras relacionadas semánticamente como Francia, España e Italia terminaron agrupadas.\n",
    "\n",
    "Sin embargo, no se trata solo de proximidad: las incrustaciones de palabras también se organizaron a lo largo de ejes significativos en el espacio de incrustación. Aquí hay un ejemplo famoso: si calculas Rey - Hombre + Mujer (sumando y restando los vectores de incrustación de estas palabras), entonces el resultado estará muy cerca de la incrustación de la palabra Reina (ver Figura 13-7). En otras palabras, ¡la palabra incrustaciones codifica el concepto de género! Del mismo modo, puede calcular Madrid - España + Francia, y el resultado está cerca de París, lo que parece mostrar que la noción de ciudad capital también se codificó en las incrustaciones.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098125967/files/assets/mls3_1307.png)\n",
    "\n",
    "(_Figura 13-7. Las incrustaciones de palabras de palabras similares tienden a estar cerca, y algunos ejes parecen codificar conceptos significativos_)\n",
    "\n",
    "Desafortunadamente, las incrustaciones de palabras a veces capturan nuestros peores prejuicios. Por ejemplo, aunque aprenden correctamente que el hombre es para el rey como la mujer es para la reina, también parecen aprender que el hombre es para el médico como la mujer es para la enfermera: ¡un sesgo bastante sexista! Para ser justos, este ejemplo en particular es probablemente exagerado, como se señaló en un artículo de 2019⁠7 de Malvina Nissim et al. Sin embargo, garantizar la equidad en los algoritmos de aprendizaje profundo es un tema de investigación importante y activo.\n",
    "\n",
    "Keras proporciona una capa de incrustación `Embedding`, que envuelve una matriz de incrustación: esta matriz tiene una fila por categoría y una columna por dimensión de `Embedding`. De forma predeterminada, se inicializa aleatoriamente. Para convertir un ID de categoría en una `Embedding`, la capa `Embedding` simplemente busca y devuelve la fila que corresponde a esa categoría. ¡Eso es todo al respecto! Por ejemplo, inicialicemos una capa de incrustación con cinco filas e `Embedding` 2D, y usémosla para codificar algunas categorías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
    "embedding_layer(np.array([2, 4, 2]))\n",
    "'''\n",
    "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
    "array([[-0.04663396,  0.01846724],\n",
    "       [-0.02736737, -0.02768031],\n",
    "       [-0.04663396,  0.01846724]], dtype=float32)>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788c2f6",
   "metadata": {},
   "source": [
    "Como puede ver, la categoría 2 se codifica (dos veces) como el vector 2D `[-0.04663396, 0.01846724]`, mientras que la categoría 4 se codifica como `[-0.02736737, -0.02768031]`. Dado que la capa aún no está entrenada, estas codificaciones son aleatorias.\n",
    "\n",
    "#### ADVERTENCIA\n",
    "\n",
    "Una capa de `Embedding` se inicializa aleatoriamente, por lo que no tiene sentido usarla fuera de un modelo como una capa de preprocesamiento independiente a menos que la inicialice con pesos previamente entrenados.\n",
    "\n",
    "Si desea incrustar un atributo de texto categórico, simplemente puede encadenar una capa `StringLookup` y una capa `Embedding`, así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> tf.random.set_seed(42)\n",
    ">>> ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    ">>> str_lookup_layer = tf.keras.layers.StringLookup()\n",
    ">>> str_lookup_layer.adapt(ocean_prox)\n",
    ">>> lookup_and_embed = tf.keras.Sequential([\n",
    "...     str_lookup_layer,\n",
    "...     tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),\n",
    "...                               output_dim=2)\n",
    "... ])\n",
    "...\n",
    ">>> lookup_and_embed(np.array([[\"<1H OCEAN\"], [\"ISLAND\"], [\"<1H OCEAN\"]]))\n",
    "'''\n",
    "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
    "array([[-0.01896119,  0.02223358],\n",
    "       [ 0.02401174,  0.03724445],\n",
    "       [-0.01896119,  0.02223358]], dtype=float32)>\n",
    "'''s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291536d",
   "metadata": {},
   "source": [
    "Tenga en cuenta que el número de filas en la matriz de incrustación debe ser igual al tamaño del vocabulario: ese es el número total de categorías, incluidas las categorías conocidas más los depósitos OOV (solo uno por defecto). El método `vocabulary_size()` de la clase `StringLookup` devuelve convenientemente este número.\n",
    "\n",
    "#### PROPINA\n",
    "\n",
    "En este ejemplo utilizamos incrustaciones 2D, pero como regla general, las incrustaciones suelen tener de 10 a 300 dimensiones, dependiendo de la tarea, el tamaño del vocabulario y el tamaño de su conjunto de entrenamiento. Tendrás que ajustar este hiperparámetro.\n",
    "\n",
    "#### ------------------------------------------------\n",
    "\n",
    "Juntando todo, ahora podemos crear un modelo de Keras que pueda procesar una característica de texto categórica junto con características numéricas regulares y aprender una incrustación para cada categoría (así como para cada cubo OOV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab940a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_train_cat, y_train = [...]  # load the training set\n",
    "X_valid_num, X_valid_cat, y_valid = [...]  # and the validation set\n",
    "\n",
    "num_input = tf.keras.layers.Input(shape=[8], name=\"num\")\n",
    "cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"cat\")\n",
    "cat_embeddings = lookup_and_embed(cat_input)\n",
    "encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\n",
    "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
    "model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,\n",
    "                    validation_data=((X_valid_num, X_valid_cat), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd58c80",
   "metadata": {},
   "source": [
    "Este modelo toma dos entradas: `num_input`, que contiene ocho características numéricas por instancia, más `cat_input`, que contiene una única entrada de texto categórico por instancia. El modelo utiliza el modelo `lookup_and_embed` que creamos anteriormente para codificar cada categoría de proximidad al océano como la incrustación entrenable correspondiente. A continuación, concatena las entradas numéricas y las incrustaciones utilizando la función `concatenate()` para producir las entradas codificadas completas, que están listas para ser alimentadas a una red neuronal. Podríamos agregar cualquier tipo de red neuronal en este punto, pero por simplicidad simplemente agregamos una única capa de salida densa y luego creamos el modelo Keras con las entradas y salidas que acabamos de definir. A continuación compilamos el modelo y lo entrenamos, pasando las entradas numéricas y categóricas.\n",
    "\n",
    "Como vio en el Capítulo 10, dado que las capas de entrada se denominan `\"num\"` y `\"cat\"`, también podríamos haber pasado los datos de entrenamiento al método `fit()` usando un diccionario en lugar de una tupla: `{\"num\": X_train_num, \"gato\": X_train_cat}`. Alternativamente, podríamos haber pasado un `tf.data.Dataset` que contenga lotes, cada uno representado como `((X_batch_num, X_batch_cat), y_batch)` o como `({\"num\": X_batch_num, \"cat\": X_batch_cat}, y_batch)`. Y por supuesto lo mismo ocurre con los datos de validación.\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "La codificación one-hot seguida de una capa `Dense` (sin función de activación ni sesgos) es equivalente a una capa de `Embedding`. Sin embargo, la capa de `Embedding` utiliza muchos menos cálculos, ya que evita muchas multiplicaciones por cero; la diferencia de rendimiento se vuelve clara cuando crece el tamaño de la matriz de incrustación. La matriz de peso de la capa `Dense` desempeña el papel de matriz de `Embedding`. Por ejemplo, usar vectores one-hot de tamaño 20 y una capa Densa con 10 unidades es equivalente a usar una capa de `Embedding` con `input_dim=20` y `output_dim=10`. Como resultado, sería un desperdicio utilizar más dimensiones de incrustación que el número de unidades en la capa que sigue a la capa de `Embedding`.\n",
    "\n",
    "#### -----------------------------------------------\n",
    "\n",
    "Vale, ahora que has aprendido a codificar características categóricas, es hora de centrar nuestra atención en el preprocesamiento de texto.\n",
    "\n",
    "\n",
    "# Preprocesamiento de texto\n",
    "\n",
    "Keras proporciona una capa TextVectorization para el preprocesamiento básico de texto. Al igual que la capa `StringLookup`, debes pasarle un vocabulario al crearla o dejar que aprenda el vocabulario a partir de algunos datos de entrenamiento usando el método `adapt()`. Veamos un ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75814ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
    "text_vec_layer = tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])\n",
    "\n",
    "'''\n",
    "<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
    "array([[2, 1, 0, 0],\n",
    "       [6, 2, 1, 2]])>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada89f0",
   "metadata": {},
   "source": [
    "Las dos frases \"¡Sé bueno!\" y “Pregunta: ¿ser o ser?” se codificaron como `[2, 1, 0, 0]` y `[6, 2, 1, 2]`, respectivamente. El vocabulario se aprendió a partir de las cuatro oraciones en los datos de entrenamiento: “be” = 2, “to” = 3, etc. Para construir el vocabulario, el método `adapt()` primero convirtió las oraciones de entrenamiento a minúsculas y eliminó la puntuación, lo cual es ¿Por qué “Ser”, “ser” y “ser”? están todos codificados como “be” = 2. A continuación, las oraciones se dividieron en espacios en blanco y las palabras resultantes se ordenaron por frecuencia descendente, produciendo el vocabulario final. Al codificar oraciones, las palabras desconocidas se codifican como 1. Por último, dado que la primera oración es más corta que la segunda, se completó con ceros.\n",
    "\n",
    "#### PROPINA\n",
    "\n",
    "La capa `TextVectorization` tiene muchas opciones. Por ejemplo, puede conservar el caso y la puntuación si lo desea, estableciendo `standarize = None`, o puede pasar cualquier función de estandarización que desee como argumento de `standarize`. Puede evitar la división configurando `split=None`, o puede pasar su propia función de división. Puede configurar el argumento `output_sequence_length` para garantizar que todas las secuencias de salida se recorten o rellenen a la longitud deseada, o puede configurar ragged=True para obtener un tensor irregular en lugar de un tensor normal. Consulte la documentación para conocer más opciones.\n",
    "\n",
    "#### ---------------------------------------------\n",
    "\n",
    "Los ID de palabras deben codificarse, generalmente usando una capa de `Embedding`: lo haremos en el Capítulo 16. Alternativamente, puede configurar el argumento modo_salida de la capa `TextVectorization` en `\"multi_hot\"` o `\"count\"` para obtener las codificaciones correspondientes. Sin embargo, simplemente contar palabras no suele ser lo ideal: palabras como “para” y “el” son tan frecuentes que apenas importan, mientras que palabras más raras como “baloncesto” son mucho más informativas. Por lo tanto, en lugar de configurar Output_mode en `\"multi_hot\"` o `\"count\"`, generalmente es preferible configurarlo en `\"tf_idf\"`, que significa frecuencia de término × frecuencia de documento inversa (TF-IDF). Esto es similar a la codificación de recuento, pero las palabras que aparecen con frecuencia en los datos de entrenamiento se reducen y, a la inversa, las palabras raras se aumentan. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ad831",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])\n",
    "\n",
    "'''\n",
    "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
    "array([[0.96725637, 0.6931472 , 0. , 0. , 0. , 0.        ],\n",
    "       [0.96725637, 1.3862944 , 0. , 0. , 0. , 1.0986123 ]], dtype=float32)>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e164d",
   "metadata": {},
   "source": [
    "Hay muchas variantes de TF-IDF, pero la forma en que la capa TextVectorization lo implementa es multiplicando cada recuento de palabras por un peso igual a log(1 + d / (f + 1)), donde d es el número total de oraciones (también conocido como , documentos) en los datos de entrenamiento y f cuenta cuántas de estas oraciones de entrenamiento contienen la palabra dada. Por ejemplo, en este caso hay d = 4 oraciones en los datos de entrenamiento, y la palabra “be” aparece en f = 3 de ellas. Dado que la palabra \"ser\" aparece dos veces en la oración \"Pregunta: ¿ser o ser?\", se codifica como 2 × log(1 + 4 / (1 + 3)) ≈ 1,3862944. La palabra “pregunta” sólo aparece una vez, pero como es una palabra menos común, su codificación es casi igual de alta: 1 × log(1 + 4 / (1 + 1)) ≈ 1,0986123. Tenga en cuenta que el peso promedio se utiliza para palabras desconocidas.\n",
    "\n",
    "Este enfoque de la codificación de texto es fácil de usar y puede dar resultados bastante buenos para las tareas básicas de procesamiento del lenguaje natural, pero tiene varias limitaciones importantes: solo funciona con lenguajes que separan las palabras con espacios, no distingue entre homónimos (por ejemplo, \"soportar\" frente a \"oso de peluche\"), no da ninguna pista a su modelo de que palabras como \"evolución\" y \"evolucionario\" estén relacionadas, etc. Y si usas la codificación multi-hot, count o TF-IDF, entonces se pierde el orden de las palabras. Entonces, ¿cuáles son las otras opciones?\n",
    "\n",
    "Una opción es utilizar la biblioteca de texto TensorFlow, que proporciona funciones de preprocesamiento de texto más avanzadas que la capa `TextVectorization`. Por ejemplo, incluye varios tokenizadores de subpalabras capaces de dividir el texto en tokens más pequeños que las palabras, lo que hace posible que el modelo detecte más fácilmente que la \"evolución\" y la \"evolución\" tienen algo en común (más sobre la tokenización de subpalabras en el Capítulo 16).\n",
    "\n",
    "Otra opción es utilizar componentes de modelo de lenguaje preentrenados. Echemos un vistazo a esto ahora.\n",
    "\n",
    "## Uso de componentes de modelo de lenguaje preentrenado\n",
    "\n",
    "La biblioteca de TensorFlow Hub facilita la reutilización de los componentes del modelo preentrenados en sus propios modelos, para texto, imagen, audio y más. Estos componentes del modelo se llaman módulos. Simplemente navegue por el repositorio de TF Hub, encuentre el que necesita y copie el ejemplo de código en su proyecto, y el módulo se descargará automáticamente y se incluirá en una capa de Keras que puede incluir directamente en su modelo. Los módulos suelen contener tanto código de preprocesamiento como pesos preentrenados, y generalmente no requieren entrenamiento adicional (pero, por supuesto, el resto de su modelo sin duda requerirá entrenamiento).\n",
    "\n",
    "Por ejemplo, hay disponibles algunos potentes modelos de lenguaje preentrenados. Los más potentes son bastante grandes (varios gigabytes), así que para un ejemplo rápido usemos el módulo thennlm `nnlm-en-dim50`, versión 2, que es un módulo bastante básico que toma texto sin procesar como entrada y genera incrustaciones de oraciones de 50 dimensiones. Importaremos TensorFlow Hub y lo usaremos para cargar el módulo, luego usaremos ese módulo para codificar dos frases en vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
    "sentence_embeddings.numpy().round(2)\n",
    "\n",
    "'''\n",
    "array([[-0.25,  0.28,  0.01,  0.1 ,  [...] ,  0.05,  0.31],\n",
    "       [-0.2 ,  0.2 , -0.08,  0.02,  [...] , -0.04,  0.15]], dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72808334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6123b813",
   "metadata": {},
   "source": [
    "La capa `hub.KerasLayer` descarga el módulo desde la URL proporcionada. Este módulo en particular es un codificador de oraciones: toma cadenas como entrada y codifica cada una como un único vector (en este caso, un vector de 50 dimensiones). Internamente, analiza la cadena (dividiendo palabras en espacios) e incrusta cada palabra usando una matriz de incrustación que fue previamente entrenada en un corpus enorme: el corpus Google News 7B (¡siete mil millones de palabras!). Luego calcula la media de todas las incrustaciones de palabras y el resultado es la incrustación de la oración.⁠9\n",
    "\n",
    "Solo necesita incluir este `hub_layer` en su modelo y estará listo para comenzar. Tenga en cuenta que este modelo de idioma en particular fue entrenado en el idioma inglés, pero hay muchos otros idiomas disponibles, así como modelos multilingües.\n",
    "\n",
    "Por último, pero no menos importante, la excelente biblioteca de Transformers de código abierto de Hugging Face también facilita la inclusión de poderosos componentes de modelos de lenguaje dentro de tus propios modelos. Puedes navegar por Hugging Face Hub, elegir el modelo que quieras y usar los ejemplos de código proporcionados para empezar. Solía contener solo modelos de lenguaje, pero ahora se ha ampliado para incluir modelos de imagen y más.\n",
    "\n",
    "Volveremos al procesamiento del lenguaje natural con más profundidad en el capítulo 16. Ahora echemos un vistazo a las capas de preprocesamiento de imágenes de Keras.\n",
    "\n",
    "\n",
    "## Capas de preprocesamiento de imágenes\n",
    "\n",
    "La API de preprocesamiento de Keras incluye tres capas de preprocesamiento de imágenes:\n",
    "\n",
    "- `tf.keras.layers.Resizing` cambia el tamaño de las imágenes de entrada al tamaño deseado. Por ejemplo, `Resizing(height=100, width=200)` cambia el tamaño de cada imagen a 100 × 200, posiblemente distorsionando la imagen. Si configura `crop_to_aspect_ratio=True`, la imagen se recortará según la proporción de imagen de destino para evitar distorsiones.\n",
    "\n",
    "+ `tf.keras.layers.Rescaling` cambia la escala de los valores de píxeles. Por ejemplo, `Rescal⁠ing(scale=2/255, offset=-1)` escala los valores de 0 → 255 a –1 → 1.\n",
    "\n",
    "- `tf.keras.layers.CenterCrop` recorta la imagen, manteniendo solo un parche central de la altura y el ancho deseados.\n",
    "\n",
    "Por ejemplo, carguemos un par de imágenes de muestra y las recortemos al centro. Para esto, usaremos la función `load_sample_images()` de Scikit-Learn; esto carga dos imágenes en color, una de un templo chino y la otra de una flor (esto requiere la biblioteca Pillow, que ya debería estar instalada si está utilizando Colab o si siguió las instrucciones de instalación):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "images = load_sample_images()[\"images\"]\n",
    "crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)\n",
    "cropped_images = crop_image_layer(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e760534",
   "metadata": {},
   "source": [
    "Keras también incluye varias capas para el aumento de datos, como `RandomCrop`, `RandomFlip`, `RandomTranslation`, `RandomRotation`, `RandomZoom`, `RandomHeight`, `RandomWidth` y `RandomContrast`. Estas capas solo están activas durante el entrenamiento y aplican aleatoriamente alguna transformación a las imágenes de entrada (sus nombres se explican por sí solos). El aumento de datos aumentará artificialmente el tamaño del conjunto de entrenamiento, lo que a menudo conduce a un mejor rendimiento, siempre que las imágenes transformadas parezcan imágenes realistas (no aumentadas). Cubriremos el procesamiento de imágenes más de cerca en el próximo capítulo.\n",
    "\n",
    "#### NOTA\n",
    "\n",
    "En esencia, las capas de preprocesamiento de Keras se basan en la API de bajo nivel de TensorFlow. Por ejemplo, la capa de `Normalization` usa `tf.nn.moments()` para calcular tanto la media como la varianza, la capa de `Discretization` usa `tf.raw_ops.Bucketize()`, `CategoricalEncoding` usa `tf.math.bincount()`, `IntegerLookup` y `StringLookup` usan el paquete `tf.lookup`, `Hashing` y `TextVectorization` usan varias operaciones del paquete `tf.strings`, `Embedding` usa `tf.nn.embedding_lookup()` y las capas de preprocesamiento de imágenes usan las operaciones del paquete `tf.image`. Si la API de preprocesamiento de Keras no es suficiente para sus necesidades, es posible que en ocasiones necesite utilizar directamente la API de bajo nivel de TensorFlow.\n",
    "\n",
    "#### -----------------------------------------------------------\n",
    "\n",
    "Ahora echamos un vistazo a otra forma de cargar datos de manera fácil y eficiente en TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14fcb7",
   "metadata": {},
   "source": [
    "# El Proyecto De Conjuntos De Datos De TensorFlow\n",
    "\n",
    "El proyecto TensorFlow Datasets (TFDS) hace que sea muy fácil cargar conjuntos de datos comunes, desde pequeños como MNIST o Fashion MNIST hasta grandes conjuntos de datos como ImageNet (¡necesitarás bastante espacio en disco!). La lista incluye conjuntos de datos de imágenes, conjuntos de datos de texto (incluidos los conjuntos de datos de traducción), conjuntos de datos de audio y vídeo, series temporales y mucho más. Puedes visitar https://homl.info/tfds para ver la lista completa, junto con una descripción de cada conjunto de datos. También puede consultar Conozca sus datos, que es una herramienta para explorar y comprender muchos de los conjuntos de datos proporcionados por TFDS.\n",
    "\n",
    "TFDS no está incluido con TensorFlow, pero si se está ejecutando en Colab o si siguió las instrucciones de instalación en https://homl.info/install, entonces ya está instalado. A continuación, puede importar `tensorflow_datasets`, generalmente como `tfds`, y luego llamar a la función `tfds.load()`, que descargará los datos que desee (a menos que ya se hayan descargado anteriormente) y devolverá los datos como un diccionario de conjuntos de datos (normalmente uno para entrenamiento y otro para pruebas, pero esto depende del conjunto de datos que elija). Por ejemplo, vamos a descargar MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938dd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55634541",
   "metadata": {},
   "source": [
    "A continuación, puede aplicar cualquier transformación que desee (normalmente barajando, por lotes y preencuado), y estará listo para entrenar a su modelo. He aquí un ejemplo sencillo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f759dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    # [...] do something with the images and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de6d71",
   "metadata": {},
   "source": [
    "#### TIP\n",
    "\n",
    "La función `load()` puede mezclar los archivos que descarga: simplemente configure `shuffle_files=True`. Sin embargo, esto puede ser insuficiente, por lo que es mejor mezclar un poco más los datos de entrenamiento.\n",
    "\n",
    "#### ----------------------------------------------\n",
    "\n",
    "Tenga en cuenta que cada elemento del conjunto de datos es un diccionario que contiene tanto las características como las etiquetas. Pero Keras espera que cada elemento sea una tupla que contenga dos elementos (de nuevo, las características y las etiquetas). Podrías transformar el conjunto de datos usando el método themap `map()`, así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176620f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(buffer_size=10_000, seed=42).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee7ce6",
   "metadata": {},
   "source": [
    "Pero es más sencillo pedirle a la función `load()` que haga esto por usted estableciendo `as_supervised=True` (obviamente, esto funciona solo para conjuntos de datos etiquetados).\n",
    "\n",
    "Por último, TFDS proporciona una forma conveniente de dividir `split` los datos utilizando el argumento de división. Por ejemplo, si desea utilizar el primer 90% del conjunto de entrenamiento para entrenamiento, el 10% restante para validación y todo el conjunto de prueba para pruebas, entonces puede configurar `split=[\"train[:90%]\", \"train[90%:]\", \"test\"]`. La función `load()` devolverá los tres conjuntos. Aquí hay un ejemplo completo, cargando y dividiendo el conjunto de datos MNIST usando TFDS, luego usando estos conjuntos para entrenar y evaluar un modelo Keras simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6928b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = tfds.load(\n",
    "    name=\"mnist\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "train_set = train_set.shuffle(buffer_size=10_000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = valid_set.batch(32).cache()\n",
    "test_set = test_set.batch(32).cache()\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5140625",
   "metadata": {},
   "source": [
    "¡Enhorabuena, has llegado al final de este capítulo bastante técnico! Puede sentir que está un poco lejos de la belleza abstracta de las redes neuronales, pero el hecho es que el aprendizaje profundo a menudo implica grandes cantidades de datos, y saber cómo cargarlos, analizarlos y procesarlos previamente de manera eficiente es una habilidad crucial. En el siguiente capítulo, analizaremos las redes neuronales convolucionales, que se encuentran entre las arquitecturas de redes neuronales más exitosas para el procesamiento de imágenes y muchas otras aplicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6058b",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "¿Por qué querrías usar la API de tf.data?\n",
    "\n",
    "¿Cuáles son los beneficios de dividir un conjunto de datos grande en varios archivos?\n",
    "\n",
    "Durante el entrenamiento, ¿cómo puede saber que su canalización de entrada es el cuello de botella? ¿Qué puedes hacer para arreglarlo?\n",
    "\n",
    "¿Puedes guardar cualquier dato binario en un archivo TFRecord, o solo en buffers de protocolo serializados?\n",
    "\n",
    "Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
    "\n",
    "Al usar TFRecords, ¿cuándo te gustaría activar la compresión? ¿Por qué no lo haces sistemáticamente?\n",
    "\n",
    "Los datos se pueden procesar previamente directamente al escribir los archivos de datos, o dentro de la canalización tf.data, o en capas de preprocesamiento dentro de su modelo. ¿Puedes enumerar algunos pros y contras de cada opción?\n",
    "\n",
    "Nombra algunas formas comunes en las que puedes codificar características enteras categóricas. ¿Qué pasa con el texto?\n",
    "\n",
    "Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. \n",
    "\n",
    "Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label.⁠10 Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data.\n",
    "\n",
    "In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
    "\n",
    "Descargue el conjunto de datos de reseñas de películas grandes, que contiene 50 000 reseñas de películas de la base de datos de películas de Internet (IMDb). Los datos están organizados en dos directorios, entrenamiento y prueba, cada uno de los cuales contiene un subdirectorio pos con 12.500 revisiones positivas y un subdirectorio neg con 12.500 revisiones negativas. Cada revisión se almacena en un archivo de texto separado. Hay otros archivos y carpetas (incluidas las versiones de bolsa de palabras preprocesadas), pero los ignoraremos en este ejercicio.\n",
    "\n",
    "Divida el conjunto de pruebas en un conjunto de validación (15 000) y un conjunto de pruebas (10 000).\n",
    "Utilice tf.data para crear un conjunto de datos eficiente para cada conjunto.\n",
    "\n",
    "Create a binary classification model, using a TextVectorization layer to preprocess each review.\n",
    "\n",
    "Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.\n",
    "\n",
    "Entrena el modelo y mira qué precisión obtienes. Intenta optimizar tus tuberías para que el entrenamiento sea lo más rápido posible.\n",
    "\n",
    "Utilice TFDS para cargar el mismo conjunto de datos más fácilmente:tfds.load(\"imdb_reviews\")\n",
    "\n",
    "Las soluciones a estos ejercicios están disponibles al final del cuaderno de este capítulo, en https://homl.info/colab3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a2aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
